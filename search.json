[
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "About Me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "story_telling.html#my-data-science-portfolio",
    "href": "story_telling.html#my-data-science-portfolio",
    "title": "About Me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "Competition/project1.html",
    "href": "Competition/project1.html",
    "title": "Client Report - What’s in a Name?",
    "section": "",
    "text": "What’s in a Name?\n\n\n\n\nElevator pitch\nIn comparing baby names and birth years looking for trends, I have learned that my name “Braxton” was not all that popular when I was born. If you were to get a call from a “Brittney” on the phone, it would be safe to assume that her age would be 34 years old. I also analyzed the effect of certain events on popularity of names.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\ndf \n\n\n\n\n\n\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n0\nAaden\n2005\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n1\nAaden\n2007\n0.0\n5.0\n0.0\n5.0\n20.0\n6.0\n0.0\n0.0\n...\n5.0\n14.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n98.0\n\n\n2\nAaden\n2008\n0.0\n15.0\n13.0\n20.0\n135.0\n10.0\n9.0\n0.0\n...\n15.0\n99.0\n8.0\n26.0\n0.0\n11.0\n19.0\n5.0\n0.0\n939.0\n\n\n3\nAaden\n2009\n0.0\n28.0\n20.0\n23.0\n158.0\n22.0\n12.0\n0.0\n...\n33.0\n140.0\n6.0\n17.0\n0.0\n31.0\n23.0\n14.0\n0.0\n1242.0\n\n\n4\nAaden\n2010\n0.0\n8.0\n6.0\n12.0\n62.0\n9.0\n5.0\n0.0\n...\n9.0\n56.0\n0.0\n11.0\n0.0\n7.0\n12.0\n0.0\n0.0\n414.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n393379\nZyon\n2011\n0.0\n0.0\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n...\n5.0\n9.0\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n117.0\n\n\n393380\nZyon\n2012\n0.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n7.5\n0.0\n12.0\n0.0\n0.0\n0.0\n0.0\n0.0\n118.5\n\n\n393381\nZyon\n2013\n0.0\n0.0\n0.0\n0.0\n6.0\n0.0\n0.0\n0.0\n...\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n93.0\n\n\n393382\nZyon\n2014\n0.0\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n...\n0.0\n7.0\n0.0\n9.0\n0.0\n0.0\n0.0\n0.0\n0.0\n73.0\n\n\n393383\nZyon\n2015\n0.0\n0.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n0.0\n10.0\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n87.5\n\n\n\n\n393384 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 1\nHow does your name at your birth year compare to its use historically?\nHistoricly it looks like my name was given after a slight rise in popularity. However, I was given my name before the largest spike after about 2002.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nPlot of the name Braxton\n# Include and execute your code here\nmy_name_df = df[df['name'] == 'Braxton']\nyear_2000_data = my_name_df[my_name_df['year'] == 2000]\n\n(ggplot(my_name_df, aes(x='year', y='Total')) +\n    geom_point(color = 'blue') +\n    ggtitle('The Use of Braxton') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5)) +\n    geom_text(aes(x=year_2000_data['year'], y=year_2000_data['Total']), label='782', nudge_x=-0.5, nudge_y=400) +\n    geom_segment(aes(x=year_2000_data['year'], xend=year_2000_data['year'], y=year_2000_data['Total'] - 10, yend=year_2000_data['Total']+350), arrow=arrow(length=5, type='closed'), color='red')\n)\n\n\n   \n   \nThe Use of Braxton\n\n\n\n\ntable of the name Braxton\ndisplay(my_name_df)\n\n\n\n\n\n\nBraxton’s Name Usage\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n50029\nBraxton\n1914\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n13.0\n\n\n50030\nBraxton\n1915\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n11.0\n\n\n50031\nBraxton\n1916\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12.0\n\n\n50032\nBraxton\n1917\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n50033\nBraxton\n1918\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n10.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50095\nBraxton\n2011\n6.0\n123.0\n84.0\n37.0\n62.0\n43.0\n0.0\n0.0\n...\n131.0\n188.0\n93.0\n40.0\n0.0\n33.0\n35.0\n68.0\n16.0\n2491.0\n\n\n50096\nBraxton\n2012\n5.0\n111.0\n79.0\n37.0\n61.0\n39.0\n5.0\n0.0\n...\n151.0\n227.0\n65.0\n58.0\n0.0\n41.0\n62.0\n95.0\n22.0\n2986.0\n\n\n50097\nBraxton\n2013\n0.0\n100.0\n84.0\n53.0\n92.0\n49.0\n7.0\n0.0\n...\n158.0\n226.0\n63.0\n64.0\n0.0\n42.0\n62.0\n74.0\n9.0\n3085.0\n\n\n50098\nBraxton\n2014\n10.0\n121.0\n86.0\n45.0\n106.0\n43.0\n6.0\n0.0\n...\n150.0\n147.0\n79.0\n60.0\n0.0\n39.0\n56.0\n74.0\n20.0\n3186.0\n\n\n50099\nBraxton\n2015\n0.0\n103.0\n76.0\n48.0\n102.0\n38.0\n6.0\n0.0\n...\n173.0\n278.0\n54.0\n61.0\n0.0\n57.0\n74.0\n67.0\n9.0\n3265.0\n\n\n\n\n71 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nIt appears that the most popular year for the use of the name “Brittney” was 1990. This means that we could make a educated guess that a person named Britteny would be 34 years old.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nBar chart of the name Brittany\nbrittany_df = df[df['name'] == 'Brittany']\n\n(ggplot(brittany_df, aes(x='year', y='Total')) +\n    geom_bar(stat='identity') +\n    ggtitle('The Popularity of Brittney Over Time') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nThe Popularity of Brittney Over Time\n\n\n\n\nTable of the name Brittany\n# Include and execute your code here\n\ndisplay(brittany_df)\n\n\n\n\n\n\nBrittany’s Name Usage\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n53205\nBrittany\n1968\n0.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n53206\nBrittany\n1969\n0.0\n0.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12.0\n\n\n53207\nBrittany\n1970\n0.0\n0.0\n0.0\n0.0\n5.0\n5.0\n0.0\n0.0\n...\n0.0\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n32.0\n\n\n53208\nBrittany\n1971\n0.0\n0.0\n0.0\n5.0\n17.0\n0.0\n0.0\n0.0\n...\n0.0\n14.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n81.0\n\n\n53209\nBrittany\n1972\n0.0\n0.0\n0.0\n0.0\n11.0\n10.0\n0.0\n0.0\n...\n8.0\n14.0\n16.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n158.0\n\n\n53210\nBrittany\n1973\n0.0\n0.0\n0.0\n6.0\n17.0\n0.0\n0.0\n0.0\n...\n0.0\n18.0\n13.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n166.0\n\n\n53211\nBrittany\n1974\n0.0\n0.0\n7.0\n0.0\n19.0\n0.0\n0.0\n0.0\n...\n0.0\n17.0\n28.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n198.0\n\n\n53212\nBrittany\n1975\n0.0\n0.0\n0.0\n0.0\n28.0\n0.0\n0.0\n0.0\n...\n10.0\n23.0\n28.0\n0.0\n0.0\n9.0\n5.0\n0.0\n0.0\n277.0\n\n\n53213\nBrittany\n1976\n0.0\n0.0\n6.0\n0.0\n36.0\n5.0\n0.0\n0.0\n...\n15.0\n35.0\n21.0\n7.0\n0.0\n7.0\n11.0\n0.0\n0.0\n304.0\n\n\n53214\nBrittany\n1977\n0.0\n9.0\n8.0\n5.0\n39.0\n10.0\n0.0\n0.0\n...\n16.0\n48.0\n55.0\n5.0\n0.0\n0.0\n7.0\n0.0\n0.0\n448.0\n\n\n53215\nBrittany\n1978\n0.0\n8.0\n12.0\n8.0\n50.0\n17.0\n0.0\n0.0\n...\n15.0\n48.0\n67.0\n7.0\n0.0\n11.0\n5.0\n0.0\n0.0\n592.0\n\n\n53216\nBrittany\n1979\n0.0\n6.0\n13.0\n0.0\n47.0\n19.0\n10.0\n0.0\n...\n20.0\n76.0\n63.0\n14.0\n0.0\n13.0\n10.0\n7.0\n8.0\n764.0\n\n\n53217\nBrittany\n1980\n0.0\n11.0\n20.0\n13.0\n121.0\n41.0\n5.0\n0.0\n...\n39.0\n142.0\n110.0\n18.0\n0.0\n36.0\n28.0\n10.0\n10.0\n1383.0\n\n\n53218\nBrittany\n1981\n11.0\n23.0\n32.0\n22.0\n127.0\n37.0\n14.0\n0.0\n...\n54.0\n166.0\n116.0\n21.0\n0.0\n47.0\n24.0\n10.0\n14.0\n1701.0\n\n\n53219\nBrittany\n1982\n12.0\n50.0\n53.0\n47.0\n290.0\n88.0\n18.0\n5.0\n...\n102.0\n300.0\n152.0\n50.0\n7.0\n73.0\n58.0\n25.0\n16.0\n3093.0\n\n\n53220\nBrittany\n1983\n20.0\n71.0\n80.0\n66.0\n406.0\n131.0\n26.0\n19.0\n...\n133.0\n415.0\n156.0\n83.0\n7.0\n95.0\n68.0\n42.0\n19.0\n4377.0\n\n\n53221\nBrittany\n1984\n34.0\n158.0\n111.0\n112.0\n698.0\n225.0\n59.0\n22.0\n...\n243.0\n735.0\n159.0\n148.0\n6.0\n134.0\n114.0\n78.0\n36.0\n7664.0\n\n\n53222\nBrittany\n1985\n53.0\n351.0\n224.0\n217.0\n715.5\n319.0\n118.0\n48.0\n...\n500.0\n1256.0\n182.0\n390.0\n25.0\n231.0\n260.0\n235.0\n40.0\n14010.0\n\n\n53223\nBrittany\n1986\n49.0\n241.5\n269.0\n309.0\n886.5\n389.0\n165.0\n79.0\n...\n586.0\n792.5\n183.0\n533.0\n45.0\n287.0\n361.0\n229.0\n48.0\n17856.5\n\n\n53224\nBrittany\n1987\n56.0\n528.0\n267.0\n322.0\n977.0\n365.0\n196.0\n97.0\n...\n633.0\n795.5\n193.0\n628.0\n34.0\n289.0\n351.0\n241.0\n42.0\n18825.5\n\n\n53225\nBrittany\n1988\n48.0\n585.0\n304.0\n383.0\n1214.5\n370.0\n271.0\n126.0\n...\n409.5\n925.0\n167.0\n884.0\n43.0\n368.0\n455.0\n270.0\n41.0\n21952.0\n\n\n53226\nBrittany\n1989\n53.0\n837.0\n457.0\n549.0\n1513.0\n439.0\n371.0\n86.5\n...\n537.5\n1266.0\n185.0\n1247.0\n71.0\n500.0\n724.0\n402.0\n50.0\n30848.0\n\n\n53227\nBrittany\n1990\n75.0\n788.0\n440.0\n483.0\n1372.5\n450.0\n345.0\n90.0\n...\n546.5\n1181.5\n173.0\n1229.0\n86.0\n470.0\n671.0\n379.0\n38.0\n32562.5\n\n\n53228\nBrittany\n1991\n55.0\n585.0\n327.0\n379.0\n1155.5\n361.0\n293.0\n132.0\n...\n872.0\n983.0\n140.0\n959.0\n47.0\n363.0\n556.0\n352.0\n43.0\n26963.5\n\n\n53229\nBrittany\n1992\n51.0\n526.0\n282.0\n292.0\n915.0\n325.0\n223.0\n87.0\n...\n726.0\n1785.0\n119.0\n854.0\n57.0\n307.0\n465.0\n292.0\n40.0\n23416.5\n\n\n53230\nBrittany\n1993\n37.0\n511.0\n229.0\n298.0\n1597.0\n275.0\n199.0\n62.0\n...\n637.0\n1547.0\n123.0\n697.0\n44.0\n286.0\n389.0\n242.0\n26.0\n21728.0\n\n\n53231\nBrittany\n1994\n47.0\n422.0\n237.0\n227.0\n666.5\n227.0\n168.0\n72.0\n...\n599.0\n1359.0\n91.0\n641.0\n43.0\n229.0\n351.0\n228.0\n32.0\n17808.5\n\n\n53232\nBrittany\n1995\n34.0\n342.0\n190.0\n202.0\n605.5\n179.0\n156.0\n49.0\n...\n516.0\n1293.0\n110.0\n537.0\n40.0\n216.0\n271.0\n194.0\n19.0\n15875.5\n\n\n53233\nBrittany\n1996\n29.0\n318.0\n163.0\n195.0\n977.0\n163.0\n133.0\n31.0\n...\n400.0\n1147.0\n107.0\n455.0\n25.0\n166.0\n239.0\n147.0\n18.0\n13796.0\n\n\n53234\nBrittany\n1997\n19.0\n253.0\n166.0\n138.0\n825.0\n141.0\n111.0\n24.0\n...\n342.0\n966.0\n85.0\n334.0\n19.0\n147.0\n230.0\n102.0\n7.0\n11527.0\n\n\n53235\nBrittany\n1998\n29.0\n190.0\n109.0\n165.0\n735.0\n134.0\n85.0\n22.0\n...\n324.0\n828.0\n75.0\n310.0\n23.0\n121.0\n170.0\n114.0\n11.0\n9843.0\n\n\n53236\nBrittany\n1999\n18.0\n163.0\n97.0\n114.0\n607.0\n135.0\n69.0\n20.0\n...\n265.0\n698.0\n70.0\n246.0\n13.0\n104.0\n126.0\n83.0\n11.0\n7942.0\n\n\n53237\nBrittany\n2000\n14.0\n111.0\n63.0\n88.0\n354.0\n71.0\n33.0\n14.0\n...\n199.0\n432.0\n41.0\n176.0\n9.0\n59.0\n91.0\n71.0\n6.0\n5183.0\n\n\n53238\nBrittany\n2001\n6.0\n92.0\n43.0\n60.0\n223.0\n35.0\n27.0\n9.0\n...\n109.0\n287.0\n21.0\n83.0\n5.0\n33.0\n33.0\n50.0\n0.0\n2915.0\n\n\n53239\nBrittany\n2002\n0.0\n39.0\n28.0\n32.0\n144.0\n32.0\n9.0\n7.0\n...\n66.0\n206.0\n22.0\n70.0\n5.0\n16.0\n30.0\n29.0\n0.0\n1912.0\n\n\n53240\nBrittany\n2003\n0.0\n32.0\n22.0\n25.0\n148.0\n23.0\n16.0\n5.0\n...\n54.0\n177.0\n16.0\n42.0\n0.0\n15.0\n25.0\n19.0\n0.0\n1559.0\n\n\n53241\nBrittany\n2004\n0.0\n31.0\n23.0\n24.0\n139.0\n20.0\n0.0\n5.0\n...\n50.0\n137.0\n16.0\n37.0\n0.0\n14.0\n18.0\n28.0\n6.0\n1323.5\n\n\n53242\nBrittany\n2005\n0.0\n28.0\n18.0\n29.0\n116.0\n24.0\n7.0\n0.0\n...\n45.0\n148.0\n18.0\n35.0\n0.0\n9.0\n17.0\n13.0\n0.0\n1168.0\n\n\n53243\nBrittany\n2006\n0.0\n20.0\n13.0\n20.0\n121.0\n12.0\n6.0\n0.0\n...\n41.0\n103.0\n9.0\n30.0\n0.0\n10.0\n14.0\n9.0\n0.0\n1009.0\n\n\n53244\nBrittany\n2007\n0.0\n14.0\n12.0\n26.0\n126.0\n14.0\n0.0\n0.0\n...\n45.0\n96.0\n7.0\n27.0\n0.0\n14.0\n9.0\n9.0\n0.0\n891.0\n\n\n53245\nBrittany\n2008\n0.0\n15.0\n0.0\n14.0\n102.0\n5.0\n0.0\n0.0\n...\n30.0\n92.0\n9.0\n19.0\n0.0\n9.0\n16.0\n5.0\n0.0\n749.0\n\n\n53246\nBrittany\n2009\n0.0\n7.0\n6.0\n10.0\n105.0\n12.0\n0.0\n0.0\n...\n18.0\n83.0\n6.0\n26.0\n0.0\n10.0\n7.0\n0.0\n0.0\n644.0\n\n\n53247\nBrittany\n2010\n0.0\n9.0\n0.0\n20.0\n116.0\n9.0\n0.0\n0.0\n...\n31.0\n94.0\n11.0\n22.0\n0.0\n8.0\n6.0\n0.0\n0.0\n698.0\n\n\n53248\nBrittany\n2011\n0.0\n12.0\n7.0\n17.0\n109.0\n10.0\n9.0\n0.0\n...\n18.0\n91.0\n6.0\n32.0\n0.0\n7.0\n11.0\n0.0\n0.0\n717.0\n\n\n53249\nBrittany\n2012\n0.0\n12.0\n13.0\n9.0\n137.0\n11.0\n8.0\n6.0\n...\n23.0\n94.0\n6.0\n24.0\n0.0\n7.0\n10.0\n0.0\n0.0\n745.0\n\n\n53250\nBrittany\n2013\n0.0\n13.0\n0.0\n14.0\n110.0\n7.0\n9.0\n5.0\n...\n11.0\n113.0\n7.0\n25.0\n0.0\n12.0\n8.0\n0.0\n0.0\n699.0\n\n\n53251\nBrittany\n2014\n0.0\n11.0\n5.0\n8.0\n112.0\n9.0\n10.0\n0.0\n...\n21.0\n110.0\n8.0\n15.0\n0.0\n10.0\n6.0\n0.0\n0.0\n660.0\n\n\n53252\nBrittany\n2015\n0.0\n9.0\n6.0\n10.0\n109.0\n11.0\n0.0\n0.0\n...\n16.0\n124.0\n9.0\n26.0\n0.0\n9.0\n0.0\n0.0\n0.0\n636.0\n\n\n\n\n48 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names in a single chart. What trends do you notice?\nIt seems to be that ‘Mary’ was the most common name for most of the early 1920’s. Once we get to the 1970’s however, all four names seem to take a steep decline and the doesn’t climb back up. I am lead to assume that as the world became less religious, biblical names became less popular.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nLine chart of the names Mary, Martha, Peter, and Paul\nnew_names_df = df[df['name'].isin(['Mary', 'Martha', 'Peter', 'Paul']) & (df['year'] &gt;= 1920) & (df['year'] &lt;= 2000)]\n\n(ggplot (new_names_df, aes(x='year', y='Total', color='name')) +\ngeom_line() + \n    ggtitle('Mary, Martha, Peter, and Paul Over Time') + xlab('Year') + ylab('Total') + \n    scale_x_continuous(format='d') + \n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nMary, Martha, Peter, and Paul Over Time\n\n\n\n\nMary, Martha, Peter, and Paul table\ndisplay(new_names_df)\n\n\n\n\n\n\nMary, Martha, Peter, and Paul Over Time\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n264124\nMartha\n1920\n11.0\n258.0\n224.0\n21.0\n131.0\n66.0\n55.0\n20.0\n...\n416.0\n418.0\n16.0\n269.0\n11.0\n73.0\n116.0\n172.0\n11.0\n8705.0\n\n\n264125\nMartha\n1921\n0.0\n307.0\n216.0\n18.0\n161.0\n70.0\n57.0\n34.0\n...\n420.0\n478.0\n20.0\n297.0\n13.0\n57.0\n101.0\n204.0\n9.0\n9254.0\n\n\n264126\nMartha\n1922\n9.0\n326.0\n219.0\n23.0\n126.0\n67.0\n56.0\n17.0\n...\n421.0\n501.0\n18.0\n273.0\n14.0\n39.0\n82.0\n225.0\n5.0\n9018.0\n\n\n264127\nMartha\n1923\n0.0\n341.0\n236.0\n27.0\n159.0\n63.0\n38.0\n24.0\n...\n442.0\n233.0\n22.0\n293.0\n11.0\n45.0\n72.0\n210.0\n10.0\n8731.0\n\n\n264128\nMartha\n1924\n0.0\n342.0\n257.0\n39.0\n166.0\n58.0\n49.0\n23.0\n...\n507.0\n487.0\n15.0\n155.0\n15.0\n41.0\n72.0\n196.0\n20.0\n9163.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n303691\nPeter\n1996\n0.0\n22.0\n8.0\n42.0\n501.0\n55.0\n134.0\n22.0\n...\n30.0\n183.0\n27.0\n103.0\n20.0\n109.0\n114.0\n8.0\n0.0\n4069.0\n\n\n303692\nPeter\n1997\n17.0\n19.0\n5.0\n47.0\n476.0\n44.0\n111.0\n25.0\n...\n24.0\n175.0\n27.0\n73.0\n17.0\n74.0\n98.0\n0.0\n0.0\n3821.0\n\n\n303693\nPeter\n1998\n14.0\n21.0\n8.0\n42.0\n471.0\n42.0\n104.0\n14.0\n...\n27.0\n134.0\n41.0\n78.0\n12.0\n98.0\n89.0\n0.0\n0.0\n3377.0\n\n\n303694\nPeter\n1999\n15.0\n24.0\n9.0\n48.0\n402.0\n66.0\n101.0\n23.0\n...\n30.0\n138.0\n26.0\n105.0\n10.0\n77.0\n81.0\n0.0\n0.0\n3430.0\n\n\n303695\nPeter\n2000\n11.0\n14.0\n11.0\n40.0\n379.0\n66.0\n72.0\n20.0\n...\n26.0\n147.0\n24.0\n62.0\n12.0\n68.0\n70.0\n0.0\n0.0\n3137.0\n\n\n\n\n324 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nLooking at the graph it looks as though there was a little rise in the use of the name after 1993. The rise after 1993 is slight and would be hard to conclude that the increase is correlated with the release of “The Sandlot”.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nLine chart of the name Benjamin\nbenjamin_df = df[df['name'] == 'Benjamin']\n\n(ggplot(benjamin_df, aes(x='year', y='Total')) +\n    geom_line(color = 'blue') +\n    geom_vline(xintercept = 1993, linetype = 'dashed', color = 'red') +\n    geom_text(x = 1992, y = 15000, label = 'Sandlot was Released', hjust = 1) +\n    geom_text(x = 1960, y = 14200, label = '(1993)', hjust = 0) +\n    ggtitle('Benjamin from The Sandlot') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nBenjamin from The Sandlot\n\n\n\n\nBenjamin from The Sandlot table\ndisplay(benjamin_df)\n\n\n\n\n\n\nBenjamin from The Sandlot\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n41526\nBenjamin\n1910\n0.0\n13.0\n10.0\n5.0\n5.0\n0.0\n7.0\n0.0\n...\n0.0\n22.0\n0.0\n23.0\n0.0\n0.0\n5.0\n0.0\n0.0\n497.0\n\n\n41527\nBenjamin\n1911\n0.0\n14.0\n10.0\n0.0\n7.0\n7.0\n22.0\n7.0\n...\n7.0\n16.0\n0.0\n26.0\n0.0\n0.0\n7.0\n0.0\n0.0\n645.0\n\n\n41528\nBenjamin\n1912\n0.0\n25.0\n13.0\n0.0\n18.0\n7.0\n25.0\n7.0\n...\n24.0\n41.0\n0.0\n47.0\n0.0\n7.0\n19.0\n13.0\n0.0\n1245.0\n\n\n41529\nBenjamin\n1913\n0.0\n29.0\n19.0\n0.0\n16.0\n9.0\n31.0\n15.0\n...\n22.0\n57.0\n5.0\n49.0\n0.0\n14.0\n19.0\n15.0\n0.0\n1451.0\n\n\n41530\nBenjamin\n1914\n0.0\n50.0\n31.0\n5.0\n29.0\n12.0\n37.0\n8.0\n...\n35.0\n71.0\n0.0\n59.0\n0.0\n11.0\n21.0\n11.0\n0.0\n1802.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41627\nBenjamin\n2011\n23.0\n165.0\n91.0\n281.0\n1592.0\n247.0\n157.0\n57.0\n...\n244.0\n988.0\n193.0\n403.0\n33.0\n308.0\n271.0\n49.0\n17.0\n12743.0\n\n\n41628\nBenjamin\n2012\n31.0\n141.0\n87.0\n264.0\n1676.0\n212.0\n157.0\n52.0\n...\n252.0\n1035.0\n158.0\n364.0\n29.0\n336.0\n238.0\n57.0\n20.0\n12411.5\n\n\n41629\nBenjamin\n2013\n34.0\n125.0\n81.0\n300.0\n1845.0\n242.0\n145.0\n37.0\n...\n279.0\n1079.0\n178.0\n363.0\n21.0\n333.0\n262.0\n56.0\n18.0\n13460.0\n\n\n41630\nBenjamin\n2014\n36.0\n128.0\n93.0\n270.0\n1873.0\n252.0\n179.0\n64.0\n...\n249.0\n1152.0\n153.0\n352.0\n26.0\n370.0\n214.0\n50.0\n25.0\n13761.0\n\n\n41631\nBenjamin\n2015\n34.0\n151.0\n110.0\n275.0\n1809.0\n248.0\n178.0\n57.0\n...\n239.0\n1154.0\n168.0\n383.0\n21.0\n356.0\n232.0\n72.0\n29.0\n13608.0\n\n\n\n\n106 rows × 54 columns\n\n\n\n–&gt; –&gt;\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 1"
    ]
  },
  {
    "objectID": "Competition/project4.html",
    "href": "Competition/project4.html",
    "title": "Client Report - Can You Predict That?",
    "section": "",
    "text": "Using the power of advanced machine learning, we developed a model that successfully predicted whether a home was built before or after 1980. Through model tuning we were able to get the accuracy of our model’s prediction at the 90% threshold making our model relatively reliable in predicting the year a home was built. We first Identified possible relationships between homes built before and after 1980. Our model found that one story architecture, garage type, and quality ratings of C were most correlated with homes from built before 1980. Our model was then explained and justified using a classification report, confusion matrix and an ROC curve confirming that our model was accurate and reliable. I also built a model to predict the year a home was built using the “GradientBoostingRegressor.” This model was able to successfully predict 89% of its results. For Justification we look at Mean Absolute Error (MAE), Mean Squared Error (MSE), R-Squared value, and a confusion matrix. \n\n\nRead and format project data\n# Include and execute your code here\n\ndwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "Competition/project4.html#elevator-pitch",
    "href": "Competition/project4.html#elevator-pitch",
    "title": "Client Report - Can You Predict That?",
    "section": "",
    "text": "Using the power of advanced machine learning, we developed a model that successfully predicted whether a home was built before or after 1980. Through model tuning we were able to get the accuracy of our model’s prediction at the 90% threshold making our model relatively reliable in predicting the year a home was built. We first Identified possible relationships between homes built before and after 1980. Our model found that one story architecture, garage type, and quality ratings of C were most correlated with homes from built before 1980. Our model was then explained and justified using a classification report, confusion matrix and an ROC curve confirming that our model was accurate and reliable. I also built a model to predict the year a home was built using the “GradientBoostingRegressor.” This model was able to successfully predict 89% of its results. For Justification we look at Mean Absolute Error (MAE), Mean Squared Error (MSE), R-Squared value, and a confusion matrix. \n\n\nRead and format project data\n# Include and execute your code here\n\ndwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "Competition/project4.html#questiontask-1",
    "href": "Competition/project4.html#questiontask-1",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np \nfrom lets_plot import * \nLetsPlot.setup_html(isolated_frame=True)\n\ndf_subset = dwellings_ml.filter(\n    ['livearea', 'finbsmnt', 'basement', \n    'yearbuilt', 'nocars', 'numbdrm', 'numbaths', 'before1980',\n    'stories', 'yrbuilt']).sample(500)\n\ndf_subset = df_subset.dropna(subset=['livearea', 'finbsmnt', 'before1980', 'basement', 'numbaths', 'numbdrm'])\n\ndf_subset['before1980'] = df_subset['before1980'].astype(bool)\n\n\nThis scatter plot shows us the possible relationship between the number of bedrooms built in the house and the total living space, or square footage, of the home. There seems to be a linear relationship between house size and number of rooms built. As the number of rooms increase, so does the total livable square footage.\n\n\nLiving Area vs Number of Bedrooms\n# Include and execute your code here\nc_1 = ggplot(df_subset, aes(x='livearea', y='numbdrm', color='before1980')) + geom_point(alpha=0.5) + ggtitle(\"Living Area vs Number of Bedrooms\") + xlab(\"Living Area\") + ylab(\"Number of Bedrooms\") + scale_color_manual(name='Built Before 1980', labels=['Yes (Blue)', 'No (Red)'], values=['blue', 'red']) + theme(legend_position='right') \n\nc_1.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nLiving Area vs Number of Bedrooms\n\n\nThis box and whisker plot shows is the relationship between basement sizes on homes built before 1980 and after 1980. From this we can see that homes that were built before 1980 had slightly larger mean basement size than homes built after 1980.\n\n\nFinished Basement Area by Before1980\n# Include and execute your code here\n\nc_2 = ggplot(df_subset, aes(x='before1980', y='finbsmnt', fill='before1980')) + geom_boxplot() + ggtitle('Finished Basement Area by Before1980') + xlab('Built Before 1980') + ylab('Finished Basement Area') + scale_fill_discrete(name=\"Built Before 1980\")\nc_2.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThis stacked bar chart tells us the relationship between the number of bathrooms built in a house based on if the house was built before or after 1980. It looks like the majority of houses built before 1980 had two bathrooms. \n\n\nNumber of Bathrooms by Before1980\n# Include and execute your code here\n\nc_3 = ggplot(df_subset, aes(x='numbaths', fill='before1980')) + geom_bar(position='stack') + ggtitle('Number of Bathrooms by Before1980') + xlab('Number of Bathrooms') + ylab('Count') + scale_fill_discrete(name=\"Built Before 1980\")\nc_3.show()",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "Competition/project4.html#questiontask-2",
    "href": "Competition/project4.html#questiontask-2",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\nI ended up using the DecisionTreeClassifier to create my classification model. I settled on this because this led to me getting the highest accuracy in the quickest time. I tried to use the Gradient Boosting, but it ended up taking longer to get the results. I also tried to add the following parameters: max_depth, min_sample_split, and min_samples_leaf. I found that this took a lot longer to get the results and it did not improve the accuracy rating at all when compared to the DecisionTreeClassifier with no parameters. I was able to achieve a 90% accuracy rating.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\n\n\n\n\nClassification Report\nX = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny = dwellings_ml.filter(regex = \"before1980\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .35, random_state = 76)\n\nmodel_1 = tree.DecisionTreeClassifier(random_state=76, max_depth=10, min_samples_split=10, min_samples_leaf=2)\nmodel_1.fit(X_train, y_train)\n\ny_pred = model_1.predict(X_test)\n\nprint('\\nTest Accuracy:', metrics.accuracy_score(y_test, y_pred))\nprint('\\nClassification Report: \\n', metrics.classification_report(y_test, y_pred))\n\n\n\nTest Accuracy: 0.9007481296758105\n\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       0.86      0.87      0.87      2980\n           1       0.92      0.92      0.92      5040\n\n    accuracy                           0.90      8020\n   macro avg       0.89      0.89      0.89      8020\nweighted avg       0.90      0.90      0.90      8020",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "Competition/project4.html#questiontask-3",
    "href": "Competition/project4.html#questiontask-3",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a feature importance chart and a description of the features.\nThe most important features identified in my model were “arcstyle_ONE_STORY”, “gartype_Att”, and “quality_C.” The model puts the most importance on how many stories the house is using “arcstyle_ONE_STORY.” One story homes were more popular before 1980 so there were more built of these types of homes. The second most important feature my model used was whether or not the house had an attached, or detached garage, “gartype_Att.” Most of the houses built before 1980 had detached garages. The third greatest importance the model used was the quality of the home from “quality_C.” Homes with this quality rating were more likely to be older homes. The model put these three fields together to best predict if the house was built before or after 1980.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nFeature Importance Chart\n# Include and execute your code here\nimportance = model_1.feature_importances_\nfeature_names = X.columns\n\nfeature_importance_df = pd.DataFrame({'feature': feature_names, 'Importance': importance})\nfeature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n\nfeature_importance_df_top = feature_importance_df.head(10)\n\nc_4 = ggplot(feature_importance_df_top, aes(x='feature', y='Importance', fill= 'Importance')) + geom_bar(stat='identity') + theme(axis_text_x=element_text(angle=90, hjust=1)) + ggtitle('Feature Importance') + xlab('Feature') + ylab('Importance')\nc_4.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nFeature Importance",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "Competition/project4.html#questiontask-4",
    "href": "Competition/project4.html#questiontask-4",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\nTo justify the results and quality of my model, I chose to look at the following three metrics: 1 - Classification report containing the accuracy, 2 - Confusion matrix, and 3 - The Receiver-operating characteristic (ROC) curve. Based on the results of this test, I can conclude that the quality of my model is strong and can be relied upon with an acceptable accuracy and true positive rate. Under each of the reports I will explain the significance and how to interpret the results.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nPerformance Metrics\n# Include and execute your code here\nprint('\\nTest Accuracy:', metrics.accuracy_score(y_test, y_pred))\nprint('\\nClassification Report: \\n', metrics.classification_report(y_test, y_pred))\n\n\n\nTest Accuracy: 0.9007481296758105\n\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       0.86      0.87      0.87      2980\n           1       0.92      0.92      0.92      5040\n\n    accuracy                           0.90      8020\n   macro avg       0.89      0.89      0.89      8020\nweighted avg       0.90      0.90      0.90      8020\n\n\n\nThe classification report and accuracy measure the percentage of correctly classified predictions compared to the total results. The higher the accuracy percentage is the more accurate the model is. A high accuracy rating is anything greater than or equal to 90% (0.90). The precision measures the percentage of total correct guesses divided by the number of true positives + false positives. For “0” this means that 86% of the home predicted to be built after 1980 are correct, and for the “1” 96% of the homes predicted to be built before 1980 are correct. Recall measures the total true positives divided by true positives + false negatives. For “0”, the model correctly identified 87% of the homes built after 1980 and for “1” correctly identified 92% of the homes built before 1980. The F-1 Score is calculated as 2 * (Precision * Recall / Precision + Recall). The F-1 score provides a balanced metric to look at our model performance. Our model is slightly stronger at predicting if a home was built before 1980 than after 1980 but overall reliable at predicting if it was built before or after 1980.\n\n\nConfusion Matrix\n# Include and execute your code here\n\nconf_matrix = metrics.confusion_matrix(y_test, y_pred)\nconf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted: (After 1980)', 'Predicted: (Before 1980)'], index=['Actual: (After 1980)', 'Actual: (Before 1980)'])\n\nconf_matrix_plot = ggplot(conf_matrix_df.reset_index().melt(id_vars='index'), aes(x='index', y='variable', fill='value')) + geom_tile() + geom_text(aes(label='value'), color='black', size=6) + scale_fill_brewer(palette='RdYlBu') + ggtitle('Confusion Matrix') + xlab('Predicted') + ylab('Actual')\n\nconf_matrix_plot.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThe confusion matrix is a table that is used to describe the performance of a classification model. The matrix shows the number of true positives (4630), false positives (386), true negatives (2594), and false negatives (410). From the confusion matrix we can see that our model had relatively few false positives / negatives in relation to the correct predictions. This proves how well my model was at predicting if a home was built before or after 1980.\n\n\nROC Curve\n# Include and execute your code here\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\n\nroc_df = pd.DataFrame({'FPR': fpr, 'TPR': tpr, 'Thresholds': thresholds})\n\nroc_chart = ggplot(roc_df, aes(x='FPR', y='TPR')) + geom_line(color='blue') + ggtitle('ROC Curve') + xlab('False Positive Rate') + ylab('True Positive Rate')\n\nroc_chart.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThe ROC curve is a graphical representation of the true positive rate against the false positive rate. The closer the curve is to the top left corner, the better the model is. The area under the curve (AUC) is used to measure the performance of the model. The higher the AUC, the better the model is. Our model had an AUC of 0.92 which shows us it performed very well at predicting when a house was built.",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "Competition/project4.html#stretchtask-5",
    "href": "Competition/project4.html#stretchtask-5",
    "title": "Client Report - Can You Predict That?",
    "section": "STRETCH|TASK 5",
    "text": "STRETCH|TASK 5\nCan you build a model that predicts the year a house was built? Explain the model and the evaluation metrics you would use to determine if the model is good.\nFor the model I chose to use the “GradientBoostingRegressor” because of its ability to sift through complex, non-linear relationships to get to the target prediction. In this case our goal was to predict what year a house was built which requires the model to look through the data and attempt to find patterns that could help predict when a house was built. The “GradientBoostingRegressor” is also very good at not over filtering the data like other models. This model type also very flexible and forgiving when it comes to tuning the model. Below are the figures and print outs I used to evaluate the accuracy of my model.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nRead and format data\n# Include and execute your code here\n\nX_5 = dwellings_ml.drop(dwellings_ml.filter(regex = 'yrbuilt|parcel').columns, axis = 1)\ny_5 = dwellings_ml[\"yrbuilt\"]\n\nX_5 = X_5.dropna()\ny_5 = y_5[X_5.index]\n\nX_train, X_test, y5_train, y5_test = train_test_split(X_5, y_5, test_size = .35, random_state = 76)\n\nrf_model = GradientBoostingRegressor(random_state=76, n_estimators=500, learning_rate=0.1, max_depth=15, min_samples_split=10, min_samples_leaf=2)\nrf_model.fit(X_train, y5_train)\n\ny5_pred = rf_model.predict(X_test)\n\nmae = metrics.mean_absolute_error(y5_test, y5_pred)\nmse = metrics.mean_squared_error(y5_test, y5_pred)\nr2 = metrics.r2_score(y5_test, y5_pred)\n\nprint(f'Mean Absolute Error (MAE): {mae:.2f}')\nprint(f'Mean Squared Error (MSE): {mse:.2f}')\nprint(f'R-Squared (R^2): {r2:.2f}')\n\n\nMean Absolute Error (MAE): 7.49\nMean Squared Error (MSE): 155.45\nR-Squared (R^2): 0.89\n\n\n\n\nClassification Report\n# Include and execute your code here\n\nbins = [0, 1980, 1995, 2010, 2023]\nlables = ['Pre 1980', '1980-1995', '1996-2010', 'Post 2010']\n\ny5_test_bins = pd.cut(y5_test, bins=bins, labels=lables)\ny5_pred_bins = pd.cut(y5_pred, bins=bins, labels=lables)\n\nprint('\\nTest Accuracy:', metrics.accuracy_score(y5_test_bins, y5_pred_bins))\nprint('\\nClassification Report: \\n', metrics.classification_report(y5_test_bins, y5_pred_bins))\n\n\n\nTest Accuracy: 0.9337905236907731\n\nClassification Report: \n               precision    recall  f1-score   support\n\n   1980-1995       0.67      0.68      0.68       418\n   1996-2010       0.84      0.91      0.87      1832\n   Post 2010       0.88      0.73      0.80       671\n    Pre 1980       1.00      0.99      0.99      5099\n\n    accuracy                           0.93      8020\n   macro avg       0.85      0.83      0.84      8020\nweighted avg       0.94      0.93      0.93      8020\n\n\n\nThe Mean Absolute Error (MAE) tells us that on average, our model’s predictions are off by only 7.5 years. With regression tasks like this a MAE of 10 years is typically considered very good. The Mean Squared Error (MSE) is similar to the MAE, but the MSE penalizes the model’s larger errors heavier than smaller errors. Our score of 156.62 equates to about 12.5 years of difference with a range of years that spans over 100 years. When we compare our R-Squared result with the Accuracy test below we can see that overall, our model is right within the acceptable margin of error.\n\n\nConfusion Matrix\n# Include and execute your code here\n\nconf5_matrix = metrics.confusion_matrix(y5_test_bins, y5_pred_bins, labels=lables)\nconf5_matrix_df = pd.DataFrame(conf5_matrix, columns=['Pred:(Pre 80)', 'Pred:(80-95)', 'Pred:(96-10)', 'Pred:(Post 10)'], index=['Actl:(Pre 80)', 'Actl:(80-95)', 'Actl:(96-10)', 'Actl:(Post 10)'])\n\nconf5_matrix_norm = conf5_matrix.astype('float') / conf5_matrix.sum(axis=1)[:, np.newaxis]\n\nconf5_matrix_norm_df = pd.DataFrame(conf5_matrix_norm, columns=['Pred:(Pre 80)', 'Pred:(80-95)', 'Pred:(96-10)', 'Pred:(Post 10)'], index=['Actl:(Pre 80)', 'Actl:(80-95)', 'Actl:(96-10)', 'Actl:(Post 10)'])\n\nconf5_matrix_norm_df_melted = conf5_matrix_norm_df.reset_index().melt(id_vars='index', var_name='Predicted', value_name='Accuracy')\n\nlbl_plot = (\n  pd.DataFrame(100 * conf5_matrix_norm)\n  .stack()\n  .reset_index(drop=True)\n  .round(1)\n  .astype(str) + '%'\n)\n\nconf5_matrix_plot = ggplot(conf5_matrix_norm_df_melted, aes(x='Predicted', y='index', fill='Accuracy')) + geom_tile() + geom_text(aes(label=lbl_plot), color='white', size=6) + scale_fill_gradient(low='red', high='blue', name='Accuracy') + ggtitle('Confusion Matrix') + xlab('Predicted') + ylab('Actual') + theme(axis_text_x=element_text(angle=90, hjust=1))\n\nconf5_matrix_plot.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nConfusion Matrix\n\n\nThe confusion matrix is a table that is used to describe the performance of a classification model. The matrix shows the number of true positives, false positives, true negatives, and false negatives. From the confusion matrix we can see that our model had relatively few false positives / negatives in relation to the correct predictions. This proves how well my model was at predicting when a house was built.",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "Competition/project5.html",
    "href": "Competition/project5.html",
    "title": "Client Report - The War with Star Wars",
    "section": "",
    "text": "After cleansing the Star Wars survey data for better handling. The cleaning consisted of shorting column names, handling missing values, and flited the raw data to extract the correct and useable survey responses. I then successfully recreated two of the visuals featured on the article provided to validate our cleaned data. The visual output and measurements were identical to those of the article. After this we then used the cleaned data to create a machine learning model to predict if someone makes over $50,000 a year based on their survey responses. Our final model was 65% accurate at predicting if a participant makes over $50k a year. To provide extra validation we then successfully recreated one more visual from the article.\n\n\nRead and format project data\nurl = 'https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv'\n\ndf_cols = pd.read_csv(url, encoding = \"ISO-8859-1\", nrows = 1).melt()\ndf = pd.read_csv(url, encoding = \"ISO-8859-1\", skiprows =2, header = None )",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "Competition/project5.html#elevator-pitch",
    "href": "Competition/project5.html#elevator-pitch",
    "title": "Client Report - The War with Star Wars",
    "section": "",
    "text": "After cleansing the Star Wars survey data for better handling. The cleaning consisted of shorting column names, handling missing values, and flited the raw data to extract the correct and useable survey responses. I then successfully recreated two of the visuals featured on the article provided to validate our cleaned data. The visual output and measurements were identical to those of the article. After this we then used the cleaned data to create a machine learning model to predict if someone makes over $50,000 a year based on their survey responses. Our final model was 65% accurate at predicting if a participant makes over $50k a year. To provide extra validation we then successfully recreated one more visual from the article.\n\n\nRead and format project data\nurl = 'https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv'\n\ndf_cols = pd.read_csv(url, encoding = \"ISO-8859-1\", nrows = 1).melt()\ndf = pd.read_csv(url, encoding = \"ISO-8859-1\", skiprows =2, header = None )",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "Competition/project5.html#questiontask-1",
    "href": "Competition/project5.html#questiontask-1",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nShorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.\nThe column names were long and very difficult to work with what we wanted to accomplish in this product. I cleaned up some of the data and shorted the column names to a more unified method. The new column names can be seen below.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\n\n\n\n\nTable of column names\n# Include and execute your code here\n\nvar_replace = {\n    'Which of the following Star Wars films have you seen\\\\? Please select all that apply\\\\.':'seen',\n    'Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.':'rank',\n    'Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.':'view',\n    'Do you consider yourself to be a fan of the Star Trek franchise\\\\?':'star_trek_fan',\n    'Do you consider yourself to be a fan of the Expanded Universe\\\\?\\x8cæ':'expanded_fan',\n    'Are you familiar with the Expanded Universe\\\\?':'know_expanded',\n    'Have you seen any of the 6 films in the Star Wars franchise\\\\?':'seen_any',\n    'Do you consider yourself to be a fan of the Star Wars film franchise\\\\?':'star_wars_fans',\n    'Which character shot first\\\\?':'shot_first',\n    'Unnamed: \\d{1,2}':np.nan,\n    ' ':'_',\n}\n\nval_replace = {\n    'Response':'',\n    'Star Wars: Episode ':'',\n    ' ':'_'\n}\n\ndf_cols_new = (df_cols\n    .assign(\n        val_replace = lambda x:  x.value.str.strip().replace(val_replace, regex=True),\n        var_replace = lambda x: x.variable.str.strip().replace(var_replace, regex=True)\n    )\n    .fillna(method = 'ffill')\n    .fillna(value = \"\")\n    .assign(column_names = lambda x: x.var_replace.str.cat(x.val_replace, sep = \"_\").str.strip('__').str.lower())\n    )\n\ndf.columns = df_cols_new.column_names.to_list()\n\ndf.columns\n\n\nIndex(['respondentid', 'seen_any', 'star_wars_fans',\n       'seen_i__the_phantom_menace', 'seen_ii__attack_of_the_clones',\n       'seen_iii__revenge_of_the_sith', 'seen_iv__a_new_hope',\n       'seen_v_the_empire_strikes_back', 'seen_vi_return_of_the_jedi',\n       'rank_i__the_phantom_menace', 'rank_ii__attack_of_the_clones',\n       'rank_iii__revenge_of_the_sith', 'rank_iv__a_new_hope',\n       'rank_v_the_empire_strikes_back', 'rank_vi_return_of_the_jedi',\n       'view_han_solo', 'view_luke_skywalker', 'view_princess_leia_organa',\n       'view_anakin_skywalker', 'view_obi_wan_kenobi',\n       'view_emperor_palpatine', 'view_darth_vader', 'view_lando_calrissian',\n       'view_boba_fett', 'view_c-3p0', 'view_r2_d2', 'view_jar_jar_binks',\n       'view_padme_amidala', 'view_yoda', 'shot_first', 'know_expanded',\n       'expanded_fan', 'star_trek_fan', 'gender', 'age', 'household_income',\n       'education', 'location_(census_region)'],\n      dtype='object')\nColumn Names\n\n\nBelow is an example of what the column names were before and what I changed them to. The “variable” column represents what the original column name was. The “value” column original values in response to the question for the column. The “val_replace” and “var_replace” columns represent the cleaned values and variables used to replace the older, harder to use data and names. The last column “column_names” represents the cleaned and shortened column names we used for easier computer handling.\n\n\nTable of column names\n# Include and execute your code here\ndf_cols_new.head()\n\n\n\n\n\n\nColumn Names\n\n\n\nvariable\nvalue\nval_replace\nvar_replace\ncolumn_names\n\n\n\n\n0\nRespondentID\n\n\nRespondentID\nrespondentid\n\n\n1\nHave you seen any of the 6 films in the Star W...\nResponse\n\nseen_any\nseen_any\n\n\n2\nDo you consider yourself to be a fan of the St...\nResponse\n\nstar_wars_fans\nstar_wars_fans\n\n\n3\nWhich of the following Star Wars films have yo...\nStar Wars: Episode I The Phantom Menace\nI__The_Phantom_Menace\nseen\nseen_i__the_phantom_menace\n\n\n4\nUnnamed: 4\nStar Wars: Episode II Attack of the Clones\nII__Attack_of_the_Clones\nseen\nseen_ii__attack_of_the_clones",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "Competition/project5.html#questiontask-2",
    "href": "Competition/project5.html#questiontask-2",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nClean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made.\nIn order to properly work with machine learning, we have to do some more data cleaning. The following output and data represents the steps I tool to clean the data.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\n\n\nA. Filter the dataset to respondents that have seen at least one film. I also altered this code to filter out any responses that said they had seen any Star Wars movie but did not mark what movies they had seen.\n\n\nFilter data\n# Include and execute your code here\n\nseen_columns = [\n    'seen_i__the_phantom_menace',\n    'seen_ii__attack_of_the_clones',\n    'seen_iii__revenge_of_the_sith',\n    'seen_iv__a_new_hope',\n    'seen_v_the_empire_strikes_back',\n    'seen_vi_return_of_the_jedi',\n]\n\ndf_filtered = df[df['seen_any'] == 'Yes']\n\nfor col in seen_columns:\n    df_filtered[col] = df_filtered[col].notna()\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nage\nhousehold_income\neducation\nlocation_(census_region)\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\n18-29\nNaN\nHigh school degree\nSouth Atlantic\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n18-29\n$0 - $24,999\nHigh school degree\nWest North Central\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n18-29\n$25,000 - $49,999\nBachelor degree\nMiddle Atlantic\n\n\n\n\n5 rows × 38 columns\n\nSeen Any Star Wars Movie\n\n\nB. Create a new column that converts the age ranges to a single number. Drop the age range categorical column\n\n\nChange Age Field\n# Include and execute your code here\n\nage_map = {\n    '18-29': 24,\n    '30-44': 37,\n    '45-60': 52,\n    '&gt;60': 65\n}\n\ndf_filtered['age_num'] = df_filtered['age'].map(age_map)\ndf_filtered.drop('age', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\nAge Field\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nhousehold_income\neducation\nlocation_(census_region)\nage_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nNaN\nHigh school degree\nSouth Atlantic\n24.0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n$0 - $24,999\nHigh school degree\nWest North Central\n24.0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n24.0\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n24.0\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n$25,000 - $49,999\nBachelor degree\nMiddle Atlantic\n24.0\n\n\n\n\n5 rows × 38 columns\n\n\n\nC. Create a new column that converts the education groupings to a single number. Drop the school categorical column\n\n\nFilter data\n# Include and execute your code here\n\neducation_map = {\n  'Less than high school degree': 9,\n  'High school degree': 12,\n  'Some college or Associate degree': 14,\n  'Bachelor degree': 16,\n  'Graduate degree': 20\n}\n\ndf_filtered['education_num'] = df_filtered['education'].map(education_map)\ndf_filtered.drop('education', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nhousehold_income\nlocation_(census_region)\nage_num\neducation_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nNaN\nSouth Atlantic\n24.0\n12.0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n$0 - $24,999\nWest North Central\n24.0\n12.0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n$100,000 - $149,999\nWest North Central\n24.0\n14.0\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n$100,000 - $149,999\nWest North Central\n24.0\n14.0\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n$25,000 - $49,999\nMiddle Atlantic\n24.0\n16.0\n\n\n\n\n5 rows × 38 columns\n\nEducation Field\n\n\nD. Create a new column that converts the income ranges to a single number. Drop the income range categorical column\n\n\nFilter data\n# Include and execute your code here\n\ndf_filtered['household_income'] = df_filtered['household_income'].fillna('unknown')\n\nincome_map = {\n  '$0 - $24,999': 12500,\n  '$25,000 - $49,999': 37500,\n  '$50,000 - $99,999': 75000,\n  '$100,000 - $149,999': 125000,\n  '$150,000+': 150000,\n  'unknown': 0\n}\n\ndf_filtered['income_num'] = df_filtered['household_income'].map(income_map)\ndf_filtered.drop('household_income', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nlocation_(census_region)\nage_num\neducation_num\nincome_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nSouth Atlantic\n24.0\n12.0\n0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\nWest North Central\n24.0\n12.0\n12500\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\nWest North Central\n24.0\n14.0\n125000\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\nWest North Central\n24.0\n14.0\n125000\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\nMiddle Atlantic\n24.0\n16.0\n37500\n\n\n\n\n5 rows × 38 columns\n\nIncome Field\n\n\nE. Create your target (also known as “y” or “label”) column based on the new income range column\n\n\nFilter data\n# Include and execute your code here\n\ndf_filtered['income_over_50k'] = (df_filtered['income_num'] &gt; 50000).astype(int)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nlocation_(census_region)\nage_num\neducation_num\nincome_num\nincome_over_50k\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nI don't understand this question\nYes\nNo\nNo\nMale\nSouth Atlantic\n24.0\n12.0\n0\n0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nI don't understand this question\nNo\nNaN\nNo\nMale\nWest North Central\n24.0\n12.0\n12500\n0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nI don't understand this question\nNo\nNaN\nYes\nMale\nWest North Central\n24.0\n14.0\n125000\n1\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nGreedo\nYes\nNo\nNo\nMale\nWest North Central\n24.0\n14.0\n125000\n1\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nHan\nYes\nNo\nYes\nMale\nMiddle Atlantic\n24.0\n16.0\n37500\n0\n\n\n\n\n5 rows × 39 columns\n\nIncome Over 50k\n\n\nF. One-hot encode all remaining categorical columns\n\n\nFilter data\n# Include and execute your code here\n\ndf_encoded = pd.get_dummies(df_filtered, drop_first=True)\n\ndf_encoded.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\nrank_ii__attack_of_the_clones\nrank_iii__revenge_of_the_sith\n...\nstar_trek_fan_Yes\ngender_Male\nlocation_(census_region)_East South Central\nlocation_(census_region)_Middle Atlantic\nlocation_(census_region)_Mountain\nlocation_(census_region)_New England\nlocation_(census_region)_Pacific\nlocation_(census_region)_South Atlantic\nlocation_(census_region)_West North Central\nlocation_(census_region)_West South Central\n\n\n\n\n0\n3292879998\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n2.0\n1.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2\n3292765271\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n2.0\n3.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n3292763116\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n6.0\n1.0\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\n3292731220\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n4.0\n6.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n5\n3292719380\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n4.0\n3.0\n...\nTrue\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 102 columns\n\nOne-hot encoding",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "Competition/project5.html#questiontask-3",
    "href": "Competition/project5.html#questiontask-3",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nValidate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article\nI recreated the following visuals from the article to validate our data clensing: 1. “Which ‘Star Wars’ Movies Have You Seen?” and 2. “Who Shot First?”\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWhich Star Wars Movies Have You Seen?\n# Include and execute your code here\n\nseen_columns = [\n    'seen_i__the_phantom_menace',\n    'seen_ii__attack_of_the_clones',\n    'seen_iii__revenge_of_the_sith',\n    'seen_iv__a_new_hope',\n    'seen_v_the_empire_strikes_back',\n    'seen_vi_return_of_the_jedi',\n]\n\nmovie_titles = [\n    \"The Phantom Menace\",\n    \"Attack of the Clones\",\n    \"Revenge of the Sith\",\n    \"A New Hope\",\n    \"The Empire Strikes Back\",\n    \"Return of the Jedi\",\n]\n\ngood_resp = df_filtered[seen_columns].sum(axis=1) &gt; 0\n\ntotal_seen_any = good_resp.sum()\nprob_seen = [(df_filtered[col].sum() / total_seen_any) for col in seen_columns]\n\n\ndf_seen = pd.DataFrame({\n  'movie': movie_titles,\n  'probability': prob_seen\n})\n\np = (ggplot(df_seen, aes(x='movie', y='probability')) + geom_bar(stat='identity', fill='blue') + labs(title='Which Star Wars Movies Have You Seen?', subtitle='Percentage of respondents who have seen each film', x='Percentage of Respondents', y='Star Wars Movies') + theme(axis_text_x=element_text(angle=45, hjust=1)))\n\np.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nWhich Star Wars Movies Have You Seen?\n\n\n\n\nWho Shot First?\n# Include and execute your code here\n\nshot_first_counts = df_filtered['shot_first'].value_counts()\n\nshot_first_percent = (shot_first_counts / shot_first_counts.sum()) * 100\n\ndf_shot_first = pd.DataFrame({\n  'response': shot_first_percent.index,\n  'percentage': shot_first_percent.values\n})\n\ndf_shot_first['response'] = df_shot_first['response'].replace({\"I don't understand this question\": \"Don't Understand\"})\ndf_shot_first['response'] = pd.Categorical(df_shot_first['response'], categories=['Don\\'t Understand', 'Greedo', 'Han'], ordered=True)\ndf_shot_first['percent_label'] = df_shot_first['percentage'].round(0).astype(int).astype(str) + '%'\n\n\np2 = (ggplot(df_shot_first, aes(x='response', y='percentage')) + geom_bar(stat='identity', fill='blue') + geom_text(aes(label='percent_label'), nudge_y=2, size=10) + labs(title='Who Shot First?', subtitle='According to 834 respondents', x='Response', y='Percentage') + coord_flip() + theme(axis_text_x=element_text(hjust=1)))\n\np2.show()",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "Competition/project5.html#questiontask-4",
    "href": "Competition/project5.html#questiontask-4",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nBuild a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.\nFor my model, I chose to use all the columns in the DataFrame except any of the columns that would directly give up the answer (e.g. household_income, income_num, location, and income_over_50k). After I filtered out any undesirable data, I split the data into training and testing sets, and trained the model to make predictions on if a person makes more or less than $50k a year. I then ran a accuracy score and classification report to display how successful my model was. The accuracy of my model 0.655 using the “RandomForestClassifier”. This means that our model can predict if a person makes over or under $50k with a 67% accuracy.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n\n\n\nRandom Forest Model\n# Include and execute your code here\n\nX_pred = df_encoded.drop(columns=df_encoded.filter(regex=(\"(household_income|income_num|income_over_50k|location_\\(census_region\\)_East South Central|location_\\(census_region\\)_Middle Atlantic|location_\\(census_region\\)_Mountain|location_\\(census_region\\)_New England|location_\\(census_region\\)_Pacific|location_\\(census_region\\)_South Atlantic|location_\\(census_region\\)_West North Central|location_\\(census_region\\)_West South Central|respondentid)\")).columns)\ny_pred = df_encoded['income_over_50k']\n\nX_train, X_test, y_train, y_test = train_test_split(X_pred, y_pred, test_size=0.35, random_state=76)\n\nrf = RandomForestClassifier(n_estimators=300 ,random_state=76, max_depth=20, min_samples_split=2)\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\nprint('Accuracy:', accuracy_score(y_test, y_pred))\nprint('\\nClassification Report:\\n', classification_report(y_test, y_pred, target_names=['Under 50k', 'Over 50k']))\n\n\nAccuracy: 0.6432926829268293\n\nClassification Report:\n               precision    recall  f1-score   support\n\n   Under 50k       0.69      0.64      0.67       183\n    Over 50k       0.59      0.64      0.61       145\n\n    accuracy                           0.64       328\n   macro avg       0.64      0.64      0.64       328\nweighted avg       0.65      0.64      0.64       328",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "Competition/project5.html#stretchtask-5",
    "href": "Competition/project5.html#stretchtask-5",
    "title": "Client Report - The War with Star Wars",
    "section": "STRETCH|TASK 5",
    "text": "STRETCH|TASK 5\nValidate the data provided on GitHub lines up with the article by recreating a 3rd visual from the article.\nFor this stretch challenge we were asked to recreate one more visual from the article to double check the validity of our data. I chose to recreate the “What’s the Best ‘Star Wars’ Movie?” In this visual we first had to filter the data further to those who have seen all the Star Wars movies. We then had to calculate and display the share of respondents who rated each fil as their favorite.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWhat is the best Star Wars movie?\n# Include and execute your code here\n\nrank_columns = [\n    'rank_i__the_phantom_menace',\n    'rank_ii__attack_of_the_clones',\n    'rank_iii__revenge_of_the_sith',\n    'rank_iv__a_new_hope',\n    'rank_v_the_empire_strikes_back',\n    'rank_vi_return_of_the_jedi',\n]\n\nmovie_titles = [\n    \"The Phantom Menace\",\n    \"Attack of the Clones\",\n    \"Revenge of the Sith\",\n    \"A New Hope\",\n    \"The Empire Strikes Back\",\n    \"Return of the Jedi\",\n]\n\nseen_all = df_filtered[seen_columns].all(axis=1)\ndf_seen_all = df_filtered[seen_all]\n\nfav_counts = [df_seen_all[col].value_counts().get(1, 0) for col in rank_columns]\n\ntotal_resp = df_seen_all.shape[0]\npercent_fav = [(count / total_resp) * 100 for count in fav_counts]\n\ndf_fav = pd.DataFrame({\n  'movie': movie_titles,\n  'percentage': percent_fav\n})\ndf_fav['percent_label'] = df_fav['percentage'].round(0).astype(int).astype(str) + '%'\ndf_fav['movie'] = pd.Categorical(df_fav['movie'], categories=movie_titles[::-1], ordered=True)\n\np5 = (ggplot(df_fav, aes(x='movie', y='percentage')) + geom_bar(stat='identity', fill='blue') + geom_text(aes(label='percent_label'), nudge_y=2, size=10) + labs(title='Favorite Star Wars Movies', subtitle='Of respondents who have seen all six films', x='Percentage of Respondents', y='Star Wars Movies') + coord_flip() + theme(axis_text_x=element_text(hjust=1)))\n\np5.show()",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Exploration/project1.html",
    "href": "Cleansing_Exploration/project1.html",
    "title": "Client Report - What’s in a Name?",
    "section": "",
    "text": "What’s in a Name?\n\n\n\n\nElevator pitch\nIn comparing baby names and birth years looking for trends, I have learned that my name “Braxton” was not all that popular when I was born. If you were to get a call from a “Brittney” on the phone, it would be safe to assume that her age would be 34 years old. I also analyzed the effect of certain events on popularity of names.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\ndf \n\n\n\n\n\n\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n0\nAaden\n2005\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n1\nAaden\n2007\n0.0\n5.0\n0.0\n5.0\n20.0\n6.0\n0.0\n0.0\n...\n5.0\n14.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n98.0\n\n\n2\nAaden\n2008\n0.0\n15.0\n13.0\n20.0\n135.0\n10.0\n9.0\n0.0\n...\n15.0\n99.0\n8.0\n26.0\n0.0\n11.0\n19.0\n5.0\n0.0\n939.0\n\n\n3\nAaden\n2009\n0.0\n28.0\n20.0\n23.0\n158.0\n22.0\n12.0\n0.0\n...\n33.0\n140.0\n6.0\n17.0\n0.0\n31.0\n23.0\n14.0\n0.0\n1242.0\n\n\n4\nAaden\n2010\n0.0\n8.0\n6.0\n12.0\n62.0\n9.0\n5.0\n0.0\n...\n9.0\n56.0\n0.0\n11.0\n0.0\n7.0\n12.0\n0.0\n0.0\n414.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n393379\nZyon\n2011\n0.0\n0.0\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n...\n5.0\n9.0\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n117.0\n\n\n393380\nZyon\n2012\n0.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n7.5\n0.0\n12.0\n0.0\n0.0\n0.0\n0.0\n0.0\n118.5\n\n\n393381\nZyon\n2013\n0.0\n0.0\n0.0\n0.0\n6.0\n0.0\n0.0\n0.0\n...\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n93.0\n\n\n393382\nZyon\n2014\n0.0\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n...\n0.0\n7.0\n0.0\n9.0\n0.0\n0.0\n0.0\n0.0\n0.0\n73.0\n\n\n393383\nZyon\n2015\n0.0\n0.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n0.0\n10.0\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n87.5\n\n\n\n\n393384 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 1\nHow does your name at your birth year compare to its use historically?\nHistoricly it looks like my name was given after a slight rise in popularity. However, I was given my name before the largest spike after about 2002.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nPlot of the name Braxton\n# Include and execute your code here\nmy_name_df = df[df['name'] == 'Braxton']\nyear_2000_data = my_name_df[my_name_df['year'] == 2000]\n\n(ggplot(my_name_df, aes(x='year', y='Total')) +\n    geom_point(color = 'blue') +\n    ggtitle('The Use of Braxton') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5)) +\n    geom_text(aes(x=year_2000_data['year'], y=year_2000_data['Total']), label='782', nudge_x=-0.5, nudge_y=400) +\n    geom_segment(aes(x=year_2000_data['year'], xend=year_2000_data['year'], y=year_2000_data['Total'] - 10, yend=year_2000_data['Total']+350), arrow=arrow(length=5, type='closed'), color='red')\n)\n\n\n   \n   \nThe Use of Braxton\n\n\n\n\ntable of the name Braxton\ndisplay(my_name_df)\n\n\n\n\n\n\nBraxton’s Name Usage\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n50029\nBraxton\n1914\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n13.0\n\n\n50030\nBraxton\n1915\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n11.0\n\n\n50031\nBraxton\n1916\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12.0\n\n\n50032\nBraxton\n1917\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n50033\nBraxton\n1918\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n10.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50095\nBraxton\n2011\n6.0\n123.0\n84.0\n37.0\n62.0\n43.0\n0.0\n0.0\n...\n131.0\n188.0\n93.0\n40.0\n0.0\n33.0\n35.0\n68.0\n16.0\n2491.0\n\n\n50096\nBraxton\n2012\n5.0\n111.0\n79.0\n37.0\n61.0\n39.0\n5.0\n0.0\n...\n151.0\n227.0\n65.0\n58.0\n0.0\n41.0\n62.0\n95.0\n22.0\n2986.0\n\n\n50097\nBraxton\n2013\n0.0\n100.0\n84.0\n53.0\n92.0\n49.0\n7.0\n0.0\n...\n158.0\n226.0\n63.0\n64.0\n0.0\n42.0\n62.0\n74.0\n9.0\n3085.0\n\n\n50098\nBraxton\n2014\n10.0\n121.0\n86.0\n45.0\n106.0\n43.0\n6.0\n0.0\n...\n150.0\n147.0\n79.0\n60.0\n0.0\n39.0\n56.0\n74.0\n20.0\n3186.0\n\n\n50099\nBraxton\n2015\n0.0\n103.0\n76.0\n48.0\n102.0\n38.0\n6.0\n0.0\n...\n173.0\n278.0\n54.0\n61.0\n0.0\n57.0\n74.0\n67.0\n9.0\n3265.0\n\n\n\n\n71 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nIt appears that the most popular year for the use of the name “Brittney” was 1990. This means that we could make a educated guess that a person named Britteny would be 34 years old.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nBar chart of the name Brittany\nbrittany_df = df[df['name'] == 'Brittany']\n\n(ggplot(brittany_df, aes(x='year', y='Total')) +\n    geom_bar(stat='identity') +\n    ggtitle('The Popularity of Brittney Over Time') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nThe Popularity of Brittney Over Time\n\n\n\n\nTable of the name Brittany\n# Include and execute your code here\n\ndisplay(brittany_df)\n\n\n\n\n\n\nBrittany’s Name Usage\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n53205\nBrittany\n1968\n0.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n53206\nBrittany\n1969\n0.0\n0.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12.0\n\n\n53207\nBrittany\n1970\n0.0\n0.0\n0.0\n0.0\n5.0\n5.0\n0.0\n0.0\n...\n0.0\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n32.0\n\n\n53208\nBrittany\n1971\n0.0\n0.0\n0.0\n5.0\n17.0\n0.0\n0.0\n0.0\n...\n0.0\n14.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n81.0\n\n\n53209\nBrittany\n1972\n0.0\n0.0\n0.0\n0.0\n11.0\n10.0\n0.0\n0.0\n...\n8.0\n14.0\n16.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n158.0\n\n\n53210\nBrittany\n1973\n0.0\n0.0\n0.0\n6.0\n17.0\n0.0\n0.0\n0.0\n...\n0.0\n18.0\n13.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n166.0\n\n\n53211\nBrittany\n1974\n0.0\n0.0\n7.0\n0.0\n19.0\n0.0\n0.0\n0.0\n...\n0.0\n17.0\n28.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n198.0\n\n\n53212\nBrittany\n1975\n0.0\n0.0\n0.0\n0.0\n28.0\n0.0\n0.0\n0.0\n...\n10.0\n23.0\n28.0\n0.0\n0.0\n9.0\n5.0\n0.0\n0.0\n277.0\n\n\n53213\nBrittany\n1976\n0.0\n0.0\n6.0\n0.0\n36.0\n5.0\n0.0\n0.0\n...\n15.0\n35.0\n21.0\n7.0\n0.0\n7.0\n11.0\n0.0\n0.0\n304.0\n\n\n53214\nBrittany\n1977\n0.0\n9.0\n8.0\n5.0\n39.0\n10.0\n0.0\n0.0\n...\n16.0\n48.0\n55.0\n5.0\n0.0\n0.0\n7.0\n0.0\n0.0\n448.0\n\n\n53215\nBrittany\n1978\n0.0\n8.0\n12.0\n8.0\n50.0\n17.0\n0.0\n0.0\n...\n15.0\n48.0\n67.0\n7.0\n0.0\n11.0\n5.0\n0.0\n0.0\n592.0\n\n\n53216\nBrittany\n1979\n0.0\n6.0\n13.0\n0.0\n47.0\n19.0\n10.0\n0.0\n...\n20.0\n76.0\n63.0\n14.0\n0.0\n13.0\n10.0\n7.0\n8.0\n764.0\n\n\n53217\nBrittany\n1980\n0.0\n11.0\n20.0\n13.0\n121.0\n41.0\n5.0\n0.0\n...\n39.0\n142.0\n110.0\n18.0\n0.0\n36.0\n28.0\n10.0\n10.0\n1383.0\n\n\n53218\nBrittany\n1981\n11.0\n23.0\n32.0\n22.0\n127.0\n37.0\n14.0\n0.0\n...\n54.0\n166.0\n116.0\n21.0\n0.0\n47.0\n24.0\n10.0\n14.0\n1701.0\n\n\n53219\nBrittany\n1982\n12.0\n50.0\n53.0\n47.0\n290.0\n88.0\n18.0\n5.0\n...\n102.0\n300.0\n152.0\n50.0\n7.0\n73.0\n58.0\n25.0\n16.0\n3093.0\n\n\n53220\nBrittany\n1983\n20.0\n71.0\n80.0\n66.0\n406.0\n131.0\n26.0\n19.0\n...\n133.0\n415.0\n156.0\n83.0\n7.0\n95.0\n68.0\n42.0\n19.0\n4377.0\n\n\n53221\nBrittany\n1984\n34.0\n158.0\n111.0\n112.0\n698.0\n225.0\n59.0\n22.0\n...\n243.0\n735.0\n159.0\n148.0\n6.0\n134.0\n114.0\n78.0\n36.0\n7664.0\n\n\n53222\nBrittany\n1985\n53.0\n351.0\n224.0\n217.0\n715.5\n319.0\n118.0\n48.0\n...\n500.0\n1256.0\n182.0\n390.0\n25.0\n231.0\n260.0\n235.0\n40.0\n14010.0\n\n\n53223\nBrittany\n1986\n49.0\n241.5\n269.0\n309.0\n886.5\n389.0\n165.0\n79.0\n...\n586.0\n792.5\n183.0\n533.0\n45.0\n287.0\n361.0\n229.0\n48.0\n17856.5\n\n\n53224\nBrittany\n1987\n56.0\n528.0\n267.0\n322.0\n977.0\n365.0\n196.0\n97.0\n...\n633.0\n795.5\n193.0\n628.0\n34.0\n289.0\n351.0\n241.0\n42.0\n18825.5\n\n\n53225\nBrittany\n1988\n48.0\n585.0\n304.0\n383.0\n1214.5\n370.0\n271.0\n126.0\n...\n409.5\n925.0\n167.0\n884.0\n43.0\n368.0\n455.0\n270.0\n41.0\n21952.0\n\n\n53226\nBrittany\n1989\n53.0\n837.0\n457.0\n549.0\n1513.0\n439.0\n371.0\n86.5\n...\n537.5\n1266.0\n185.0\n1247.0\n71.0\n500.0\n724.0\n402.0\n50.0\n30848.0\n\n\n53227\nBrittany\n1990\n75.0\n788.0\n440.0\n483.0\n1372.5\n450.0\n345.0\n90.0\n...\n546.5\n1181.5\n173.0\n1229.0\n86.0\n470.0\n671.0\n379.0\n38.0\n32562.5\n\n\n53228\nBrittany\n1991\n55.0\n585.0\n327.0\n379.0\n1155.5\n361.0\n293.0\n132.0\n...\n872.0\n983.0\n140.0\n959.0\n47.0\n363.0\n556.0\n352.0\n43.0\n26963.5\n\n\n53229\nBrittany\n1992\n51.0\n526.0\n282.0\n292.0\n915.0\n325.0\n223.0\n87.0\n...\n726.0\n1785.0\n119.0\n854.0\n57.0\n307.0\n465.0\n292.0\n40.0\n23416.5\n\n\n53230\nBrittany\n1993\n37.0\n511.0\n229.0\n298.0\n1597.0\n275.0\n199.0\n62.0\n...\n637.0\n1547.0\n123.0\n697.0\n44.0\n286.0\n389.0\n242.0\n26.0\n21728.0\n\n\n53231\nBrittany\n1994\n47.0\n422.0\n237.0\n227.0\n666.5\n227.0\n168.0\n72.0\n...\n599.0\n1359.0\n91.0\n641.0\n43.0\n229.0\n351.0\n228.0\n32.0\n17808.5\n\n\n53232\nBrittany\n1995\n34.0\n342.0\n190.0\n202.0\n605.5\n179.0\n156.0\n49.0\n...\n516.0\n1293.0\n110.0\n537.0\n40.0\n216.0\n271.0\n194.0\n19.0\n15875.5\n\n\n53233\nBrittany\n1996\n29.0\n318.0\n163.0\n195.0\n977.0\n163.0\n133.0\n31.0\n...\n400.0\n1147.0\n107.0\n455.0\n25.0\n166.0\n239.0\n147.0\n18.0\n13796.0\n\n\n53234\nBrittany\n1997\n19.0\n253.0\n166.0\n138.0\n825.0\n141.0\n111.0\n24.0\n...\n342.0\n966.0\n85.0\n334.0\n19.0\n147.0\n230.0\n102.0\n7.0\n11527.0\n\n\n53235\nBrittany\n1998\n29.0\n190.0\n109.0\n165.0\n735.0\n134.0\n85.0\n22.0\n...\n324.0\n828.0\n75.0\n310.0\n23.0\n121.0\n170.0\n114.0\n11.0\n9843.0\n\n\n53236\nBrittany\n1999\n18.0\n163.0\n97.0\n114.0\n607.0\n135.0\n69.0\n20.0\n...\n265.0\n698.0\n70.0\n246.0\n13.0\n104.0\n126.0\n83.0\n11.0\n7942.0\n\n\n53237\nBrittany\n2000\n14.0\n111.0\n63.0\n88.0\n354.0\n71.0\n33.0\n14.0\n...\n199.0\n432.0\n41.0\n176.0\n9.0\n59.0\n91.0\n71.0\n6.0\n5183.0\n\n\n53238\nBrittany\n2001\n6.0\n92.0\n43.0\n60.0\n223.0\n35.0\n27.0\n9.0\n...\n109.0\n287.0\n21.0\n83.0\n5.0\n33.0\n33.0\n50.0\n0.0\n2915.0\n\n\n53239\nBrittany\n2002\n0.0\n39.0\n28.0\n32.0\n144.0\n32.0\n9.0\n7.0\n...\n66.0\n206.0\n22.0\n70.0\n5.0\n16.0\n30.0\n29.0\n0.0\n1912.0\n\n\n53240\nBrittany\n2003\n0.0\n32.0\n22.0\n25.0\n148.0\n23.0\n16.0\n5.0\n...\n54.0\n177.0\n16.0\n42.0\n0.0\n15.0\n25.0\n19.0\n0.0\n1559.0\n\n\n53241\nBrittany\n2004\n0.0\n31.0\n23.0\n24.0\n139.0\n20.0\n0.0\n5.0\n...\n50.0\n137.0\n16.0\n37.0\n0.0\n14.0\n18.0\n28.0\n6.0\n1323.5\n\n\n53242\nBrittany\n2005\n0.0\n28.0\n18.0\n29.0\n116.0\n24.0\n7.0\n0.0\n...\n45.0\n148.0\n18.0\n35.0\n0.0\n9.0\n17.0\n13.0\n0.0\n1168.0\n\n\n53243\nBrittany\n2006\n0.0\n20.0\n13.0\n20.0\n121.0\n12.0\n6.0\n0.0\n...\n41.0\n103.0\n9.0\n30.0\n0.0\n10.0\n14.0\n9.0\n0.0\n1009.0\n\n\n53244\nBrittany\n2007\n0.0\n14.0\n12.0\n26.0\n126.0\n14.0\n0.0\n0.0\n...\n45.0\n96.0\n7.0\n27.0\n0.0\n14.0\n9.0\n9.0\n0.0\n891.0\n\n\n53245\nBrittany\n2008\n0.0\n15.0\n0.0\n14.0\n102.0\n5.0\n0.0\n0.0\n...\n30.0\n92.0\n9.0\n19.0\n0.0\n9.0\n16.0\n5.0\n0.0\n749.0\n\n\n53246\nBrittany\n2009\n0.0\n7.0\n6.0\n10.0\n105.0\n12.0\n0.0\n0.0\n...\n18.0\n83.0\n6.0\n26.0\n0.0\n10.0\n7.0\n0.0\n0.0\n644.0\n\n\n53247\nBrittany\n2010\n0.0\n9.0\n0.0\n20.0\n116.0\n9.0\n0.0\n0.0\n...\n31.0\n94.0\n11.0\n22.0\n0.0\n8.0\n6.0\n0.0\n0.0\n698.0\n\n\n53248\nBrittany\n2011\n0.0\n12.0\n7.0\n17.0\n109.0\n10.0\n9.0\n0.0\n...\n18.0\n91.0\n6.0\n32.0\n0.0\n7.0\n11.0\n0.0\n0.0\n717.0\n\n\n53249\nBrittany\n2012\n0.0\n12.0\n13.0\n9.0\n137.0\n11.0\n8.0\n6.0\n...\n23.0\n94.0\n6.0\n24.0\n0.0\n7.0\n10.0\n0.0\n0.0\n745.0\n\n\n53250\nBrittany\n2013\n0.0\n13.0\n0.0\n14.0\n110.0\n7.0\n9.0\n5.0\n...\n11.0\n113.0\n7.0\n25.0\n0.0\n12.0\n8.0\n0.0\n0.0\n699.0\n\n\n53251\nBrittany\n2014\n0.0\n11.0\n5.0\n8.0\n112.0\n9.0\n10.0\n0.0\n...\n21.0\n110.0\n8.0\n15.0\n0.0\n10.0\n6.0\n0.0\n0.0\n660.0\n\n\n53252\nBrittany\n2015\n0.0\n9.0\n6.0\n10.0\n109.0\n11.0\n0.0\n0.0\n...\n16.0\n124.0\n9.0\n26.0\n0.0\n9.0\n0.0\n0.0\n0.0\n636.0\n\n\n\n\n48 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names in a single chart. What trends do you notice?\nIt seems to be that ‘Mary’ was the most common name for most of the early 1920’s. Once we get to the 1970’s however, all four names seem to take a steep decline and the doesn’t climb back up. I am lead to assume that as the world became less religious, biblical names became less popular.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nLine chart of the names Mary, Martha, Peter, and Paul\nnew_names_df = df[df['name'].isin(['Mary', 'Martha', 'Peter', 'Paul']) & (df['year'] &gt;= 1920) & (df['year'] &lt;= 2000)]\n\n(ggplot (new_names_df, aes(x='year', y='Total', color='name')) +\ngeom_line() + \n    ggtitle('Mary, Martha, Peter, and Paul Over Time') + xlab('Year') + ylab('Total') + \n    scale_x_continuous(format='d') + \n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nMary, Martha, Peter, and Paul Over Time\n\n\n\n\nMary, Martha, Peter, and Paul table\ndisplay(new_names_df)\n\n\n\n\n\n\nMary, Martha, Peter, and Paul Over Time\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n264124\nMartha\n1920\n11.0\n258.0\n224.0\n21.0\n131.0\n66.0\n55.0\n20.0\n...\n416.0\n418.0\n16.0\n269.0\n11.0\n73.0\n116.0\n172.0\n11.0\n8705.0\n\n\n264125\nMartha\n1921\n0.0\n307.0\n216.0\n18.0\n161.0\n70.0\n57.0\n34.0\n...\n420.0\n478.0\n20.0\n297.0\n13.0\n57.0\n101.0\n204.0\n9.0\n9254.0\n\n\n264126\nMartha\n1922\n9.0\n326.0\n219.0\n23.0\n126.0\n67.0\n56.0\n17.0\n...\n421.0\n501.0\n18.0\n273.0\n14.0\n39.0\n82.0\n225.0\n5.0\n9018.0\n\n\n264127\nMartha\n1923\n0.0\n341.0\n236.0\n27.0\n159.0\n63.0\n38.0\n24.0\n...\n442.0\n233.0\n22.0\n293.0\n11.0\n45.0\n72.0\n210.0\n10.0\n8731.0\n\n\n264128\nMartha\n1924\n0.0\n342.0\n257.0\n39.0\n166.0\n58.0\n49.0\n23.0\n...\n507.0\n487.0\n15.0\n155.0\n15.0\n41.0\n72.0\n196.0\n20.0\n9163.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n303691\nPeter\n1996\n0.0\n22.0\n8.0\n42.0\n501.0\n55.0\n134.0\n22.0\n...\n30.0\n183.0\n27.0\n103.0\n20.0\n109.0\n114.0\n8.0\n0.0\n4069.0\n\n\n303692\nPeter\n1997\n17.0\n19.0\n5.0\n47.0\n476.0\n44.0\n111.0\n25.0\n...\n24.0\n175.0\n27.0\n73.0\n17.0\n74.0\n98.0\n0.0\n0.0\n3821.0\n\n\n303693\nPeter\n1998\n14.0\n21.0\n8.0\n42.0\n471.0\n42.0\n104.0\n14.0\n...\n27.0\n134.0\n41.0\n78.0\n12.0\n98.0\n89.0\n0.0\n0.0\n3377.0\n\n\n303694\nPeter\n1999\n15.0\n24.0\n9.0\n48.0\n402.0\n66.0\n101.0\n23.0\n...\n30.0\n138.0\n26.0\n105.0\n10.0\n77.0\n81.0\n0.0\n0.0\n3430.0\n\n\n303695\nPeter\n2000\n11.0\n14.0\n11.0\n40.0\n379.0\n66.0\n72.0\n20.0\n...\n26.0\n147.0\n24.0\n62.0\n12.0\n68.0\n70.0\n0.0\n0.0\n3137.0\n\n\n\n\n324 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nLooking at the graph it looks as though there was a little rise in the use of the name after 1993. The rise after 1993 is slight and would be hard to conclude that the increase is correlated with the release of “The Sandlot”.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nLine chart of the name Benjamin\nbenjamin_df = df[df['name'] == 'Benjamin']\n\n(ggplot(benjamin_df, aes(x='year', y='Total')) +\n    geom_line(color = 'blue') +\n    geom_vline(xintercept = 1993, linetype = 'dashed', color = 'red') +\n    geom_text(x = 1992, y = 15000, label = 'Sandlot was Released', hjust = 1) +\n    geom_text(x = 1960, y = 14200, label = '(1993)', hjust = 0) +\n    ggtitle('Benjamin from The Sandlot') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nBenjamin from The Sandlot\n\n\n\n\nBenjamin from The Sandlot table\ndisplay(benjamin_df)\n\n\n\n\n\n\nBenjamin from The Sandlot\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n41526\nBenjamin\n1910\n0.0\n13.0\n10.0\n5.0\n5.0\n0.0\n7.0\n0.0\n...\n0.0\n22.0\n0.0\n23.0\n0.0\n0.0\n5.0\n0.0\n0.0\n497.0\n\n\n41527\nBenjamin\n1911\n0.0\n14.0\n10.0\n0.0\n7.0\n7.0\n22.0\n7.0\n...\n7.0\n16.0\n0.0\n26.0\n0.0\n0.0\n7.0\n0.0\n0.0\n645.0\n\n\n41528\nBenjamin\n1912\n0.0\n25.0\n13.0\n0.0\n18.0\n7.0\n25.0\n7.0\n...\n24.0\n41.0\n0.0\n47.0\n0.0\n7.0\n19.0\n13.0\n0.0\n1245.0\n\n\n41529\nBenjamin\n1913\n0.0\n29.0\n19.0\n0.0\n16.0\n9.0\n31.0\n15.0\n...\n22.0\n57.0\n5.0\n49.0\n0.0\n14.0\n19.0\n15.0\n0.0\n1451.0\n\n\n41530\nBenjamin\n1914\n0.0\n50.0\n31.0\n5.0\n29.0\n12.0\n37.0\n8.0\n...\n35.0\n71.0\n0.0\n59.0\n0.0\n11.0\n21.0\n11.0\n0.0\n1802.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41627\nBenjamin\n2011\n23.0\n165.0\n91.0\n281.0\n1592.0\n247.0\n157.0\n57.0\n...\n244.0\n988.0\n193.0\n403.0\n33.0\n308.0\n271.0\n49.0\n17.0\n12743.0\n\n\n41628\nBenjamin\n2012\n31.0\n141.0\n87.0\n264.0\n1676.0\n212.0\n157.0\n52.0\n...\n252.0\n1035.0\n158.0\n364.0\n29.0\n336.0\n238.0\n57.0\n20.0\n12411.5\n\n\n41629\nBenjamin\n2013\n34.0\n125.0\n81.0\n300.0\n1845.0\n242.0\n145.0\n37.0\n...\n279.0\n1079.0\n178.0\n363.0\n21.0\n333.0\n262.0\n56.0\n18.0\n13460.0\n\n\n41630\nBenjamin\n2014\n36.0\n128.0\n93.0\n270.0\n1873.0\n252.0\n179.0\n64.0\n...\n249.0\n1152.0\n153.0\n352.0\n26.0\n370.0\n214.0\n50.0\n25.0\n13761.0\n\n\n41631\nBenjamin\n2015\n34.0\n151.0\n110.0\n275.0\n1809.0\n248.0\n178.0\n57.0\n...\n239.0\n1154.0\n168.0\n383.0\n21.0\n356.0\n232.0\n72.0\n29.0\n13608.0\n\n\n\n\n106 rows × 54 columns\n\n\n\n–&gt; –&gt;\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html",
    "href": "Cleansing_Exploration/project4.html",
    "title": "Client Report - Can You Predict That?",
    "section": "",
    "text": "Using the power of advanced machine learning, we developed a model that successfully predicted whether a home was built before or after 1980. Through model tuning we were able to get the accuracy of our model’s prediction at the 90% threshold making our model relatively reliable in predicting the year a home was built. We first Identified possible relationships between homes built before and after 1980. Our model found that one story architecture, garage type, and quality ratings of C were most correlated with homes from built before 1980. Our model was then explained and justified using a classification report, confusion matrix and an ROC curve confirming that our model was accurate and reliable. I also built a model to predict the year a home was built using the “GradientBoostingRegressor.” This model was able to successfully predict 89% of its results. For Justification we look at Mean Absolute Error (MAE), Mean Squared Error (MSE), R-Squared value, and a confusion matrix. \n\n\nRead and format project data\n# Include and execute your code here\n\ndwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html#elevator-pitch",
    "href": "Cleansing_Exploration/project4.html#elevator-pitch",
    "title": "Client Report - Can You Predict That?",
    "section": "",
    "text": "Using the power of advanced machine learning, we developed a model that successfully predicted whether a home was built before or after 1980. Through model tuning we were able to get the accuracy of our model’s prediction at the 90% threshold making our model relatively reliable in predicting the year a home was built. We first Identified possible relationships between homes built before and after 1980. Our model found that one story architecture, garage type, and quality ratings of C were most correlated with homes from built before 1980. Our model was then explained and justified using a classification report, confusion matrix and an ROC curve confirming that our model was accurate and reliable. I also built a model to predict the year a home was built using the “GradientBoostingRegressor.” This model was able to successfully predict 89% of its results. For Justification we look at Mean Absolute Error (MAE), Mean Squared Error (MSE), R-Squared value, and a confusion matrix. \n\n\nRead and format project data\n# Include and execute your code here\n\ndwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html#questiontask-1",
    "href": "Cleansing_Exploration/project4.html#questiontask-1",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np \nfrom lets_plot import * \nLetsPlot.setup_html(isolated_frame=True)\n\ndf_subset = dwellings_ml.filter(\n    ['livearea', 'finbsmnt', 'basement', \n    'yearbuilt', 'nocars', 'numbdrm', 'numbaths', 'before1980',\n    'stories', 'yrbuilt']).sample(500)\n\ndf_subset = df_subset.dropna(subset=['livearea', 'finbsmnt', 'before1980', 'basement', 'numbaths', 'numbdrm'])\n\ndf_subset['before1980'] = df_subset['before1980'].astype(bool)\n\n\nThis scatter plot shows us the possible relationship between the number of bedrooms built in the house and the total living space, or square footage, of the home. There seems to be a linear relationship between house size and number of rooms built. As the number of rooms increase, so does the total livable square footage.\n\n\nLiving Area vs Number of Bedrooms\n# Include and execute your code here\nc_1 = ggplot(df_subset, aes(x='livearea', y='numbdrm', color='before1980')) + geom_point(alpha=0.5) + ggtitle(\"Living Area vs Number of Bedrooms\") + xlab(\"Living Area\") + ylab(\"Number of Bedrooms\") + scale_color_manual(name='Built Before 1980', labels=['Yes (Blue)', 'No (Red)'], values=['blue', 'red']) + theme(legend_position='right') \n\nc_1.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nLiving Area vs Number of Bedrooms\n\n\nThis box and whisker plot shows is the relationship between basement sizes on homes built before 1980 and after 1980. From this we can see that homes that were built before 1980 had slightly larger mean basement size than homes built after 1980.\n\n\nFinished Basement Area by Before1980\n# Include and execute your code here\n\nc_2 = ggplot(df_subset, aes(x='before1980', y='finbsmnt', fill='before1980')) + geom_boxplot() + ggtitle('Finished Basement Area by Before1980') + xlab('Built Before 1980') + ylab('Finished Basement Area') + scale_fill_discrete(name=\"Built Before 1980\")\nc_2.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThis stacked bar chart tells us the relationship between the number of bathrooms built in a house based on if the house was built before or after 1980. It looks like the majority of houses built before 1980 had two bathrooms. \n\n\nNumber of Bathrooms by Before1980\n# Include and execute your code here\n\nc_3 = ggplot(df_subset, aes(x='numbaths', fill='before1980')) + geom_bar(position='stack') + ggtitle('Number of Bathrooms by Before1980') + xlab('Number of Bathrooms') + ylab('Count') + scale_fill_discrete(name=\"Built Before 1980\")\nc_3.show()"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html#questiontask-2",
    "href": "Cleansing_Exploration/project4.html#questiontask-2",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\nI ended up using the DecisionTreeClassifier to create my classification model. I settled on this because this led to me getting the highest accuracy in the quickest time. I tried to use the Gradient Boosting, but it ended up taking longer to get the results. I also tried to add the following parameters: max_depth, min_sample_split, and min_samples_leaf. I found that this took a lot longer to get the results and it did not improve the accuracy rating at all when compared to the DecisionTreeClassifier with no parameters. I was able to achieve a 90% accuracy rating.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\n\n\n\n\nClassification Report\nX = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny = dwellings_ml.filter(regex = \"before1980\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .35, random_state = 76)\n\nmodel_1 = tree.DecisionTreeClassifier(random_state=76, max_depth=10, min_samples_split=10, min_samples_leaf=2)\nmodel_1.fit(X_train, y_train)\n\ny_pred = model_1.predict(X_test)\n\nprint('\\nTest Accuracy:', metrics.accuracy_score(y_test, y_pred))\nprint('\\nClassification Report: \\n', metrics.classification_report(y_test, y_pred))\n\n\n\nTest Accuracy: 0.9007481296758105\n\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       0.86      0.87      0.87      2980\n           1       0.92      0.92      0.92      5040\n\n    accuracy                           0.90      8020\n   macro avg       0.89      0.89      0.89      8020\nweighted avg       0.90      0.90      0.90      8020"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html#questiontask-3",
    "href": "Cleansing_Exploration/project4.html#questiontask-3",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a feature importance chart and a description of the features.\nThe most important features identified in my model were “arcstyle_ONE_STORY”, “gartype_Att”, and “quality_C.” The model puts the most importance on how many stories the house is using “arcstyle_ONE_STORY.” One story homes were more popular before 1980 so there were more built of these types of homes. The second most important feature my model used was whether or not the house had an attached, or detached garage, “gartype_Att.” Most of the houses built before 1980 had detached garages. The third greatest importance the model used was the quality of the home from “quality_C.” Homes with this quality rating were more likely to be older homes. The model put these three fields together to best predict if the house was built before or after 1980.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nFeature Importance Chart\n# Include and execute your code here\nimportance = model_1.feature_importances_\nfeature_names = X.columns\n\nfeature_importance_df = pd.DataFrame({'feature': feature_names, 'Importance': importance})\nfeature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n\nfeature_importance_df_top = feature_importance_df.head(10)\n\nc_4 = ggplot(feature_importance_df_top, aes(x='feature', y='Importance', fill= 'Importance')) + geom_bar(stat='identity') + theme(axis_text_x=element_text(angle=90, hjust=1)) + ggtitle('Feature Importance') + xlab('Feature') + ylab('Importance')\nc_4.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nFeature Importance"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html#questiontask-4",
    "href": "Cleansing_Exploration/project4.html#questiontask-4",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\nTo justify the results and quality of my model, I chose to look at the following three metrics: 1 - Classification report containing the accuracy, 2 - Confusion matrix, and 3 - The Receiver-operating characteristic (ROC) curve. Based on the results of this test, I can conclude that the quality of my model is strong and can be relied upon with an acceptable accuracy and true positive rate. Under each of the reports I will explain the significance and how to interpret the results.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nPerformance Metrics\n# Include and execute your code here\nprint('\\nTest Accuracy:', metrics.accuracy_score(y_test, y_pred))\nprint('\\nClassification Report: \\n', metrics.classification_report(y_test, y_pred))\n\n\n\nTest Accuracy: 0.9007481296758105\n\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       0.86      0.87      0.87      2980\n           1       0.92      0.92      0.92      5040\n\n    accuracy                           0.90      8020\n   macro avg       0.89      0.89      0.89      8020\nweighted avg       0.90      0.90      0.90      8020\n\n\n\nThe classification report and accuracy measure the percentage of correctly classified predictions compared to the total results. The higher the accuracy percentage is the more accurate the model is. A high accuracy rating is anything greater than or equal to 90% (0.90). The precision measures the percentage of total correct guesses divided by the number of true positives + false positives. For “0” this means that 86% of the home predicted to be built after 1980 are correct, and for the “1” 96% of the homes predicted to be built before 1980 are correct. Recall measures the total true positives divided by true positives + false negatives. For “0”, the model correctly identified 87% of the homes built after 1980 and for “1” correctly identified 92% of the homes built before 1980. The F-1 Score is calculated as 2 * (Precision * Recall / Precision + Recall). The F-1 score provides a balanced metric to look at our model performance. Our model is slightly stronger at predicting if a home was built before 1980 than after 1980 but overall reliable at predicting if it was built before or after 1980.\n\n\nConfusion Matrix\n# Include and execute your code here\n\nconf_matrix = metrics.confusion_matrix(y_test, y_pred)\nconf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted: (After 1980)', 'Predicted: (Before 1980)'], index=['Actual: (After 1980)', 'Actual: (Before 1980)'])\n\nconf_matrix_plot = ggplot(conf_matrix_df.reset_index().melt(id_vars='index'), aes(x='index', y='variable', fill='value')) + geom_tile() + geom_text(aes(label='value'), color='black', size=6) + scale_fill_brewer(palette='RdYlBu') + ggtitle('Confusion Matrix') + xlab('Predicted') + ylab('Actual')\n\nconf_matrix_plot.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThe confusion matrix is a table that is used to describe the performance of a classification model. The matrix shows the number of true positives (4630), false positives (386), true negatives (2594), and false negatives (410). From the confusion matrix we can see that our model had relatively few false positives / negatives in relation to the correct predictions. This proves how well my model was at predicting if a home was built before or after 1980.\n\n\nROC Curve\n# Include and execute your code here\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\n\nroc_df = pd.DataFrame({'FPR': fpr, 'TPR': tpr, 'Thresholds': thresholds})\n\nroc_chart = ggplot(roc_df, aes(x='FPR', y='TPR')) + geom_line(color='blue') + ggtitle('ROC Curve') + xlab('False Positive Rate') + ylab('True Positive Rate')\n\nroc_chart.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThe ROC curve is a graphical representation of the true positive rate against the false positive rate. The closer the curve is to the top left corner, the better the model is. The area under the curve (AUC) is used to measure the performance of the model. The higher the AUC, the better the model is. Our model had an AUC of 0.92 which shows us it performed very well at predicting when a house was built."
  },
  {
    "objectID": "Cleansing_Exploration/project4.html#stretchtask-5",
    "href": "Cleansing_Exploration/project4.html#stretchtask-5",
    "title": "Client Report - Can You Predict That?",
    "section": "STRETCH|TASK 5",
    "text": "STRETCH|TASK 5\nCan you build a model that predicts the year a house was built? Explain the model and the evaluation metrics you would use to determine if the model is good.\nFor the model I chose to use the “GradientBoostingRegressor” because of its ability to sift through complex, non-linear relationships to get to the target prediction. In this case our goal was to predict what year a house was built which requires the model to look through the data and attempt to find patterns that could help predict when a house was built. The “GradientBoostingRegressor” is also very good at not over filtering the data like other models. This model type also very flexible and forgiving when it comes to tuning the model. Below are the figures and print outs I used to evaluate the accuracy of my model.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nRead and format data\n# Include and execute your code here\n\nX_5 = dwellings_ml.drop(dwellings_ml.filter(regex = 'yrbuilt|parcel').columns, axis = 1)\ny_5 = dwellings_ml[\"yrbuilt\"]\n\nX_5 = X_5.dropna()\ny_5 = y_5[X_5.index]\n\nX_train, X_test, y5_train, y5_test = train_test_split(X_5, y_5, test_size = .35, random_state = 76)\n\nrf_model = GradientBoostingRegressor(random_state=76, n_estimators=500, learning_rate=0.1, max_depth=15, min_samples_split=10, min_samples_leaf=2)\nrf_model.fit(X_train, y5_train)\n\ny5_pred = rf_model.predict(X_test)\n\nmae = metrics.mean_absolute_error(y5_test, y5_pred)\nmse = metrics.mean_squared_error(y5_test, y5_pred)\nr2 = metrics.r2_score(y5_test, y5_pred)\n\nprint(f'Mean Absolute Error (MAE): {mae:.2f}')\nprint(f'Mean Squared Error (MSE): {mse:.2f}')\nprint(f'R-Squared (R^2): {r2:.2f}')\n\n\nMean Absolute Error (MAE): 7.49\nMean Squared Error (MSE): 155.45\nR-Squared (R^2): 0.89\n\n\n\n\nClassification Report\n# Include and execute your code here\n\nbins = [0, 1980, 1995, 2010, 2023]\nlables = ['Pre 1980', '1980-1995', '1996-2010', 'Post 2010']\n\ny5_test_bins = pd.cut(y5_test, bins=bins, labels=lables)\ny5_pred_bins = pd.cut(y5_pred, bins=bins, labels=lables)\n\nprint('\\nTest Accuracy:', metrics.accuracy_score(y5_test_bins, y5_pred_bins))\nprint('\\nClassification Report: \\n', metrics.classification_report(y5_test_bins, y5_pred_bins))\n\n\n\nTest Accuracy: 0.9337905236907731\n\nClassification Report: \n               precision    recall  f1-score   support\n\n   1980-1995       0.67      0.68      0.68       418\n   1996-2010       0.84      0.91      0.87      1832\n   Post 2010       0.88      0.73      0.80       671\n    Pre 1980       1.00      0.99      0.99      5099\n\n    accuracy                           0.93      8020\n   macro avg       0.85      0.83      0.84      8020\nweighted avg       0.94      0.93      0.93      8020\n\n\n\nThe Mean Absolute Error (MAE) tells us that on average, our model’s predictions are off by only 7.5 years. With regression tasks like this a MAE of 10 years is typically considered very good. The Mean Squared Error (MSE) is similar to the MAE, but the MSE penalizes the model’s larger errors heavier than smaller errors. Our score of 156.62 equates to about 12.5 years of difference with a range of years that spans over 100 years. When we compare our R-Squared result with the Accuracy test below we can see that overall, our model is right within the acceptable margin of error.\n\n\nConfusion Matrix\n# Include and execute your code here\n\nconf5_matrix = metrics.confusion_matrix(y5_test_bins, y5_pred_bins, labels=lables)\nconf5_matrix_df = pd.DataFrame(conf5_matrix, columns=['Pred:(Pre 80)', 'Pred:(80-95)', 'Pred:(96-10)', 'Pred:(Post 10)'], index=['Actl:(Pre 80)', 'Actl:(80-95)', 'Actl:(96-10)', 'Actl:(Post 10)'])\n\nconf5_matrix_norm = conf5_matrix.astype('float') / conf5_matrix.sum(axis=1)[:, np.newaxis]\n\nconf5_matrix_norm_df = pd.DataFrame(conf5_matrix_norm, columns=['Pred:(Pre 80)', 'Pred:(80-95)', 'Pred:(96-10)', 'Pred:(Post 10)'], index=['Actl:(Pre 80)', 'Actl:(80-95)', 'Actl:(96-10)', 'Actl:(Post 10)'])\n\nconf5_matrix_norm_df_melted = conf5_matrix_norm_df.reset_index().melt(id_vars='index', var_name='Predicted', value_name='Accuracy')\n\nlbl_plot = (\n  pd.DataFrame(100 * conf5_matrix_norm)\n  .stack()\n  .reset_index(drop=True)\n  .round(1)\n  .astype(str) + '%'\n)\n\nconf5_matrix_plot = ggplot(conf5_matrix_norm_df_melted, aes(x='Predicted', y='index', fill='Accuracy')) + geom_tile() + geom_text(aes(label=lbl_plot), color='white', size=6) + scale_fill_gradient(low='red', high='blue', name='Accuracy') + ggtitle('Confusion Matrix') + xlab('Predicted') + ylab('Actual') + theme(axis_text_x=element_text(angle=90, hjust=1))\n\nconf5_matrix_plot.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nConfusion Matrix\n\n\nThe confusion matrix is a table that is used to describe the performance of a classification model. The matrix shows the number of true positives, false positives, true negatives, and false negatives. From the confusion matrix we can see that our model had relatively few false positives / negatives in relation to the correct predictions. This proves how well my model was at predicting when a house was built."
  },
  {
    "objectID": "Cleansing_Exploration/project5.html",
    "href": "Cleansing_Exploration/project5.html",
    "title": "Client Report - The War with Star Wars",
    "section": "",
    "text": "After cleansing the Star Wars survey data for better handling. The cleaning consisted of shorting column names, handling missing values, and flited the raw data to extract the correct and useable survey responses. I then successfully recreated two of the visuals featured on the article provided to validate our cleaned data. The visual output and measurements were identical to those of the article. After this we then used the cleaned data to create a machine learning model to predict if someone makes over $50,000 a year based on their survey responses. Our final model was 65% accurate at predicting if a participant makes over $50k a year. To provide extra validation we then successfully recreated one more visual from the article.\n\n\nRead and format project data\nurl = 'https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv'\n\ndf_cols = pd.read_csv(url, encoding = \"ISO-8859-1\", nrows = 1).melt()\ndf = pd.read_csv(url, encoding = \"ISO-8859-1\", skiprows =2, header = None )"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html#elevator-pitch",
    "href": "Cleansing_Exploration/project5.html#elevator-pitch",
    "title": "Client Report - The War with Star Wars",
    "section": "",
    "text": "After cleansing the Star Wars survey data for better handling. The cleaning consisted of shorting column names, handling missing values, and flited the raw data to extract the correct and useable survey responses. I then successfully recreated two of the visuals featured on the article provided to validate our cleaned data. The visual output and measurements were identical to those of the article. After this we then used the cleaned data to create a machine learning model to predict if someone makes over $50,000 a year based on their survey responses. Our final model was 65% accurate at predicting if a participant makes over $50k a year. To provide extra validation we then successfully recreated one more visual from the article.\n\n\nRead and format project data\nurl = 'https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv'\n\ndf_cols = pd.read_csv(url, encoding = \"ISO-8859-1\", nrows = 1).melt()\ndf = pd.read_csv(url, encoding = \"ISO-8859-1\", skiprows =2, header = None )"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html#questiontask-1",
    "href": "Cleansing_Exploration/project5.html#questiontask-1",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nShorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.\nThe column names were long and very difficult to work with what we wanted to accomplish in this product. I cleaned up some of the data and shorted the column names to a more unified method. The new column names can be seen below.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\n\n\n\n\nTable of column names\n# Include and execute your code here\n\nvar_replace = {\n    'Which of the following Star Wars films have you seen\\\\? Please select all that apply\\\\.':'seen',\n    'Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.':'rank',\n    'Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.':'view',\n    'Do you consider yourself to be a fan of the Star Trek franchise\\\\?':'star_trek_fan',\n    'Do you consider yourself to be a fan of the Expanded Universe\\\\?\\x8cæ':'expanded_fan',\n    'Are you familiar with the Expanded Universe\\\\?':'know_expanded',\n    'Have you seen any of the 6 films in the Star Wars franchise\\\\?':'seen_any',\n    'Do you consider yourself to be a fan of the Star Wars film franchise\\\\?':'star_wars_fans',\n    'Which character shot first\\\\?':'shot_first',\n    'Unnamed: \\d{1,2}':np.nan,\n    ' ':'_',\n}\n\nval_replace = {\n    'Response':'',\n    'Star Wars: Episode ':'',\n    ' ':'_'\n}\n\ndf_cols_new = (df_cols\n    .assign(\n        val_replace = lambda x:  x.value.str.strip().replace(val_replace, regex=True),\n        var_replace = lambda x: x.variable.str.strip().replace(var_replace, regex=True)\n    )\n    .fillna(method = 'ffill')\n    .fillna(value = \"\")\n    .assign(column_names = lambda x: x.var_replace.str.cat(x.val_replace, sep = \"_\").str.strip('__').str.lower())\n    )\n\ndf.columns = df_cols_new.column_names.to_list()\n\ndf.columns\n\n\nIndex(['respondentid', 'seen_any', 'star_wars_fans',\n       'seen_i__the_phantom_menace', 'seen_ii__attack_of_the_clones',\n       'seen_iii__revenge_of_the_sith', 'seen_iv__a_new_hope',\n       'seen_v_the_empire_strikes_back', 'seen_vi_return_of_the_jedi',\n       'rank_i__the_phantom_menace', 'rank_ii__attack_of_the_clones',\n       'rank_iii__revenge_of_the_sith', 'rank_iv__a_new_hope',\n       'rank_v_the_empire_strikes_back', 'rank_vi_return_of_the_jedi',\n       'view_han_solo', 'view_luke_skywalker', 'view_princess_leia_organa',\n       'view_anakin_skywalker', 'view_obi_wan_kenobi',\n       'view_emperor_palpatine', 'view_darth_vader', 'view_lando_calrissian',\n       'view_boba_fett', 'view_c-3p0', 'view_r2_d2', 'view_jar_jar_binks',\n       'view_padme_amidala', 'view_yoda', 'shot_first', 'know_expanded',\n       'expanded_fan', 'star_trek_fan', 'gender', 'age', 'household_income',\n       'education', 'location_(census_region)'],\n      dtype='object')\nColumn Names\n\n\nBelow is an example of what the column names were before and what I changed them to. The “variable” column represents what the original column name was. The “value” column original values in response to the question for the column. The “val_replace” and “var_replace” columns represent the cleaned values and variables used to replace the older, harder to use data and names. The last column “column_names” represents the cleaned and shortened column names we used for easier computer handling.\n\n\nTable of column names\n# Include and execute your code here\ndf_cols_new.head()\n\n\n\n\n\n\nColumn Names\n\n\n\nvariable\nvalue\nval_replace\nvar_replace\ncolumn_names\n\n\n\n\n0\nRespondentID\n\n\nRespondentID\nrespondentid\n\n\n1\nHave you seen any of the 6 films in the Star W...\nResponse\n\nseen_any\nseen_any\n\n\n2\nDo you consider yourself to be a fan of the St...\nResponse\n\nstar_wars_fans\nstar_wars_fans\n\n\n3\nWhich of the following Star Wars films have yo...\nStar Wars: Episode I The Phantom Menace\nI__The_Phantom_Menace\nseen\nseen_i__the_phantom_menace\n\n\n4\nUnnamed: 4\nStar Wars: Episode II Attack of the Clones\nII__Attack_of_the_Clones\nseen\nseen_ii__attack_of_the_clones"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html#questiontask-2",
    "href": "Cleansing_Exploration/project5.html#questiontask-2",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nClean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made.\nIn order to properly work with machine learning, we have to do some more data cleaning. The following output and data represents the steps I tool to clean the data.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\n\n\nA. Filter the dataset to respondents that have seen at least one film. I also altered this code to filter out any responses that said they had seen any Star Wars movie but did not mark what movies they had seen.\n\n\nFilter data\n# Include and execute your code here\n\nseen_columns = [\n    'seen_i__the_phantom_menace',\n    'seen_ii__attack_of_the_clones',\n    'seen_iii__revenge_of_the_sith',\n    'seen_iv__a_new_hope',\n    'seen_v_the_empire_strikes_back',\n    'seen_vi_return_of_the_jedi',\n]\n\ndf_filtered = df[df['seen_any'] == 'Yes']\n\nfor col in seen_columns:\n    df_filtered[col] = df_filtered[col].notna()\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nage\nhousehold_income\neducation\nlocation_(census_region)\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\n18-29\nNaN\nHigh school degree\nSouth Atlantic\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n18-29\n$0 - $24,999\nHigh school degree\nWest North Central\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n18-29\n$25,000 - $49,999\nBachelor degree\nMiddle Atlantic\n\n\n\n\n5 rows × 38 columns\n\nSeen Any Star Wars Movie\n\n\nB. Create a new column that converts the age ranges to a single number. Drop the age range categorical column\n\n\nChange Age Field\n# Include and execute your code here\n\nage_map = {\n    '18-29': 24,\n    '30-44': 37,\n    '45-60': 52,\n    '&gt;60': 65\n}\n\ndf_filtered['age_num'] = df_filtered['age'].map(age_map)\ndf_filtered.drop('age', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\nAge Field\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nhousehold_income\neducation\nlocation_(census_region)\nage_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nNaN\nHigh school degree\nSouth Atlantic\n24.0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n$0 - $24,999\nHigh school degree\nWest North Central\n24.0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n24.0\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n24.0\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n$25,000 - $49,999\nBachelor degree\nMiddle Atlantic\n24.0\n\n\n\n\n5 rows × 38 columns\n\n\n\nC. Create a new column that converts the education groupings to a single number. Drop the school categorical column\n\n\nFilter data\n# Include and execute your code here\n\neducation_map = {\n  'Less than high school degree': 9,\n  'High school degree': 12,\n  'Some college or Associate degree': 14,\n  'Bachelor degree': 16,\n  'Graduate degree': 20\n}\n\ndf_filtered['education_num'] = df_filtered['education'].map(education_map)\ndf_filtered.drop('education', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nhousehold_income\nlocation_(census_region)\nage_num\neducation_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nNaN\nSouth Atlantic\n24.0\n12.0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n$0 - $24,999\nWest North Central\n24.0\n12.0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n$100,000 - $149,999\nWest North Central\n24.0\n14.0\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n$100,000 - $149,999\nWest North Central\n24.0\n14.0\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n$25,000 - $49,999\nMiddle Atlantic\n24.0\n16.0\n\n\n\n\n5 rows × 38 columns\n\nEducation Field\n\n\nD. Create a new column that converts the income ranges to a single number. Drop the income range categorical column\n\n\nFilter data\n# Include and execute your code here\n\ndf_filtered['household_income'] = df_filtered['household_income'].fillna('unknown')\n\nincome_map = {\n  '$0 - $24,999': 12500,\n  '$25,000 - $49,999': 37500,\n  '$50,000 - $99,999': 75000,\n  '$100,000 - $149,999': 125000,\n  '$150,000+': 150000,\n  'unknown': 0\n}\n\ndf_filtered['income_num'] = df_filtered['household_income'].map(income_map)\ndf_filtered.drop('household_income', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nlocation_(census_region)\nage_num\neducation_num\nincome_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nSouth Atlantic\n24.0\n12.0\n0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\nWest North Central\n24.0\n12.0\n12500\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\nWest North Central\n24.0\n14.0\n125000\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\nWest North Central\n24.0\n14.0\n125000\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\nMiddle Atlantic\n24.0\n16.0\n37500\n\n\n\n\n5 rows × 38 columns\n\nIncome Field\n\n\nE. Create your target (also known as “y” or “label”) column based on the new income range column\n\n\nFilter data\n# Include and execute your code here\n\ndf_filtered['income_over_50k'] = (df_filtered['income_num'] &gt; 50000).astype(int)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nlocation_(census_region)\nage_num\neducation_num\nincome_num\nincome_over_50k\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nI don't understand this question\nYes\nNo\nNo\nMale\nSouth Atlantic\n24.0\n12.0\n0\n0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nI don't understand this question\nNo\nNaN\nNo\nMale\nWest North Central\n24.0\n12.0\n12500\n0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nI don't understand this question\nNo\nNaN\nYes\nMale\nWest North Central\n24.0\n14.0\n125000\n1\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nGreedo\nYes\nNo\nNo\nMale\nWest North Central\n24.0\n14.0\n125000\n1\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nHan\nYes\nNo\nYes\nMale\nMiddle Atlantic\n24.0\n16.0\n37500\n0\n\n\n\n\n5 rows × 39 columns\n\nIncome Over 50k\n\n\nF. One-hot encode all remaining categorical columns\n\n\nFilter data\n# Include and execute your code here\n\ndf_encoded = pd.get_dummies(df_filtered, drop_first=True)\n\ndf_encoded.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\nrank_ii__attack_of_the_clones\nrank_iii__revenge_of_the_sith\n...\nstar_trek_fan_Yes\ngender_Male\nlocation_(census_region)_East South Central\nlocation_(census_region)_Middle Atlantic\nlocation_(census_region)_Mountain\nlocation_(census_region)_New England\nlocation_(census_region)_Pacific\nlocation_(census_region)_South Atlantic\nlocation_(census_region)_West North Central\nlocation_(census_region)_West South Central\n\n\n\n\n0\n3292879998\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n2.0\n1.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2\n3292765271\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n2.0\n3.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n3292763116\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n6.0\n1.0\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\n3292731220\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n4.0\n6.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n5\n3292719380\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n4.0\n3.0\n...\nTrue\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 102 columns\n\nOne-hot encoding"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html#questiontask-3",
    "href": "Cleansing_Exploration/project5.html#questiontask-3",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nValidate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article\nI recreated the following visuals from the article to validate our data clensing: 1. “Which ‘Star Wars’ Movies Have You Seen?” and 2. “Who Shot First?”\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWhich Star Wars Movies Have You Seen?\n# Include and execute your code here\n\nseen_columns = [\n    'seen_i__the_phantom_menace',\n    'seen_ii__attack_of_the_clones',\n    'seen_iii__revenge_of_the_sith',\n    'seen_iv__a_new_hope',\n    'seen_v_the_empire_strikes_back',\n    'seen_vi_return_of_the_jedi',\n]\n\nmovie_titles = [\n    \"The Phantom Menace\",\n    \"Attack of the Clones\",\n    \"Revenge of the Sith\",\n    \"A New Hope\",\n    \"The Empire Strikes Back\",\n    \"Return of the Jedi\",\n]\n\ngood_resp = df_filtered[seen_columns].sum(axis=1) &gt; 0\n\ntotal_seen_any = good_resp.sum()\nprob_seen = [(df_filtered[col].sum() / total_seen_any) for col in seen_columns]\n\n\ndf_seen = pd.DataFrame({\n  'movie': movie_titles,\n  'probability': prob_seen\n})\n\np = (ggplot(df_seen, aes(x='movie', y='probability')) + geom_bar(stat='identity', fill='blue') + labs(title='Which Star Wars Movies Have You Seen?', subtitle='Percentage of respondents who have seen each film', x='Percentage of Respondents', y='Star Wars Movies') + theme(axis_text_x=element_text(angle=45, hjust=1)))\n\np.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nWhich Star Wars Movies Have You Seen?\n\n\n\n\nWho Shot First?\n# Include and execute your code here\n\nshot_first_counts = df_filtered['shot_first'].value_counts()\n\nshot_first_percent = (shot_first_counts / shot_first_counts.sum()) * 100\n\ndf_shot_first = pd.DataFrame({\n  'response': shot_first_percent.index,\n  'percentage': shot_first_percent.values\n})\n\ndf_shot_first['response'] = df_shot_first['response'].replace({\"I don't understand this question\": \"Don't Understand\"})\ndf_shot_first['response'] = pd.Categorical(df_shot_first['response'], categories=['Don\\'t Understand', 'Greedo', 'Han'], ordered=True)\ndf_shot_first['percent_label'] = df_shot_first['percentage'].round(0).astype(int).astype(str) + '%'\n\n\np2 = (ggplot(df_shot_first, aes(x='response', y='percentage')) + geom_bar(stat='identity', fill='blue') + geom_text(aes(label='percent_label'), nudge_y=2, size=10) + labs(title='Who Shot First?', subtitle='According to 834 respondents', x='Response', y='Percentage') + coord_flip() + theme(axis_text_x=element_text(hjust=1)))\n\np2.show()"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html#questiontask-4",
    "href": "Cleansing_Exploration/project5.html#questiontask-4",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nBuild a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.\nFor my model, I chose to use all the columns in the DataFrame except any of the columns that would directly give up the answer (e.g. household_income, income_num, location, and income_over_50k). After I filtered out any undesirable data, I split the data into training and testing sets, and trained the model to make predictions on if a person makes more or less than $50k a year. I then ran a accuracy score and classification report to display how successful my model was. The accuracy of my model 0.655 using the “RandomForestClassifier”. This means that our model can predict if a person makes over or under $50k with a 67% accuracy.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n\n\n\nRandom Forest Model\n# Include and execute your code here\n\nX_pred = df_encoded.drop(columns=df_encoded.filter(regex=(\"(household_income|income_num|income_over_50k|location_\\(census_region\\)_East South Central|location_\\(census_region\\)_Middle Atlantic|location_\\(census_region\\)_Mountain|location_\\(census_region\\)_New England|location_\\(census_region\\)_Pacific|location_\\(census_region\\)_South Atlantic|location_\\(census_region\\)_West North Central|location_\\(census_region\\)_West South Central|respondentid)\")).columns)\ny_pred = df_encoded['income_over_50k']\n\nX_train, X_test, y_train, y_test = train_test_split(X_pred, y_pred, test_size=0.35, random_state=76)\n\nrf = RandomForestClassifier(n_estimators=300 ,random_state=76, max_depth=20, min_samples_split=2)\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\nprint('Accuracy:', accuracy_score(y_test, y_pred))\nprint('\\nClassification Report:\\n', classification_report(y_test, y_pred, target_names=['Under 50k', 'Over 50k']))\n\n\nAccuracy: 0.6432926829268293\n\nClassification Report:\n               precision    recall  f1-score   support\n\n   Under 50k       0.69      0.64      0.67       183\n    Over 50k       0.59      0.64      0.61       145\n\n    accuracy                           0.64       328\n   macro avg       0.64      0.64      0.64       328\nweighted avg       0.65      0.64      0.64       328"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html#stretchtask-5",
    "href": "Cleansing_Exploration/project5.html#stretchtask-5",
    "title": "Client Report - The War with Star Wars",
    "section": "STRETCH|TASK 5",
    "text": "STRETCH|TASK 5\nValidate the data provided on GitHub lines up with the article by recreating a 3rd visual from the article.\nFor this stretch challenge we were asked to recreate one more visual from the article to double check the validity of our data. I chose to recreate the “What’s the Best ‘Star Wars’ Movie?” In this visual we first had to filter the data further to those who have seen all the Star Wars movies. We then had to calculate and display the share of respondents who rated each fil as their favorite.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWhat is the best Star Wars movie?\n# Include and execute your code here\n\nrank_columns = [\n    'rank_i__the_phantom_menace',\n    'rank_ii__attack_of_the_clones',\n    'rank_iii__revenge_of_the_sith',\n    'rank_iv__a_new_hope',\n    'rank_v_the_empire_strikes_back',\n    'rank_vi_return_of_the_jedi',\n]\n\nmovie_titles = [\n    \"The Phantom Menace\",\n    \"Attack of the Clones\",\n    \"Revenge of the Sith\",\n    \"A New Hope\",\n    \"The Empire Strikes Back\",\n    \"Return of the Jedi\",\n]\n\nseen_all = df_filtered[seen_columns].all(axis=1)\ndf_seen_all = df_filtered[seen_all]\n\nfav_counts = [df_seen_all[col].value_counts().get(1, 0) for col in rank_columns]\n\ntotal_resp = df_seen_all.shape[0]\npercent_fav = [(count / total_resp) * 100 for count in fav_counts]\n\ndf_fav = pd.DataFrame({\n  'movie': movie_titles,\n  'percentage': percent_fav\n})\ndf_fav['percent_label'] = df_fav['percentage'].round(0).astype(int).astype(str) + '%'\ndf_fav['movie'] = pd.Categorical(df_fav['movie'], categories=movie_titles[::-1], ordered=True)\n\np5 = (ggplot(df_fav, aes(x='movie', y='percentage')) + geom_bar(stat='identity', fill='blue') + geom_text(aes(label='percent_label'), nudge_y=2, size=10) + labs(title='Favorite Star Wars Movies', subtitle='Of respondents who have seen all six films', x='Percentage of Respondents', y='Star Wars Movies') + coord_flip() + theme(axis_text_x=element_text(hjust=1)))\n\np5.show()"
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "About Me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "competition.html#my-data-science-portfolio",
    "href": "competition.html#my-data-science-portfolio",
    "title": "About Me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "Full_Stack/project2.html",
    "href": "Full_Stack/project2.html",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Using the data collected from the Bureau of Transportation Statistics I have determined what airports are the best to fly out of and when to fly in general to have the best chance of avoiding delays for several reasons altogether. Chicago O’Hare International Airport (ORD) was determined to be the worst airport based on the average length of delay (1.13 Hours). ORD also had the second highest proportion of delays due to weather out of all the airports I looked at. The highest proportion of delays due to weather was San Francisco International Airport (SFO). I also determined that September has the lowest proportion of total flights delayed. The holiday months (January, July, November, and December) have the highest likelihood of experiencing delays\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\"\ndf = pd.read_json(url)\n\nAirport_nan = df['airport_name'].isna().sum()\nyear_nan = df['year'].isna().sum()\nmin_delay_nan = df['minutes_delayed_carrier'].isna().sum()\nmin_delay_nas_nan = df['minutes_delayed_nas'].isna().sum()\n\nyear_nan\n\n\nnp.int64(23)\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "Full_Stack/project2.html#elevator-pitch",
    "href": "Full_Stack/project2.html#elevator-pitch",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Using the data collected from the Bureau of Transportation Statistics I have determined what airports are the best to fly out of and when to fly in general to have the best chance of avoiding delays for several reasons altogether. Chicago O’Hare International Airport (ORD) was determined to be the worst airport based on the average length of delay (1.13 Hours). ORD also had the second highest proportion of delays due to weather out of all the airports I looked at. The highest proportion of delays due to weather was San Francisco International Airport (SFO). I also determined that September has the lowest proportion of total flights delayed. The holiday months (January, July, November, and December) have the highest likelihood of experiencing delays\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\"\ndf = pd.read_json(url)\n\nAirport_nan = df['airport_name'].isna().sum()\nyear_nan = df['year'].isna().sum()\nmin_delay_nan = df['minutes_delayed_carrier'].isna().sum()\nmin_delay_nas_nan = df['minutes_delayed_nas'].isna().sum()\n\nyear_nan\n\n\nnp.int64(23)\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "Full_Stack/project2.html#questiontask-1",
    "href": "Full_Stack/project2.html#questiontask-1",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”).\nI went through and made a new data frame for me to preform some cleaning. I then went through and searched for all the blanks, 1500+ minutes, and values that had -999 in them. then used the “.replace” to get them out and replace them with “NaN”.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n::: {#cell-Q1-Clean Data .cell execution_count=4}\n\nCleaned data\n# Include and execute your code here\ndf_clean = df\n\ndf_clean.replace([\"\", \"1500+\", -999], np.nan, inplace=True)\n\nexample = df_clean.iloc[2].to_json()\nexample\n\n\n'{\"airport_code\":\"IAD\",\"airport_name\":null,\"month\":\"January\",\"year\":2005.0,\"num_of_flights_total\":12381,\"num_of_delays_carrier\":\"414\",\"num_of_delays_late_aircraft\":1058.0,\"num_of_delays_nas\":895,\"num_of_delays_security\":4,\"num_of_delays_weather\":61,\"num_of_delays_total\":2430,\"minutes_delayed_carrier\":null,\"minutes_delayed_late_aircraft\":70919,\"minutes_delayed_nas\":35660.0,\"minutes_delayed_security\":208,\"minutes_delayed_weather\":4497,\"minutes_delayed_total\":134881}'\nCleaned Data\n\n:::",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "Full_Stack/project2.html#questiontask-2",
    "href": "Full_Stack/project2.html#questiontask-2",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays?\nI chose to look at the average time each flight is delayed determining the worst airport delays. In the table you will find the total delayed flights and total delayed minutes for each airport. The portion of delayed flights measure tells us what percentage of flights out of that airport are delayed. The last column displays the average hours each flight is delayed. This helped me determine that Chicago O’Hare International Airport (ORD) is the worst airport to fly out of.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nThe orst airport based on average delay hours\n# Include and execute your code here\n\ndf_clean_group = df_clean\n\ngrouped_data = df_clean_group.groupby('airport_code').agg(\n  total_flights=('num_of_flights_total', 'sum'),\n  total_delays=('num_of_delays_total', 'sum'),\n  total_delay_minutes=('minutes_delayed_total', 'sum')\n).reset_index()\n\ngrouped_data['proportion_delayed_flights'] = (grouped_data['total_delays'] / grouped_data['total_flights']) * 100\n\ngrouped_data['average_delay_hours'] = (grouped_data['total_delay_minutes'] / grouped_data['total_delays']) / 60\n\nworst_airport = grouped_data.loc[grouped_data['average_delay_hours'].idxmax()]\n\n\nworst_airport\n\n\nairport_code                        ORD\ntotal_flights                   3597588\ntotal_delays                     830825\ntotal_delay_minutes            56356129\nproportion_delayed_flights    23.093945\naverage_delay_hours            1.130525\nName: 3, dtype: object\nWorst Airport\n\n\n::: {#cell-Q2-Airport Tables .cell .tbl-cap-location-top tbl-cap=‘Airport Delays’ execution_count=7}\n\nAirport delays table\n# Include and execute your code here\ngrouped_data\n\n\n\n\n\n\n\n\n\nairport_code\ntotal_flights\ntotal_delays\ntotal_delay_minutes\nproportion_delayed_flights\naverage_delay_hours\n\n\n\n\n0\nATL\n4430047\n902443\n53983926\n20.370958\n0.996996\n\n\n1\nDEN\n2513974\n468519\n25173381\n18.636589\n0.895495\n\n\n2\nIAD\n851571\n168467\n10283478\n19.783083\n1.017358\n\n\n3\nORD\n3597588\n830825\n56356129\n23.093945\n1.130525\n\n\n4\nSAN\n917862\n175132\n8276248\n19.080428\n0.787620\n\n\n5\nSFO\n1630945\n425604\n26550493\n26.095546\n1.039718\n\n\n6\nSLC\n1403384\n205160\n10123371\n14.618950\n0.822396\n\n\n\n\n\n\n:::",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "Full_Stack/project2.html#questiontask-3",
    "href": "Full_Stack/project2.html#questiontask-3",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length?\nI chose to look at the proportion of delayed flights to determine what month is the best month to fly. I chose this as my metric because I feel this paints an accurate picture about what the chances are of your flight being delayed dependent on month. From what I found, the best month to fly is September. 16.5% of all the flights that occur in September end up being delayed.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nProportion of delayed flights by month\n# Include and execute your code here\ndf_mo = df_clean\n\ndf_mo['month'] = df_mo['month'].replace('Febuary', 'February')\n\ndf_clean_month = df_mo[df['month'].notna()]\n\nmonth_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n\ndf_clean_month['month'] = pd.Categorical(df_clean_month['month'], categories=month_order, ordered=True)\n\nmonthly_delays = df_clean_month.groupby('month').agg(\n  total_flights=('num_of_flights_total', 'sum'),\n  total_delays=('num_of_delays_total', 'sum'),\n  total_delay_minutes=('minutes_delayed_total', 'sum')\n).reset_index()\n\nmonthly_delays['proportion_delayed_flights'] = (monthly_delays['total_delays'] / monthly_delays['total_flights']) * 100\n\n(\n  ggplot(monthly_delays, aes(x='month', y='proportion_delayed_flights')) +\n  geom_bar(stat='identity') +\n  ggtitle('Proportion of Delayed Flights by Month') +\n  xlab('Month') +\n  ylab('Proportion of Delayed Flights (%)') +\n  theme(axis_text_x=element_text(angle=45, hjust=1))\n)\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nProportion of Delayed Flights by Month",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "Full_Stack/project2.html#questiontask-4",
    "href": "Full_Stack/project2.html#questiontask-4",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nYour job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild).\nI was able to create the new row and add it to the table. The column “weather_related_delays” will be used for the next question.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWeather delays\n# Include and execute your code here\n\ndf_clean_two = df_clean\n\ndf_clean_two['num_of_delays_late_aircraft'] = df_clean_two['num_of_delays_late_aircraft'].replace(-999, np.nan)\n\ndf_clean_two['num_of_delays_late_aircraft'].fillna(df['num_of_delays_late_aircraft'].mean(), inplace=True)\n\ndef weather_nas_delays(row):\n  if row['month'] in ['April', 'May', 'June', 'July', 'August']:\n    return 0.4 * round(row['num_of_delays_nas'], 2)\n  else:\n    return 0.65 * round(row['num_of_delays_nas'], 2)\n\ndf_clean_two['weather_related_delays'] = (\n  df_clean_two['num_of_delays_weather'] +\n  round(0.3 * df_clean['num_of_delays_late_aircraft'], 2) +\n  df_clean_two.apply(weather_nas_delays, axis=1)\n)\n\ndf_clean_two.head(5)\n\n\n\n\n\n\n\n\n\nairport_code\nairport_name\nmonth\nyear\nnum_of_flights_total\nnum_of_delays_carrier\nnum_of_delays_late_aircraft\nnum_of_delays_nas\nnum_of_delays_security\nnum_of_delays_weather\nnum_of_delays_total\nminutes_delayed_carrier\nminutes_delayed_late_aircraft\nminutes_delayed_nas\nminutes_delayed_security\nminutes_delayed_weather\nminutes_delayed_total\nweather_related_delays\n\n\n\n\n0\nATL\nAtlanta, GA: Hartsfield-Jackson Atlanta Intern...\nJanuary\n2005.0\n35048\nNaN\n1109.104072\n4598\n10\n448\n8355\n116423.0\n104415\n207467.0\n297\n36931\n465533\n3769.43\n\n\n1\nDEN\nDenver, CO: Denver International\nJanuary\n2005.0\n12687\n1041\n928.000000\n935\n11\n233\n3153\n53537.0\n70301\n36817.0\n363\n21779\n182797\n1119.15\n\n\n2\nIAD\nNaN\nJanuary\n2005.0\n12381\n414\n1058.000000\n895\n4\n61\n2430\nNaN\n70919\n35660.0\n208\n4497\n134881\n960.15\n\n\n3\nORD\nChicago, IL: Chicago O'Hare International\nJanuary\n2005.0\n28194\n1197\n2255.000000\n5415\n5\n306\n9178\n88691.0\n160811\n364382.0\n151\n24859\n638894\n4502.25\n\n\n4\nSAN\nSan Diego, CA: San Diego International\nJanuary\n2005.0\n7283\n572\n680.000000\n638\n7\n56\n1952\n27436.0\n38445\n21127.0\n218\n4326\n91552\n674.70\n\n\n\n\n\nWeather Delays",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "Full_Stack/project2.html#questiontask-5",
    "href": "Full_Stack/project2.html#questiontask-5",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Describe what you learn from this graph.\nAccording the the data San Francisco International Airport (SFO) experiences the highest proportion of delays due to bad weather days. Approximately 9.81% of the flights from SFO are delayed due to weather. Upon further research I found that San Francisco experiences frequent oceanic fog that causes delays and groundings for outbound flights. The next airport to avoid flying out of is Chicago O’Hare International Airport (ORD) with 8.5% of their flights become delayed.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWeather delays by airport\n# Include and execute your code here\ndf_clean_three = df_clean_two\n\ndf_clean_three['proportion_weather_delays'] = (df_clean_three['weather_related_delays'] / df_clean_three['num_of_flights_total']) * 100\n\nairport_weather_delays = df_clean_three.groupby('airport_code').agg(proportion_weather_delays=('proportion_weather_delays', 'mean')\n).reset_index()\n\n(\n  ggplot(airport_weather_delays, aes(x='airport_code', y='proportion_weather_delays')) +\n  geom_bar(stat='identity') +\n  ggtitle('Proportion of Weather Delays by Airport') +\n  xlab('Airport Code') +\n  ylab('Proportion of Weather Delays (%)') +\n  theme(axis_text_x=element_text(angle=45, hjust=1))\n)\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nProportion of Weather Delays by Airport",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "Full_Stack/project3.html#elevator-pitch",
    "href": "Full_Stack/project3.html#elevator-pitch",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nIn analyzing all the baseball data, we were able to gain a few valuable key insights. First, we analyzed all the players that had BYU-Idaho listed as their college. We looked at what team they played for, what year they played for the team, and how much their salary was. Next, we identified and raked players batting averages using three different parameters. We found that as we include more data over years of players careers, we can get closer to the true batting average. Lastly, we compared two teams, the Los Angeles Dodgers and the New York Yankees, total wins from 1985 to the latest year of the data. We observed that the New York Yankees had a higher frequency of wins than the Los Angeles Dodgers.\n\n\nRead and format project data\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)",
    "crumbs": [
      "Full Stack",
      "Project 3"
    ]
  },
  {
    "objectID": "Full_Stack/project3.html#questiontask-1",
    "href": "Full_Stack/project3.html#questiontask-1",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nThe table displays the highest salaries reported by the players that came from BYU-Idaho. Matt Lindstrom (Lindsma01) had the most success out of all the baseball players that went to school in BYU-Idaho. Matt Lindstrom’s highest paid year on record was with the Chicago White Sox in 2014. Matt had a salary with the White Sox of $4 million.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\n\n\nBYU-Idaho Players Salaries\n# Include and execute your code here\nq_1 = \"\"\"\nselect distinct p.playerID, c.schoolID, printf('$%,.0f',s.salary) as 'Salary', s.yearID, s.teamID\nfrom collegeplaying c\njoin Salaries s on c.playerID = s.playerID\njoin People p on c.playerID = p.playerID\nwhere c.schoolID = 'idbyuid'\norder by s.salary desc\nlimit 10\n\"\"\"\nsalary_df = pd.read_sql_query(q_1,con)\ndisplay(salary_df)\n\n\n\n\n\n\n\n\n\nplayerID\nschoolID\nSalary\nyearID\nteamID\n\n\n\n\n0\nlindsma01\nidbyuid\n$4000000\n2014\nCHA\n\n\n1\nlindsma01\nidbyuid\n$3600000\n2012\nBAL\n\n\n2\nlindsma01\nidbyuid\n$2800000\n2011\nCOL\n\n\n3\nlindsma01\nidbyuid\n$2300000\n2013\nCHA\n\n\n4\nlindsma01\nidbyuid\n$1625000\n2010\nHOU\n\n\n5\nstephga01\nidbyuid\n$1025000\n2001\nSLN\n\n\n6\nstephga01\nidbyuid\n$900000\n2002\nSLN\n\n\n7\nstephga01\nidbyuid\n$800000\n2003\nSLN\n\n\n8\nstephga01\nidbyuid\n$550000\n2000\nSLN\n\n\n9\nlindsma01\nidbyuid\n$410000\n2009\nFLO\n\n\n\n\n\nBYU-Idaho Players Salaries",
    "crumbs": [
      "Full Stack",
      "Project 3"
    ]
  },
  {
    "objectID": "Full_Stack/project3.html#questiontask-2",
    "href": "Full_Stack/project3.html#questiontask-2",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\nPart 1: Write an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\nFrom part one we can see that when we include all those players who had at least at bat for the year their batting average for the year was perfect. All the top 5 players from part one had a batting average of 100%.\n::: {#cell-Q2 Part 1 .cell execution_count=6}\n\nBatting average\n# Include and execute your code here\nq2_1 = \"\"\"\nselect playerID, yearID, (h*1.0)/ab as 'BA'\nfrom batting\nwhere ab &gt;= 1\norder by (h*1.0)/ab desc, playerID asc\nlimit 5\n\"\"\"\nba_part_1 = pd.read_sql_query(q2_1,con)\ndisplay(ba_part_1)\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nBA\n\n\n\n\n0\naberal01\n1957\n1.0\n\n\n1\nabernte02\n1960\n1.0\n\n\n2\nabramge01\n1923\n1.0\n\n\n3\nacklefr01\n1964\n1.0\n\n\n4\nalanirj01\n2019\n1.0\n\n\n\n\n\nTop 5 Batting Averages\n\n:::\nPart 2: Use the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\nFor part two we start to whittle down the results, instead of players with at least one at bat we increase the threshold to at least 10 at bats. Now, our best batting average was only 64% instead of players have a 100% batting average.\n::: {#cell-Q2 Part 2 .cell execution_count=7}\n\nBatting average\n# Include and execute your code here\nq2_2 = \"\"\"\nselect playerID, yearID, (h*1.0)/ab as 'BA'\nfrom batting\nwhere ab &gt;= 10\norder by (h*1.0)/ab desc, playerID\nlimit 5\n\"\"\"\nba_part_2 = pd.read_sql_query(q2_2,con)\ndisplay(ba_part_2)\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nBA\n\n\n\n\n0\nnymanny01\n1974\n0.642857\n\n\n1\ncarsoma01\n2013\n0.636364\n\n\n2\naltizda01\n1910\n0.600000\n\n\n3\njohnsde01\n1975\n0.600000\n\n\n4\nsilvech01\n1948\n0.571429\n\n\n\n\n\nTop 5 Batting Averages\n\n:::\nPart 3: Now calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\nFor part three we whittled down even further by obtaining players lifetime batting average across all the years they played. The highest player had a batting average of only 35.8%. Following the progression of the parameters. When we widen the number of at bats and hits, it becomes clear that the best players do not always have a 100% batting average like we saw in part one\n::: {#cell-Q2 Part 3 .cell execution_count=8}\n\nBatting average\n# Include and execute your code here\nq2_3 = \"\"\"\nselect playerID, sum(h*1.0)/sum(ab) as 'BA'\nfrom batting\nwhere ab &gt;= 100\ngroup by playerID\norder by (h*1.0)/ab desc, playerID\nlimit 5\n\"\"\"\nba_part_3 = pd.read_sql_query(q2_3,con)\ndisplay(ba_part_3)\n\n\n\n\n\n\n\n\n\nplayerID\nBA\n\n\n\n\n0\nmeyerle01\n0.357542\n\n\n1\nmcveyca01\n0.345802\n\n\n2\njacksjo01\n0.357009\n\n\n3\nhazlebo01\n0.402985\n\n\n4\nbarnero01\n0.363201\n\n\n\n\n\nTop 5 Batting Averages\n\n:::",
    "crumbs": [
      "Full Stack",
      "Project 3"
    ]
  },
  {
    "objectID": "Full_Stack/project3.html#questiontask-3",
    "href": "Full_Stack/project3.html#questiontask-3",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Lets-Plot to visualize the comparison. What do you learn?\nI chose to use total wins for each year to compare the Los Angeles Dodgers (LAN) to the total wins for the New York Yankees (NYA). I chose this measure to compare the teams because I feel as though looking at the wins of the two teams gives a great overall comparison of the success of the team. In examining the data and graph I concluded that the New York Yankees had successful year more frequently than the Los Angeles Dodgers. Both teams had their good years and bad years. Overall, the New York Yankees won more frequently than the Los Angeles Dodgers.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\nfrom lets_plot import *\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nTable of team wins\n# Include and execute your code here\n\nq_3 = \"\"\"\nselect teamID, yearID, sum(w) as 'Total_Wins'\nfrom Teams\nwhere teamID in ('NYA', 'LAN') and yearID &gt;= 1985\ngroup by teamID, yearID\norder by yearID asc\n\"\"\"\n\nwins_df = pd.read_sql_query(q_3,con)\nwins_df.tail(14)\n\n\n\n\n\n\n\n\n\nteamID\nyearID\nTotal_Wins\n\n\n\n\n56\nLAN\n2013\n92\n\n\n57\nNYA\n2013\n85\n\n\n58\nLAN\n2014\n94\n\n\n59\nNYA\n2014\n84\n\n\n60\nLAN\n2015\n92\n\n\n61\nNYA\n2015\n87\n\n\n62\nLAN\n2016\n91\n\n\n63\nNYA\n2016\n84\n\n\n64\nLAN\n2017\n104\n\n\n65\nNYA\n2017\n91\n\n\n66\nLAN\n2018\n92\n\n\n67\nNYA\n2018\n100\n\n\n68\nLAN\n2019\n106\n\n\n69\nNYA\n2019\n103\n\n\n\n\n\nWins by Team\n\n\n\n\nGraph of wins\n# Include and execute your code here\n\n(ggplot(wins_df, aes(x='yearID', y='Total_Wins', color='teamID')) + geom_line() + ggtitle('Total Wins by Team') + xlab('Year') + ylab('Total Wins') + scale_x_continuous(format='d') + theme(plot_title=element_text(hjust=0.5))\n)",
    "crumbs": [
      "Full Stack",
      "Project 3"
    ]
  },
  {
    "objectID": "Full_Stack/project3.html#questiontaskstretch-4",
    "href": "Full_Stack/project3.html#questiontaskstretch-4",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK(STRETCH) 4",
    "text": "QUESTION|TASK(STRETCH) 4\nAdvanced Salary Distribution by Position\nFrom this we can see that the on average the highest paid position is “Designated Hitter”. While this position has the highest average salary, when we look at the higest (max) salary for each position we find that “Third Baseman” and “Pitcher” are tied for the higest salary at $33 million. All positions are categorized as “High Salary”.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\n\n\nAverage salary by position\n# Include and execute your code here\n\nq_4 = \"\"\"\nselect \n  p.position, printf('$%,.0f',avg(s.salary)) as 'Average_Salary', count(distinct s.playerID) as 'Total_Players',\n  coalesce(printf('$%,.0f',max(s.salary)), 'N/A') as 'Highest_Salary',\n  case \n    when avg(s.salary) &gt; 1000000 then 'High Salary'\n    when avg(s.salary) between 5000000 and 1000000 then 'Medium salary'\n    else 'Low Salary'\n  end as salary_category\nfrom Salaries s\njoin appearances a on s.playerID = a.playerID\njoin(\n  select playerID, max(position) as position\n  from (\n    select playerID,\n      case\n        when G_p &gt; 0 then 'Pitcher'\n        when G_c &gt; 0 then 'Catcher'\n        when G_1b &gt; 0 then 'First Baseman'\n        when G_2b &gt; 0 then 'Second Baseman'\n        when G_3b &gt; 0 then 'Third Baseman'\n        when G_ss &gt; 0 then 'Shortstop'\n        when G_lf &gt; 0 then 'Left Fielder'\n        when G_cf &gt; 0 then 'Center Fielder'\n        when G_rf &gt; 0 then 'Right Fielder'\n        when G_of &gt; 0 then 'Outfielder'\n        when G_dh &gt; 0 then 'Designated Hitter'\n        when G_ph &gt; 0 then 'Pinch Hitter'\n        when G_pr &gt; 0 then 'Pinch Runner'\n        \n      end as position\n    from appearances\n  )\n  group by playerID\n) p on s.playerID = p.playerID\ngroup by p.position\norder by avg(s.salary) desc\nlimit 10\n\"\"\"\n\nsalary_df = pd.read_sql_query(q_4,con)\ndisplay(salary_df)\n\n\n\n\n\n\n\n\n\nposition\nAverage_Salary\nTotal_Players\nHighest_Salary\nsalary_category\n\n\n\n\n0\nDesignated Hitter\n$6625195\n7\n$16071429\nHigh Salary\n\n\n1\nFirst Baseman\n$4086740\n248\n$25000000\nHigh Salary\n\n\n2\nRight Fielder\n$3713566\n233\n$23854494\nHigh Salary\n\n\n3\nThird Baseman\n$2687984\n487\n$33000000\nHigh Salary\n\n\n4\nLeft Fielder\n$2640393\n521\n$27328046\nHigh Salary\n\n\n5\nShortstop\n$2500582\n169\n$22600000\nHigh Salary\n\n\n6\nPitcher\n$2364202\n2745\n$33000000\nHigh Salary\n\n\n7\nSecond Baseman\n$2020024\n370\n$24000000\nHigh Salary\n\n\n8\nCatcher\n$1617095\n298\n$20777778\nHigh Salary\n\n\n9\nCenter Fielder\n$1556184\n15\n$3900000\nHigh Salary\n\n\n\n\n\nAverage Salary by Position",
    "crumbs": [
      "Full Stack",
      "Project 3"
    ]
  },
  {
    "objectID": "Machine_Learning/project1.html",
    "href": "Machine_Learning/project1.html",
    "title": "Client Report - What’s in a Name?",
    "section": "",
    "text": "What’s in a Name?\n\n\n\n\nElevator pitch\nIn comparing baby names and birth years looking for trends, I have learned that my name “Braxton” was not all that popular when I was born. If you were to get a call from a “Brittney” on the phone, it would be safe to assume that her age would be 34 years old. I also analyzed the effect of certain events on popularity of names.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\ndf \n\n\n\n\n\n\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n0\nAaden\n2005\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n1\nAaden\n2007\n0.0\n5.0\n0.0\n5.0\n20.0\n6.0\n0.0\n0.0\n...\n5.0\n14.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n98.0\n\n\n2\nAaden\n2008\n0.0\n15.0\n13.0\n20.0\n135.0\n10.0\n9.0\n0.0\n...\n15.0\n99.0\n8.0\n26.0\n0.0\n11.0\n19.0\n5.0\n0.0\n939.0\n\n\n3\nAaden\n2009\n0.0\n28.0\n20.0\n23.0\n158.0\n22.0\n12.0\n0.0\n...\n33.0\n140.0\n6.0\n17.0\n0.0\n31.0\n23.0\n14.0\n0.0\n1242.0\n\n\n4\nAaden\n2010\n0.0\n8.0\n6.0\n12.0\n62.0\n9.0\n5.0\n0.0\n...\n9.0\n56.0\n0.0\n11.0\n0.0\n7.0\n12.0\n0.0\n0.0\n414.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n393379\nZyon\n2011\n0.0\n0.0\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n...\n5.0\n9.0\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n117.0\n\n\n393380\nZyon\n2012\n0.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n7.5\n0.0\n12.0\n0.0\n0.0\n0.0\n0.0\n0.0\n118.5\n\n\n393381\nZyon\n2013\n0.0\n0.0\n0.0\n0.0\n6.0\n0.0\n0.0\n0.0\n...\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n93.0\n\n\n393382\nZyon\n2014\n0.0\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n...\n0.0\n7.0\n0.0\n9.0\n0.0\n0.0\n0.0\n0.0\n0.0\n73.0\n\n\n393383\nZyon\n2015\n0.0\n0.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n0.0\n10.0\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n87.5\n\n\n\n\n393384 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 1\nHow does your name at your birth year compare to its use historically?\nHistoricly it looks like my name was given after a slight rise in popularity. However, I was given my name before the largest spike after about 2002.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nPlot of the name Braxton\n# Include and execute your code here\nmy_name_df = df[df['name'] == 'Braxton']\nyear_2000_data = my_name_df[my_name_df['year'] == 2000]\n\n(ggplot(my_name_df, aes(x='year', y='Total')) +\n    geom_point(color = 'blue') +\n    ggtitle('The Use of Braxton') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5)) +\n    geom_text(aes(x=year_2000_data['year'], y=year_2000_data['Total']), label='782', nudge_x=-0.5, nudge_y=400) +\n    geom_segment(aes(x=year_2000_data['year'], xend=year_2000_data['year'], y=year_2000_data['Total'] - 10, yend=year_2000_data['Total']+350), arrow=arrow(length=5, type='closed'), color='red')\n)\n\n\n   \n   \nThe Use of Braxton\n\n\n\n\ntable of the name Braxton\ndisplay(my_name_df)\n\n\n\n\n\n\nBraxton’s Name Usage\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n50029\nBraxton\n1914\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n13.0\n\n\n50030\nBraxton\n1915\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n11.0\n\n\n50031\nBraxton\n1916\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12.0\n\n\n50032\nBraxton\n1917\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n50033\nBraxton\n1918\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n10.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50095\nBraxton\n2011\n6.0\n123.0\n84.0\n37.0\n62.0\n43.0\n0.0\n0.0\n...\n131.0\n188.0\n93.0\n40.0\n0.0\n33.0\n35.0\n68.0\n16.0\n2491.0\n\n\n50096\nBraxton\n2012\n5.0\n111.0\n79.0\n37.0\n61.0\n39.0\n5.0\n0.0\n...\n151.0\n227.0\n65.0\n58.0\n0.0\n41.0\n62.0\n95.0\n22.0\n2986.0\n\n\n50097\nBraxton\n2013\n0.0\n100.0\n84.0\n53.0\n92.0\n49.0\n7.0\n0.0\n...\n158.0\n226.0\n63.0\n64.0\n0.0\n42.0\n62.0\n74.0\n9.0\n3085.0\n\n\n50098\nBraxton\n2014\n10.0\n121.0\n86.0\n45.0\n106.0\n43.0\n6.0\n0.0\n...\n150.0\n147.0\n79.0\n60.0\n0.0\n39.0\n56.0\n74.0\n20.0\n3186.0\n\n\n50099\nBraxton\n2015\n0.0\n103.0\n76.0\n48.0\n102.0\n38.0\n6.0\n0.0\n...\n173.0\n278.0\n54.0\n61.0\n0.0\n57.0\n74.0\n67.0\n9.0\n3265.0\n\n\n\n\n71 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nIt appears that the most popular year for the use of the name “Brittney” was 1990. This means that we could make a educated guess that a person named Britteny would be 34 years old.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nBar chart of the name Brittany\nbrittany_df = df[df['name'] == 'Brittany']\n\n(ggplot(brittany_df, aes(x='year', y='Total')) +\n    geom_bar(stat='identity') +\n    ggtitle('The Popularity of Brittney Over Time') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nThe Popularity of Brittney Over Time\n\n\n\n\nTable of the name Brittany\n# Include and execute your code here\n\ndisplay(brittany_df)\n\n\n\n\n\n\nBrittany’s Name Usage\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n53205\nBrittany\n1968\n0.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n53206\nBrittany\n1969\n0.0\n0.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12.0\n\n\n53207\nBrittany\n1970\n0.0\n0.0\n0.0\n0.0\n5.0\n5.0\n0.0\n0.0\n...\n0.0\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n32.0\n\n\n53208\nBrittany\n1971\n0.0\n0.0\n0.0\n5.0\n17.0\n0.0\n0.0\n0.0\n...\n0.0\n14.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n81.0\n\n\n53209\nBrittany\n1972\n0.0\n0.0\n0.0\n0.0\n11.0\n10.0\n0.0\n0.0\n...\n8.0\n14.0\n16.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n158.0\n\n\n53210\nBrittany\n1973\n0.0\n0.0\n0.0\n6.0\n17.0\n0.0\n0.0\n0.0\n...\n0.0\n18.0\n13.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n166.0\n\n\n53211\nBrittany\n1974\n0.0\n0.0\n7.0\n0.0\n19.0\n0.0\n0.0\n0.0\n...\n0.0\n17.0\n28.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n198.0\n\n\n53212\nBrittany\n1975\n0.0\n0.0\n0.0\n0.0\n28.0\n0.0\n0.0\n0.0\n...\n10.0\n23.0\n28.0\n0.0\n0.0\n9.0\n5.0\n0.0\n0.0\n277.0\n\n\n53213\nBrittany\n1976\n0.0\n0.0\n6.0\n0.0\n36.0\n5.0\n0.0\n0.0\n...\n15.0\n35.0\n21.0\n7.0\n0.0\n7.0\n11.0\n0.0\n0.0\n304.0\n\n\n53214\nBrittany\n1977\n0.0\n9.0\n8.0\n5.0\n39.0\n10.0\n0.0\n0.0\n...\n16.0\n48.0\n55.0\n5.0\n0.0\n0.0\n7.0\n0.0\n0.0\n448.0\n\n\n53215\nBrittany\n1978\n0.0\n8.0\n12.0\n8.0\n50.0\n17.0\n0.0\n0.0\n...\n15.0\n48.0\n67.0\n7.0\n0.0\n11.0\n5.0\n0.0\n0.0\n592.0\n\n\n53216\nBrittany\n1979\n0.0\n6.0\n13.0\n0.0\n47.0\n19.0\n10.0\n0.0\n...\n20.0\n76.0\n63.0\n14.0\n0.0\n13.0\n10.0\n7.0\n8.0\n764.0\n\n\n53217\nBrittany\n1980\n0.0\n11.0\n20.0\n13.0\n121.0\n41.0\n5.0\n0.0\n...\n39.0\n142.0\n110.0\n18.0\n0.0\n36.0\n28.0\n10.0\n10.0\n1383.0\n\n\n53218\nBrittany\n1981\n11.0\n23.0\n32.0\n22.0\n127.0\n37.0\n14.0\n0.0\n...\n54.0\n166.0\n116.0\n21.0\n0.0\n47.0\n24.0\n10.0\n14.0\n1701.0\n\n\n53219\nBrittany\n1982\n12.0\n50.0\n53.0\n47.0\n290.0\n88.0\n18.0\n5.0\n...\n102.0\n300.0\n152.0\n50.0\n7.0\n73.0\n58.0\n25.0\n16.0\n3093.0\n\n\n53220\nBrittany\n1983\n20.0\n71.0\n80.0\n66.0\n406.0\n131.0\n26.0\n19.0\n...\n133.0\n415.0\n156.0\n83.0\n7.0\n95.0\n68.0\n42.0\n19.0\n4377.0\n\n\n53221\nBrittany\n1984\n34.0\n158.0\n111.0\n112.0\n698.0\n225.0\n59.0\n22.0\n...\n243.0\n735.0\n159.0\n148.0\n6.0\n134.0\n114.0\n78.0\n36.0\n7664.0\n\n\n53222\nBrittany\n1985\n53.0\n351.0\n224.0\n217.0\n715.5\n319.0\n118.0\n48.0\n...\n500.0\n1256.0\n182.0\n390.0\n25.0\n231.0\n260.0\n235.0\n40.0\n14010.0\n\n\n53223\nBrittany\n1986\n49.0\n241.5\n269.0\n309.0\n886.5\n389.0\n165.0\n79.0\n...\n586.0\n792.5\n183.0\n533.0\n45.0\n287.0\n361.0\n229.0\n48.0\n17856.5\n\n\n53224\nBrittany\n1987\n56.0\n528.0\n267.0\n322.0\n977.0\n365.0\n196.0\n97.0\n...\n633.0\n795.5\n193.0\n628.0\n34.0\n289.0\n351.0\n241.0\n42.0\n18825.5\n\n\n53225\nBrittany\n1988\n48.0\n585.0\n304.0\n383.0\n1214.5\n370.0\n271.0\n126.0\n...\n409.5\n925.0\n167.0\n884.0\n43.0\n368.0\n455.0\n270.0\n41.0\n21952.0\n\n\n53226\nBrittany\n1989\n53.0\n837.0\n457.0\n549.0\n1513.0\n439.0\n371.0\n86.5\n...\n537.5\n1266.0\n185.0\n1247.0\n71.0\n500.0\n724.0\n402.0\n50.0\n30848.0\n\n\n53227\nBrittany\n1990\n75.0\n788.0\n440.0\n483.0\n1372.5\n450.0\n345.0\n90.0\n...\n546.5\n1181.5\n173.0\n1229.0\n86.0\n470.0\n671.0\n379.0\n38.0\n32562.5\n\n\n53228\nBrittany\n1991\n55.0\n585.0\n327.0\n379.0\n1155.5\n361.0\n293.0\n132.0\n...\n872.0\n983.0\n140.0\n959.0\n47.0\n363.0\n556.0\n352.0\n43.0\n26963.5\n\n\n53229\nBrittany\n1992\n51.0\n526.0\n282.0\n292.0\n915.0\n325.0\n223.0\n87.0\n...\n726.0\n1785.0\n119.0\n854.0\n57.0\n307.0\n465.0\n292.0\n40.0\n23416.5\n\n\n53230\nBrittany\n1993\n37.0\n511.0\n229.0\n298.0\n1597.0\n275.0\n199.0\n62.0\n...\n637.0\n1547.0\n123.0\n697.0\n44.0\n286.0\n389.0\n242.0\n26.0\n21728.0\n\n\n53231\nBrittany\n1994\n47.0\n422.0\n237.0\n227.0\n666.5\n227.0\n168.0\n72.0\n...\n599.0\n1359.0\n91.0\n641.0\n43.0\n229.0\n351.0\n228.0\n32.0\n17808.5\n\n\n53232\nBrittany\n1995\n34.0\n342.0\n190.0\n202.0\n605.5\n179.0\n156.0\n49.0\n...\n516.0\n1293.0\n110.0\n537.0\n40.0\n216.0\n271.0\n194.0\n19.0\n15875.5\n\n\n53233\nBrittany\n1996\n29.0\n318.0\n163.0\n195.0\n977.0\n163.0\n133.0\n31.0\n...\n400.0\n1147.0\n107.0\n455.0\n25.0\n166.0\n239.0\n147.0\n18.0\n13796.0\n\n\n53234\nBrittany\n1997\n19.0\n253.0\n166.0\n138.0\n825.0\n141.0\n111.0\n24.0\n...\n342.0\n966.0\n85.0\n334.0\n19.0\n147.0\n230.0\n102.0\n7.0\n11527.0\n\n\n53235\nBrittany\n1998\n29.0\n190.0\n109.0\n165.0\n735.0\n134.0\n85.0\n22.0\n...\n324.0\n828.0\n75.0\n310.0\n23.0\n121.0\n170.0\n114.0\n11.0\n9843.0\n\n\n53236\nBrittany\n1999\n18.0\n163.0\n97.0\n114.0\n607.0\n135.0\n69.0\n20.0\n...\n265.0\n698.0\n70.0\n246.0\n13.0\n104.0\n126.0\n83.0\n11.0\n7942.0\n\n\n53237\nBrittany\n2000\n14.0\n111.0\n63.0\n88.0\n354.0\n71.0\n33.0\n14.0\n...\n199.0\n432.0\n41.0\n176.0\n9.0\n59.0\n91.0\n71.0\n6.0\n5183.0\n\n\n53238\nBrittany\n2001\n6.0\n92.0\n43.0\n60.0\n223.0\n35.0\n27.0\n9.0\n...\n109.0\n287.0\n21.0\n83.0\n5.0\n33.0\n33.0\n50.0\n0.0\n2915.0\n\n\n53239\nBrittany\n2002\n0.0\n39.0\n28.0\n32.0\n144.0\n32.0\n9.0\n7.0\n...\n66.0\n206.0\n22.0\n70.0\n5.0\n16.0\n30.0\n29.0\n0.0\n1912.0\n\n\n53240\nBrittany\n2003\n0.0\n32.0\n22.0\n25.0\n148.0\n23.0\n16.0\n5.0\n...\n54.0\n177.0\n16.0\n42.0\n0.0\n15.0\n25.0\n19.0\n0.0\n1559.0\n\n\n53241\nBrittany\n2004\n0.0\n31.0\n23.0\n24.0\n139.0\n20.0\n0.0\n5.0\n...\n50.0\n137.0\n16.0\n37.0\n0.0\n14.0\n18.0\n28.0\n6.0\n1323.5\n\n\n53242\nBrittany\n2005\n0.0\n28.0\n18.0\n29.0\n116.0\n24.0\n7.0\n0.0\n...\n45.0\n148.0\n18.0\n35.0\n0.0\n9.0\n17.0\n13.0\n0.0\n1168.0\n\n\n53243\nBrittany\n2006\n0.0\n20.0\n13.0\n20.0\n121.0\n12.0\n6.0\n0.0\n...\n41.0\n103.0\n9.0\n30.0\n0.0\n10.0\n14.0\n9.0\n0.0\n1009.0\n\n\n53244\nBrittany\n2007\n0.0\n14.0\n12.0\n26.0\n126.0\n14.0\n0.0\n0.0\n...\n45.0\n96.0\n7.0\n27.0\n0.0\n14.0\n9.0\n9.0\n0.0\n891.0\n\n\n53245\nBrittany\n2008\n0.0\n15.0\n0.0\n14.0\n102.0\n5.0\n0.0\n0.0\n...\n30.0\n92.0\n9.0\n19.0\n0.0\n9.0\n16.0\n5.0\n0.0\n749.0\n\n\n53246\nBrittany\n2009\n0.0\n7.0\n6.0\n10.0\n105.0\n12.0\n0.0\n0.0\n...\n18.0\n83.0\n6.0\n26.0\n0.0\n10.0\n7.0\n0.0\n0.0\n644.0\n\n\n53247\nBrittany\n2010\n0.0\n9.0\n0.0\n20.0\n116.0\n9.0\n0.0\n0.0\n...\n31.0\n94.0\n11.0\n22.0\n0.0\n8.0\n6.0\n0.0\n0.0\n698.0\n\n\n53248\nBrittany\n2011\n0.0\n12.0\n7.0\n17.0\n109.0\n10.0\n9.0\n0.0\n...\n18.0\n91.0\n6.0\n32.0\n0.0\n7.0\n11.0\n0.0\n0.0\n717.0\n\n\n53249\nBrittany\n2012\n0.0\n12.0\n13.0\n9.0\n137.0\n11.0\n8.0\n6.0\n...\n23.0\n94.0\n6.0\n24.0\n0.0\n7.0\n10.0\n0.0\n0.0\n745.0\n\n\n53250\nBrittany\n2013\n0.0\n13.0\n0.0\n14.0\n110.0\n7.0\n9.0\n5.0\n...\n11.0\n113.0\n7.0\n25.0\n0.0\n12.0\n8.0\n0.0\n0.0\n699.0\n\n\n53251\nBrittany\n2014\n0.0\n11.0\n5.0\n8.0\n112.0\n9.0\n10.0\n0.0\n...\n21.0\n110.0\n8.0\n15.0\n0.0\n10.0\n6.0\n0.0\n0.0\n660.0\n\n\n53252\nBrittany\n2015\n0.0\n9.0\n6.0\n10.0\n109.0\n11.0\n0.0\n0.0\n...\n16.0\n124.0\n9.0\n26.0\n0.0\n9.0\n0.0\n0.0\n0.0\n636.0\n\n\n\n\n48 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names in a single chart. What trends do you notice?\nIt seems to be that ‘Mary’ was the most common name for most of the early 1920’s. Once we get to the 1970’s however, all four names seem to take a steep decline and the doesn’t climb back up. I am lead to assume that as the world became less religious, biblical names became less popular.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nLine chart of the names Mary, Martha, Peter, and Paul\nnew_names_df = df[df['name'].isin(['Mary', 'Martha', 'Peter', 'Paul']) & (df['year'] &gt;= 1920) & (df['year'] &lt;= 2000)]\n\n(ggplot (new_names_df, aes(x='year', y='Total', color='name')) +\ngeom_line() + \n    ggtitle('Mary, Martha, Peter, and Paul Over Time') + xlab('Year') + ylab('Total') + \n    scale_x_continuous(format='d') + \n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nMary, Martha, Peter, and Paul Over Time\n\n\n\n\nMary, Martha, Peter, and Paul table\ndisplay(new_names_df)\n\n\n\n\n\n\nMary, Martha, Peter, and Paul Over Time\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n264124\nMartha\n1920\n11.0\n258.0\n224.0\n21.0\n131.0\n66.0\n55.0\n20.0\n...\n416.0\n418.0\n16.0\n269.0\n11.0\n73.0\n116.0\n172.0\n11.0\n8705.0\n\n\n264125\nMartha\n1921\n0.0\n307.0\n216.0\n18.0\n161.0\n70.0\n57.0\n34.0\n...\n420.0\n478.0\n20.0\n297.0\n13.0\n57.0\n101.0\n204.0\n9.0\n9254.0\n\n\n264126\nMartha\n1922\n9.0\n326.0\n219.0\n23.0\n126.0\n67.0\n56.0\n17.0\n...\n421.0\n501.0\n18.0\n273.0\n14.0\n39.0\n82.0\n225.0\n5.0\n9018.0\n\n\n264127\nMartha\n1923\n0.0\n341.0\n236.0\n27.0\n159.0\n63.0\n38.0\n24.0\n...\n442.0\n233.0\n22.0\n293.0\n11.0\n45.0\n72.0\n210.0\n10.0\n8731.0\n\n\n264128\nMartha\n1924\n0.0\n342.0\n257.0\n39.0\n166.0\n58.0\n49.0\n23.0\n...\n507.0\n487.0\n15.0\n155.0\n15.0\n41.0\n72.0\n196.0\n20.0\n9163.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n303691\nPeter\n1996\n0.0\n22.0\n8.0\n42.0\n501.0\n55.0\n134.0\n22.0\n...\n30.0\n183.0\n27.0\n103.0\n20.0\n109.0\n114.0\n8.0\n0.0\n4069.0\n\n\n303692\nPeter\n1997\n17.0\n19.0\n5.0\n47.0\n476.0\n44.0\n111.0\n25.0\n...\n24.0\n175.0\n27.0\n73.0\n17.0\n74.0\n98.0\n0.0\n0.0\n3821.0\n\n\n303693\nPeter\n1998\n14.0\n21.0\n8.0\n42.0\n471.0\n42.0\n104.0\n14.0\n...\n27.0\n134.0\n41.0\n78.0\n12.0\n98.0\n89.0\n0.0\n0.0\n3377.0\n\n\n303694\nPeter\n1999\n15.0\n24.0\n9.0\n48.0\n402.0\n66.0\n101.0\n23.0\n...\n30.0\n138.0\n26.0\n105.0\n10.0\n77.0\n81.0\n0.0\n0.0\n3430.0\n\n\n303695\nPeter\n2000\n11.0\n14.0\n11.0\n40.0\n379.0\n66.0\n72.0\n20.0\n...\n26.0\n147.0\n24.0\n62.0\n12.0\n68.0\n70.0\n0.0\n0.0\n3137.0\n\n\n\n\n324 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nLooking at the graph it looks as though there was a little rise in the use of the name after 1993. The rise after 1993 is slight and would be hard to conclude that the increase is correlated with the release of “The Sandlot”.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nLine chart of the name Benjamin\nbenjamin_df = df[df['name'] == 'Benjamin']\n\n(ggplot(benjamin_df, aes(x='year', y='Total')) +\n    geom_line(color = 'blue') +\n    geom_vline(xintercept = 1993, linetype = 'dashed', color = 'red') +\n    geom_text(x = 1992, y = 15000, label = 'Sandlot was Released', hjust = 1) +\n    geom_text(x = 1960, y = 14200, label = '(1993)', hjust = 0) +\n    ggtitle('Benjamin from The Sandlot') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nBenjamin from The Sandlot\n\n\n\n\nBenjamin from The Sandlot table\ndisplay(benjamin_df)\n\n\n\n\n\n\nBenjamin from The Sandlot\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n41526\nBenjamin\n1910\n0.0\n13.0\n10.0\n5.0\n5.0\n0.0\n7.0\n0.0\n...\n0.0\n22.0\n0.0\n23.0\n0.0\n0.0\n5.0\n0.0\n0.0\n497.0\n\n\n41527\nBenjamin\n1911\n0.0\n14.0\n10.0\n0.0\n7.0\n7.0\n22.0\n7.0\n...\n7.0\n16.0\n0.0\n26.0\n0.0\n0.0\n7.0\n0.0\n0.0\n645.0\n\n\n41528\nBenjamin\n1912\n0.0\n25.0\n13.0\n0.0\n18.0\n7.0\n25.0\n7.0\n...\n24.0\n41.0\n0.0\n47.0\n0.0\n7.0\n19.0\n13.0\n0.0\n1245.0\n\n\n41529\nBenjamin\n1913\n0.0\n29.0\n19.0\n0.0\n16.0\n9.0\n31.0\n15.0\n...\n22.0\n57.0\n5.0\n49.0\n0.0\n14.0\n19.0\n15.0\n0.0\n1451.0\n\n\n41530\nBenjamin\n1914\n0.0\n50.0\n31.0\n5.0\n29.0\n12.0\n37.0\n8.0\n...\n35.0\n71.0\n0.0\n59.0\n0.0\n11.0\n21.0\n11.0\n0.0\n1802.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41627\nBenjamin\n2011\n23.0\n165.0\n91.0\n281.0\n1592.0\n247.0\n157.0\n57.0\n...\n244.0\n988.0\n193.0\n403.0\n33.0\n308.0\n271.0\n49.0\n17.0\n12743.0\n\n\n41628\nBenjamin\n2012\n31.0\n141.0\n87.0\n264.0\n1676.0\n212.0\n157.0\n52.0\n...\n252.0\n1035.0\n158.0\n364.0\n29.0\n336.0\n238.0\n57.0\n20.0\n12411.5\n\n\n41629\nBenjamin\n2013\n34.0\n125.0\n81.0\n300.0\n1845.0\n242.0\n145.0\n37.0\n...\n279.0\n1079.0\n178.0\n363.0\n21.0\n333.0\n262.0\n56.0\n18.0\n13460.0\n\n\n41630\nBenjamin\n2014\n36.0\n128.0\n93.0\n270.0\n1873.0\n252.0\n179.0\n64.0\n...\n249.0\n1152.0\n153.0\n352.0\n26.0\n370.0\n214.0\n50.0\n25.0\n13761.0\n\n\n41631\nBenjamin\n2015\n34.0\n151.0\n110.0\n275.0\n1809.0\n248.0\n178.0\n57.0\n...\n239.0\n1154.0\n168.0\n383.0\n21.0\n356.0\n232.0\n72.0\n29.0\n13608.0\n\n\n\n\n106 rows × 54 columns\n\n\n\n–&gt; –&gt;\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 1"
    ]
  },
  {
    "objectID": "Machine_Learning/project4.html",
    "href": "Machine_Learning/project4.html",
    "title": "Client Report - Can You Predict That?",
    "section": "",
    "text": "Using the power of advanced machine learning, we developed a model that successfully predicted whether a home was built before or after 1980. Through model tuning we were able to get the accuracy of our model’s prediction at the 90% threshold making our model relatively reliable in predicting the year a home was built. We first Identified possible relationships between homes built before and after 1980. Our model found that one story architecture, garage type, and quality ratings of C were most correlated with homes from built before 1980. Our model was then explained and justified using a classification report, confusion matrix and an ROC curve confirming that our model was accurate and reliable. I also built a model to predict the year a home was built using the “GradientBoostingRegressor.” This model was able to successfully predict 89% of its results. For Justification we look at Mean Absolute Error (MAE), Mean Squared Error (MSE), R-Squared value, and a confusion matrix. \n\n\nRead and format project data\n# Include and execute your code here\n\ndwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project4.html#elevator-pitch",
    "href": "Machine_Learning/project4.html#elevator-pitch",
    "title": "Client Report - Can You Predict That?",
    "section": "",
    "text": "Using the power of advanced machine learning, we developed a model that successfully predicted whether a home was built before or after 1980. Through model tuning we were able to get the accuracy of our model’s prediction at the 90% threshold making our model relatively reliable in predicting the year a home was built. We first Identified possible relationships between homes built before and after 1980. Our model found that one story architecture, garage type, and quality ratings of C were most correlated with homes from built before 1980. Our model was then explained and justified using a classification report, confusion matrix and an ROC curve confirming that our model was accurate and reliable. I also built a model to predict the year a home was built using the “GradientBoostingRegressor.” This model was able to successfully predict 89% of its results. For Justification we look at Mean Absolute Error (MAE), Mean Squared Error (MSE), R-Squared value, and a confusion matrix. \n\n\nRead and format project data\n# Include and execute your code here\n\ndwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project4.html#questiontask-1",
    "href": "Machine_Learning/project4.html#questiontask-1",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np \nfrom lets_plot import * \nLetsPlot.setup_html(isolated_frame=True)\n\ndf_subset = dwellings_ml.filter(\n    ['livearea', 'finbsmnt', 'basement', \n    'yearbuilt', 'nocars', 'numbdrm', 'numbaths', 'before1980',\n    'stories', 'yrbuilt']).sample(500)\n\ndf_subset = df_subset.dropna(subset=['livearea', 'finbsmnt', 'before1980', 'basement', 'numbaths', 'numbdrm'])\n\ndf_subset['before1980'] = df_subset['before1980'].astype(bool)\n\n\nThis scatter plot shows us the possible relationship between the number of bedrooms built in the house and the total living space, or square footage, of the home. There seems to be a linear relationship between house size and number of rooms built. As the number of rooms increase, so does the total livable square footage.\n\n\nLiving Area vs Number of Bedrooms\n# Include and execute your code here\nc_1 = ggplot(df_subset, aes(x='livearea', y='numbdrm', color='before1980')) + geom_point(alpha=0.5) + ggtitle(\"Living Area vs Number of Bedrooms\") + xlab(\"Living Area\") + ylab(\"Number of Bedrooms\") + scale_color_manual(name='Built Before 1980', labels=['Yes (Blue)', 'No (Red)'], values=['blue', 'red']) + theme(legend_position='right') \n\nc_1.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nLiving Area vs Number of Bedrooms\n\n\nThis box and whisker plot shows is the relationship between basement sizes on homes built before 1980 and after 1980. From this we can see that homes that were built before 1980 had slightly larger mean basement size than homes built after 1980.\n\n\nFinished Basement Area by Before1980\n# Include and execute your code here\n\nc_2 = ggplot(df_subset, aes(x='before1980', y='finbsmnt', fill='before1980')) + geom_boxplot() + ggtitle('Finished Basement Area by Before1980') + xlab('Built Before 1980') + ylab('Finished Basement Area') + scale_fill_discrete(name=\"Built Before 1980\")\nc_2.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThis stacked bar chart tells us the relationship between the number of bathrooms built in a house based on if the house was built before or after 1980. It looks like the majority of houses built before 1980 had two bathrooms. \n\n\nNumber of Bathrooms by Before1980\n# Include and execute your code here\n\nc_3 = ggplot(df_subset, aes(x='numbaths', fill='before1980')) + geom_bar(position='stack') + ggtitle('Number of Bathrooms by Before1980') + xlab('Number of Bathrooms') + ylab('Count') + scale_fill_discrete(name=\"Built Before 1980\")\nc_3.show()",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project4.html#questiontask-2",
    "href": "Machine_Learning/project4.html#questiontask-2",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\nI ended up using the DecisionTreeClassifier to create my classification model. I settled on this because this led to me getting the highest accuracy in the quickest time. I tried to use the Gradient Boosting, but it ended up taking longer to get the results. I also tried to add the following parameters: max_depth, min_sample_split, and min_samples_leaf. I found that this took a lot longer to get the results and it did not improve the accuracy rating at all when compared to the DecisionTreeClassifier with no parameters. I was able to achieve a 90% accuracy rating.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\n\n\n\n\nClassification Report\nX = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny = dwellings_ml.filter(regex = \"before1980\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .35, random_state = 76)\n\nmodel_1 = tree.DecisionTreeClassifier(random_state=76, max_depth=10, min_samples_split=10, min_samples_leaf=2)\nmodel_1.fit(X_train, y_train)\n\ny_pred = model_1.predict(X_test)\n\nprint('\\nTest Accuracy:', metrics.accuracy_score(y_test, y_pred))\nprint('\\nClassification Report: \\n', metrics.classification_report(y_test, y_pred))\n\n\n\nTest Accuracy: 0.9007481296758105\n\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       0.86      0.87      0.87      2980\n           1       0.92      0.92      0.92      5040\n\n    accuracy                           0.90      8020\n   macro avg       0.89      0.89      0.89      8020\nweighted avg       0.90      0.90      0.90      8020",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project4.html#questiontask-3",
    "href": "Machine_Learning/project4.html#questiontask-3",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a feature importance chart and a description of the features.\nThe most important features identified in my model were “arcstyle_ONE_STORY”, “gartype_Att”, and “quality_C.” The model puts the most importance on how many stories the house is using “arcstyle_ONE_STORY.” One story homes were more popular before 1980 so there were more built of these types of homes. The second most important feature my model used was whether or not the house had an attached, or detached garage, “gartype_Att.” Most of the houses built before 1980 had detached garages. The third greatest importance the model used was the quality of the home from “quality_C.” Homes with this quality rating were more likely to be older homes. The model put these three fields together to best predict if the house was built before or after 1980.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nFeature Importance Chart\n# Include and execute your code here\nimportance = model_1.feature_importances_\nfeature_names = X.columns\n\nfeature_importance_df = pd.DataFrame({'feature': feature_names, 'Importance': importance})\nfeature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n\nfeature_importance_df_top = feature_importance_df.head(10)\n\nc_4 = ggplot(feature_importance_df_top, aes(x='feature', y='Importance', fill= 'Importance')) + geom_bar(stat='identity') + theme(axis_text_x=element_text(angle=90, hjust=1)) + ggtitle('Feature Importance') + xlab('Feature') + ylab('Importance')\nc_4.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nFeature Importance",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project4.html#questiontask-4",
    "href": "Machine_Learning/project4.html#questiontask-4",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\nTo justify the results and quality of my model, I chose to look at the following three metrics: 1 - Classification report containing the accuracy, 2 - Confusion matrix, and 3 - The Receiver-operating characteristic (ROC) curve. Based on the results of this test, I can conclude that the quality of my model is strong and can be relied upon with an acceptable accuracy and true positive rate. Under each of the reports I will explain the significance and how to interpret the results.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nPerformance Metrics\n# Include and execute your code here\nprint('\\nTest Accuracy:', metrics.accuracy_score(y_test, y_pred))\nprint('\\nClassification Report: \\n', metrics.classification_report(y_test, y_pred))\n\n\n\nTest Accuracy: 0.9007481296758105\n\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       0.86      0.87      0.87      2980\n           1       0.92      0.92      0.92      5040\n\n    accuracy                           0.90      8020\n   macro avg       0.89      0.89      0.89      8020\nweighted avg       0.90      0.90      0.90      8020\n\n\n\nThe classification report and accuracy measure the percentage of correctly classified predictions compared to the total results. The higher the accuracy percentage is the more accurate the model is. A high accuracy rating is anything greater than or equal to 90% (0.90). The precision measures the percentage of total correct guesses divided by the number of true positives + false positives. For “0” this means that 86% of the home predicted to be built after 1980 are correct, and for the “1” 96% of the homes predicted to be built before 1980 are correct. Recall measures the total true positives divided by true positives + false negatives. For “0”, the model correctly identified 87% of the homes built after 1980 and for “1” correctly identified 92% of the homes built before 1980. The F-1 Score is calculated as 2 * (Precision * Recall / Precision + Recall). The F-1 score provides a balanced metric to look at our model performance. Our model is slightly stronger at predicting if a home was built before 1980 than after 1980 but overall reliable at predicting if it was built before or after 1980.\n\n\nConfusion Matrix\n# Include and execute your code here\n\nconf_matrix = metrics.confusion_matrix(y_test, y_pred)\nconf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted: (After 1980)', 'Predicted: (Before 1980)'], index=['Actual: (After 1980)', 'Actual: (Before 1980)'])\n\nconf_matrix_plot = ggplot(conf_matrix_df.reset_index().melt(id_vars='index'), aes(x='index', y='variable', fill='value')) + geom_tile() + geom_text(aes(label='value'), color='black', size=6) + scale_fill_brewer(palette='RdYlBu') + ggtitle('Confusion Matrix') + xlab('Predicted') + ylab('Actual')\n\nconf_matrix_plot.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThe confusion matrix is a table that is used to describe the performance of a classification model. The matrix shows the number of true positives (4630), false positives (386), true negatives (2594), and false negatives (410). From the confusion matrix we can see that our model had relatively few false positives / negatives in relation to the correct predictions. This proves how well my model was at predicting if a home was built before or after 1980.\n\n\nROC Curve\n# Include and execute your code here\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\n\nroc_df = pd.DataFrame({'FPR': fpr, 'TPR': tpr, 'Thresholds': thresholds})\n\nroc_chart = ggplot(roc_df, aes(x='FPR', y='TPR')) + geom_line(color='blue') + ggtitle('ROC Curve') + xlab('False Positive Rate') + ylab('True Positive Rate')\n\nroc_chart.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThe ROC curve is a graphical representation of the true positive rate against the false positive rate. The closer the curve is to the top left corner, the better the model is. The area under the curve (AUC) is used to measure the performance of the model. The higher the AUC, the better the model is. Our model had an AUC of 0.92 which shows us it performed very well at predicting when a house was built.",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project4.html#stretchtask-5",
    "href": "Machine_Learning/project4.html#stretchtask-5",
    "title": "Client Report - Can You Predict That?",
    "section": "STRETCH|TASK 5",
    "text": "STRETCH|TASK 5\nCan you build a model that predicts the year a house was built? Explain the model and the evaluation metrics you would use to determine if the model is good.\nFor the model I chose to use the “GradientBoostingRegressor” because of its ability to sift through complex, non-linear relationships to get to the target prediction. In this case our goal was to predict what year a house was built which requires the model to look through the data and attempt to find patterns that could help predict when a house was built. The “GradientBoostingRegressor” is also very good at not over filtering the data like other models. This model type also very flexible and forgiving when it comes to tuning the model. Below are the figures and print outs I used to evaluate the accuracy of my model.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nRead and format data\n# Include and execute your code here\n\nX_5 = dwellings_ml.drop(dwellings_ml.filter(regex = 'yrbuilt|parcel').columns, axis = 1)\ny_5 = dwellings_ml[\"yrbuilt\"]\n\nX_5 = X_5.dropna()\ny_5 = y_5[X_5.index]\n\nX_train, X_test, y5_train, y5_test = train_test_split(X_5, y_5, test_size = .35, random_state = 76)\n\nrf_model = GradientBoostingRegressor(random_state=76, n_estimators=500, learning_rate=0.1, max_depth=15, min_samples_split=10, min_samples_leaf=2)\nrf_model.fit(X_train, y5_train)\n\ny5_pred = rf_model.predict(X_test)\n\nmae = metrics.mean_absolute_error(y5_test, y5_pred)\nmse = metrics.mean_squared_error(y5_test, y5_pred)\nr2 = metrics.r2_score(y5_test, y5_pred)\n\nprint(f'Mean Absolute Error (MAE): {mae:.2f}')\nprint(f'Mean Squared Error (MSE): {mse:.2f}')\nprint(f'R-Squared (R^2): {r2:.2f}')\n\n\nMean Absolute Error (MAE): 7.49\nMean Squared Error (MSE): 155.45\nR-Squared (R^2): 0.89\n\n\n\n\nClassification Report\n# Include and execute your code here\n\nbins = [0, 1980, 1995, 2010, 2023]\nlables = ['Pre 1980', '1980-1995', '1996-2010', 'Post 2010']\n\ny5_test_bins = pd.cut(y5_test, bins=bins, labels=lables)\ny5_pred_bins = pd.cut(y5_pred, bins=bins, labels=lables)\n\nprint('\\nTest Accuracy:', metrics.accuracy_score(y5_test_bins, y5_pred_bins))\nprint('\\nClassification Report: \\n', metrics.classification_report(y5_test_bins, y5_pred_bins))\n\n\n\nTest Accuracy: 0.9337905236907731\n\nClassification Report: \n               precision    recall  f1-score   support\n\n   1980-1995       0.67      0.68      0.68       418\n   1996-2010       0.84      0.91      0.87      1832\n   Post 2010       0.88      0.73      0.80       671\n    Pre 1980       1.00      0.99      0.99      5099\n\n    accuracy                           0.93      8020\n   macro avg       0.85      0.83      0.84      8020\nweighted avg       0.94      0.93      0.93      8020\n\n\n\nThe Mean Absolute Error (MAE) tells us that on average, our model’s predictions are off by only 7.5 years. With regression tasks like this a MAE of 10 years is typically considered very good. The Mean Squared Error (MSE) is similar to the MAE, but the MSE penalizes the model’s larger errors heavier than smaller errors. Our score of 156.62 equates to about 12.5 years of difference with a range of years that spans over 100 years. When we compare our R-Squared result with the Accuracy test below we can see that overall, our model is right within the acceptable margin of error.\n\n\nConfusion Matrix\n# Include and execute your code here\n\nconf5_matrix = metrics.confusion_matrix(y5_test_bins, y5_pred_bins, labels=lables)\nconf5_matrix_df = pd.DataFrame(conf5_matrix, columns=['Pred:(Pre 80)', 'Pred:(80-95)', 'Pred:(96-10)', 'Pred:(Post 10)'], index=['Actl:(Pre 80)', 'Actl:(80-95)', 'Actl:(96-10)', 'Actl:(Post 10)'])\n\nconf5_matrix_norm = conf5_matrix.astype('float') / conf5_matrix.sum(axis=1)[:, np.newaxis]\n\nconf5_matrix_norm_df = pd.DataFrame(conf5_matrix_norm, columns=['Pred:(Pre 80)', 'Pred:(80-95)', 'Pred:(96-10)', 'Pred:(Post 10)'], index=['Actl:(Pre 80)', 'Actl:(80-95)', 'Actl:(96-10)', 'Actl:(Post 10)'])\n\nconf5_matrix_norm_df_melted = conf5_matrix_norm_df.reset_index().melt(id_vars='index', var_name='Predicted', value_name='Accuracy')\n\nlbl_plot = (\n  pd.DataFrame(100 * conf5_matrix_norm)\n  .stack()\n  .reset_index(drop=True)\n  .round(1)\n  .astype(str) + '%'\n)\n\nconf5_matrix_plot = ggplot(conf5_matrix_norm_df_melted, aes(x='Predicted', y='index', fill='Accuracy')) + geom_tile() + geom_text(aes(label=lbl_plot), color='white', size=6) + scale_fill_gradient(low='red', high='blue', name='Accuracy') + ggtitle('Confusion Matrix') + xlab('Predicted') + ylab('Actual') + theme(axis_text_x=element_text(angle=90, hjust=1))\n\nconf5_matrix_plot.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nConfusion Matrix\n\n\nThe confusion matrix is a table that is used to describe the performance of a classification model. The matrix shows the number of true positives, false positives, true negatives, and false negatives. From the confusion matrix we can see that our model had relatively few false positives / negatives in relation to the correct predictions. This proves how well my model was at predicting when a house was built.",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project5.html",
    "href": "Machine_Learning/project5.html",
    "title": "Client Report - The War with Star Wars",
    "section": "",
    "text": "After cleansing the Star Wars survey data for better handling. The cleaning consisted of shorting column names, handling missing values, and flited the raw data to extract the correct and useable survey responses. I then successfully recreated two of the visuals featured on the article provided to validate our cleaned data. The visual output and measurements were identical to those of the article. After this we then used the cleaned data to create a machine learning model to predict if someone makes over $50,000 a year based on their survey responses. Our final model was 65% accurate at predicting if a participant makes over $50k a year. To provide extra validation we then successfully recreated one more visual from the article.\n\n\nRead and format project data\nurl = 'https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv'\n\ndf_cols = pd.read_csv(url, encoding = \"ISO-8859-1\", nrows = 1).melt()\ndf = pd.read_csv(url, encoding = \"ISO-8859-1\", skiprows =2, header = None )",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "Machine_Learning/project5.html#elevator-pitch",
    "href": "Machine_Learning/project5.html#elevator-pitch",
    "title": "Client Report - The War with Star Wars",
    "section": "",
    "text": "After cleansing the Star Wars survey data for better handling. The cleaning consisted of shorting column names, handling missing values, and flited the raw data to extract the correct and useable survey responses. I then successfully recreated two of the visuals featured on the article provided to validate our cleaned data. The visual output and measurements were identical to those of the article. After this we then used the cleaned data to create a machine learning model to predict if someone makes over $50,000 a year based on their survey responses. Our final model was 65% accurate at predicting if a participant makes over $50k a year. To provide extra validation we then successfully recreated one more visual from the article.\n\n\nRead and format project data\nurl = 'https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv'\n\ndf_cols = pd.read_csv(url, encoding = \"ISO-8859-1\", nrows = 1).melt()\ndf = pd.read_csv(url, encoding = \"ISO-8859-1\", skiprows =2, header = None )",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "Machine_Learning/project5.html#questiontask-1",
    "href": "Machine_Learning/project5.html#questiontask-1",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nShorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.\nThe column names were long and very difficult to work with what we wanted to accomplish in this product. I cleaned up some of the data and shorted the column names to a more unified method. The new column names can be seen below.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\n\n\n\n\nTable of column names\n# Include and execute your code here\n\nvar_replace = {\n    'Which of the following Star Wars films have you seen\\\\? Please select all that apply\\\\.':'seen',\n    'Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.':'rank',\n    'Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.':'view',\n    'Do you consider yourself to be a fan of the Star Trek franchise\\\\?':'star_trek_fan',\n    'Do you consider yourself to be a fan of the Expanded Universe\\\\?\\x8cæ':'expanded_fan',\n    'Are you familiar with the Expanded Universe\\\\?':'know_expanded',\n    'Have you seen any of the 6 films in the Star Wars franchise\\\\?':'seen_any',\n    'Do you consider yourself to be a fan of the Star Wars film franchise\\\\?':'star_wars_fans',\n    'Which character shot first\\\\?':'shot_first',\n    'Unnamed: \\d{1,2}':np.nan,\n    ' ':'_',\n}\n\nval_replace = {\n    'Response':'',\n    'Star Wars: Episode ':'',\n    ' ':'_'\n}\n\ndf_cols_new = (df_cols\n    .assign(\n        val_replace = lambda x:  x.value.str.strip().replace(val_replace, regex=True),\n        var_replace = lambda x: x.variable.str.strip().replace(var_replace, regex=True)\n    )\n    .fillna(method = 'ffill')\n    .fillna(value = \"\")\n    .assign(column_names = lambda x: x.var_replace.str.cat(x.val_replace, sep = \"_\").str.strip('__').str.lower())\n    )\n\ndf.columns = df_cols_new.column_names.to_list()\n\ndf.columns\n\n\nIndex(['respondentid', 'seen_any', 'star_wars_fans',\n       'seen_i__the_phantom_menace', 'seen_ii__attack_of_the_clones',\n       'seen_iii__revenge_of_the_sith', 'seen_iv__a_new_hope',\n       'seen_v_the_empire_strikes_back', 'seen_vi_return_of_the_jedi',\n       'rank_i__the_phantom_menace', 'rank_ii__attack_of_the_clones',\n       'rank_iii__revenge_of_the_sith', 'rank_iv__a_new_hope',\n       'rank_v_the_empire_strikes_back', 'rank_vi_return_of_the_jedi',\n       'view_han_solo', 'view_luke_skywalker', 'view_princess_leia_organa',\n       'view_anakin_skywalker', 'view_obi_wan_kenobi',\n       'view_emperor_palpatine', 'view_darth_vader', 'view_lando_calrissian',\n       'view_boba_fett', 'view_c-3p0', 'view_r2_d2', 'view_jar_jar_binks',\n       'view_padme_amidala', 'view_yoda', 'shot_first', 'know_expanded',\n       'expanded_fan', 'star_trek_fan', 'gender', 'age', 'household_income',\n       'education', 'location_(census_region)'],\n      dtype='object')\nColumn Names\n\n\nBelow is an example of what the column names were before and what I changed them to. The “variable” column represents what the original column name was. The “value” column original values in response to the question for the column. The “val_replace” and “var_replace” columns represent the cleaned values and variables used to replace the older, harder to use data and names. The last column “column_names” represents the cleaned and shortened column names we used for easier computer handling.\n\n\nTable of column names\n# Include and execute your code here\ndf_cols_new.head()\n\n\n\n\n\n\nColumn Names\n\n\n\nvariable\nvalue\nval_replace\nvar_replace\ncolumn_names\n\n\n\n\n0\nRespondentID\n\n\nRespondentID\nrespondentid\n\n\n1\nHave you seen any of the 6 films in the Star W...\nResponse\n\nseen_any\nseen_any\n\n\n2\nDo you consider yourself to be a fan of the St...\nResponse\n\nstar_wars_fans\nstar_wars_fans\n\n\n3\nWhich of the following Star Wars films have yo...\nStar Wars: Episode I The Phantom Menace\nI__The_Phantom_Menace\nseen\nseen_i__the_phantom_menace\n\n\n4\nUnnamed: 4\nStar Wars: Episode II Attack of the Clones\nII__Attack_of_the_Clones\nseen\nseen_ii__attack_of_the_clones",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "Machine_Learning/project5.html#questiontask-2",
    "href": "Machine_Learning/project5.html#questiontask-2",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nClean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made.\nIn order to properly work with machine learning, we have to do some more data cleaning. The following output and data represents the steps I tool to clean the data.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\n\n\nA. Filter the dataset to respondents that have seen at least one film. I also altered this code to filter out any responses that said they had seen any Star Wars movie but did not mark what movies they had seen.\n\n\nFilter data\n# Include and execute your code here\n\nseen_columns = [\n    'seen_i__the_phantom_menace',\n    'seen_ii__attack_of_the_clones',\n    'seen_iii__revenge_of_the_sith',\n    'seen_iv__a_new_hope',\n    'seen_v_the_empire_strikes_back',\n    'seen_vi_return_of_the_jedi',\n]\n\ndf_filtered = df[df['seen_any'] == 'Yes']\n\nfor col in seen_columns:\n    df_filtered[col] = df_filtered[col].notna()\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nage\nhousehold_income\neducation\nlocation_(census_region)\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\n18-29\nNaN\nHigh school degree\nSouth Atlantic\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n18-29\n$0 - $24,999\nHigh school degree\nWest North Central\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n18-29\n$25,000 - $49,999\nBachelor degree\nMiddle Atlantic\n\n\n\n\n5 rows × 38 columns\n\nSeen Any Star Wars Movie\n\n\nB. Create a new column that converts the age ranges to a single number. Drop the age range categorical column\n\n\nChange Age Field\n# Include and execute your code here\n\nage_map = {\n    '18-29': 24,\n    '30-44': 37,\n    '45-60': 52,\n    '&gt;60': 65\n}\n\ndf_filtered['age_num'] = df_filtered['age'].map(age_map)\ndf_filtered.drop('age', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\nAge Field\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nhousehold_income\neducation\nlocation_(census_region)\nage_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nNaN\nHigh school degree\nSouth Atlantic\n24.0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n$0 - $24,999\nHigh school degree\nWest North Central\n24.0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n24.0\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n24.0\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n$25,000 - $49,999\nBachelor degree\nMiddle Atlantic\n24.0\n\n\n\n\n5 rows × 38 columns\n\n\n\nC. Create a new column that converts the education groupings to a single number. Drop the school categorical column\n\n\nFilter data\n# Include and execute your code here\n\neducation_map = {\n  'Less than high school degree': 9,\n  'High school degree': 12,\n  'Some college or Associate degree': 14,\n  'Bachelor degree': 16,\n  'Graduate degree': 20\n}\n\ndf_filtered['education_num'] = df_filtered['education'].map(education_map)\ndf_filtered.drop('education', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nhousehold_income\nlocation_(census_region)\nage_num\neducation_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nNaN\nSouth Atlantic\n24.0\n12.0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n$0 - $24,999\nWest North Central\n24.0\n12.0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n$100,000 - $149,999\nWest North Central\n24.0\n14.0\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n$100,000 - $149,999\nWest North Central\n24.0\n14.0\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n$25,000 - $49,999\nMiddle Atlantic\n24.0\n16.0\n\n\n\n\n5 rows × 38 columns\n\nEducation Field\n\n\nD. Create a new column that converts the income ranges to a single number. Drop the income range categorical column\n\n\nFilter data\n# Include and execute your code here\n\ndf_filtered['household_income'] = df_filtered['household_income'].fillna('unknown')\n\nincome_map = {\n  '$0 - $24,999': 12500,\n  '$25,000 - $49,999': 37500,\n  '$50,000 - $99,999': 75000,\n  '$100,000 - $149,999': 125000,\n  '$150,000+': 150000,\n  'unknown': 0\n}\n\ndf_filtered['income_num'] = df_filtered['household_income'].map(income_map)\ndf_filtered.drop('household_income', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nlocation_(census_region)\nage_num\neducation_num\nincome_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nSouth Atlantic\n24.0\n12.0\n0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\nWest North Central\n24.0\n12.0\n12500\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\nWest North Central\n24.0\n14.0\n125000\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\nWest North Central\n24.0\n14.0\n125000\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\nMiddle Atlantic\n24.0\n16.0\n37500\n\n\n\n\n5 rows × 38 columns\n\nIncome Field\n\n\nE. Create your target (also known as “y” or “label”) column based on the new income range column\n\n\nFilter data\n# Include and execute your code here\n\ndf_filtered['income_over_50k'] = (df_filtered['income_num'] &gt; 50000).astype(int)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nlocation_(census_region)\nage_num\neducation_num\nincome_num\nincome_over_50k\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nI don't understand this question\nYes\nNo\nNo\nMale\nSouth Atlantic\n24.0\n12.0\n0\n0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nI don't understand this question\nNo\nNaN\nNo\nMale\nWest North Central\n24.0\n12.0\n12500\n0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nI don't understand this question\nNo\nNaN\nYes\nMale\nWest North Central\n24.0\n14.0\n125000\n1\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nGreedo\nYes\nNo\nNo\nMale\nWest North Central\n24.0\n14.0\n125000\n1\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nHan\nYes\nNo\nYes\nMale\nMiddle Atlantic\n24.0\n16.0\n37500\n0\n\n\n\n\n5 rows × 39 columns\n\nIncome Over 50k\n\n\nF. One-hot encode all remaining categorical columns\n\n\nFilter data\n# Include and execute your code here\n\ndf_encoded = pd.get_dummies(df_filtered, drop_first=True)\n\ndf_encoded.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\nrank_ii__attack_of_the_clones\nrank_iii__revenge_of_the_sith\n...\nstar_trek_fan_Yes\ngender_Male\nlocation_(census_region)_East South Central\nlocation_(census_region)_Middle Atlantic\nlocation_(census_region)_Mountain\nlocation_(census_region)_New England\nlocation_(census_region)_Pacific\nlocation_(census_region)_South Atlantic\nlocation_(census_region)_West North Central\nlocation_(census_region)_West South Central\n\n\n\n\n0\n3292879998\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n2.0\n1.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2\n3292765271\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n2.0\n3.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n3292763116\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n6.0\n1.0\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\n3292731220\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n4.0\n6.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n5\n3292719380\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n4.0\n3.0\n...\nTrue\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 102 columns\n\nOne-hot encoding",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "Machine_Learning/project5.html#questiontask-3",
    "href": "Machine_Learning/project5.html#questiontask-3",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nValidate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article\nI recreated the following visuals from the article to validate our data clensing: 1. “Which ‘Star Wars’ Movies Have You Seen?” and 2. “Who Shot First?”\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWhich Star Wars Movies Have You Seen?\n# Include and execute your code here\n\nseen_columns = [\n    'seen_i__the_phantom_menace',\n    'seen_ii__attack_of_the_clones',\n    'seen_iii__revenge_of_the_sith',\n    'seen_iv__a_new_hope',\n    'seen_v_the_empire_strikes_back',\n    'seen_vi_return_of_the_jedi',\n]\n\nmovie_titles = [\n    \"The Phantom Menace\",\n    \"Attack of the Clones\",\n    \"Revenge of the Sith\",\n    \"A New Hope\",\n    \"The Empire Strikes Back\",\n    \"Return of the Jedi\",\n]\n\ngood_resp = df_filtered[seen_columns].sum(axis=1) &gt; 0\n\ntotal_seen_any = good_resp.sum()\nprob_seen = [(df_filtered[col].sum() / total_seen_any) for col in seen_columns]\n\n\ndf_seen = pd.DataFrame({\n  'movie': movie_titles,\n  'probability': prob_seen\n})\n\np = (ggplot(df_seen, aes(x='movie', y='probability')) + geom_bar(stat='identity', fill='blue') + labs(title='Which Star Wars Movies Have You Seen?', subtitle='Percentage of respondents who have seen each film', x='Percentage of Respondents', y='Star Wars Movies') + theme(axis_text_x=element_text(angle=45, hjust=1)))\n\np.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nWhich Star Wars Movies Have You Seen?\n\n\n\n\nWho Shot First?\n# Include and execute your code here\n\nshot_first_counts = df_filtered['shot_first'].value_counts()\n\nshot_first_percent = (shot_first_counts / shot_first_counts.sum()) * 100\n\ndf_shot_first = pd.DataFrame({\n  'response': shot_first_percent.index,\n  'percentage': shot_first_percent.values\n})\n\ndf_shot_first['response'] = df_shot_first['response'].replace({\"I don't understand this question\": \"Don't Understand\"})\ndf_shot_first['response'] = pd.Categorical(df_shot_first['response'], categories=['Don\\'t Understand', 'Greedo', 'Han'], ordered=True)\ndf_shot_first['percent_label'] = df_shot_first['percentage'].round(0).astype(int).astype(str) + '%'\n\n\np2 = (ggplot(df_shot_first, aes(x='response', y='percentage')) + geom_bar(stat='identity', fill='blue') + geom_text(aes(label='percent_label'), nudge_y=2, size=10) + labs(title='Who Shot First?', subtitle='According to 834 respondents', x='Response', y='Percentage') + coord_flip() + theme(axis_text_x=element_text(hjust=1)))\n\np2.show()",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "Machine_Learning/project5.html#questiontask-4",
    "href": "Machine_Learning/project5.html#questiontask-4",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nBuild a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.\nFor my model, I chose to use all the columns in the DataFrame except any of the columns that would directly give up the answer (e.g. household_income, income_num, location, and income_over_50k). After I filtered out any undesirable data, I split the data into training and testing sets, and trained the model to make predictions on if a person makes more or less than $50k a year. I then ran a accuracy score and classification report to display how successful my model was. The accuracy of my model 0.655 using the “RandomForestClassifier”. This means that our model can predict if a person makes over or under $50k with a 67% accuracy.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n\n\n\nRandom Forest Model\n# Include and execute your code here\n\nX_pred = df_encoded.drop(columns=df_encoded.filter(regex=(\"(household_income|income_num|income_over_50k|location_\\(census_region\\)_East South Central|location_\\(census_region\\)_Middle Atlantic|location_\\(census_region\\)_Mountain|location_\\(census_region\\)_New England|location_\\(census_region\\)_Pacific|location_\\(census_region\\)_South Atlantic|location_\\(census_region\\)_West North Central|location_\\(census_region\\)_West South Central|respondentid)\")).columns)\ny_pred = df_encoded['income_over_50k']\n\nX_train, X_test, y_train, y_test = train_test_split(X_pred, y_pred, test_size=0.35, random_state=76)\n\nrf = RandomForestClassifier(n_estimators=300 ,random_state=76, max_depth=20, min_samples_split=2)\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\nprint('Accuracy:', accuracy_score(y_test, y_pred))\nprint('\\nClassification Report:\\n', classification_report(y_test, y_pred, target_names=['Under 50k', 'Over 50k']))\n\n\nAccuracy: 0.6432926829268293\n\nClassification Report:\n               precision    recall  f1-score   support\n\n   Under 50k       0.69      0.64      0.67       183\n    Over 50k       0.59      0.64      0.61       145\n\n    accuracy                           0.64       328\n   macro avg       0.64      0.64      0.64       328\nweighted avg       0.65      0.64      0.64       328",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "Machine_Learning/project5.html#stretchtask-5",
    "href": "Machine_Learning/project5.html#stretchtask-5",
    "title": "Client Report - The War with Star Wars",
    "section": "STRETCH|TASK 5",
    "text": "STRETCH|TASK 5\nValidate the data provided on GitHub lines up with the article by recreating a 3rd visual from the article.\nFor this stretch challenge we were asked to recreate one more visual from the article to double check the validity of our data. I chose to recreate the “What’s the Best ‘Star Wars’ Movie?” In this visual we first had to filter the data further to those who have seen all the Star Wars movies. We then had to calculate and display the share of respondents who rated each fil as their favorite.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWhat is the best Star Wars movie?\n# Include and execute your code here\n\nrank_columns = [\n    'rank_i__the_phantom_menace',\n    'rank_ii__attack_of_the_clones',\n    'rank_iii__revenge_of_the_sith',\n    'rank_iv__a_new_hope',\n    'rank_v_the_empire_strikes_back',\n    'rank_vi_return_of_the_jedi',\n]\n\nmovie_titles = [\n    \"The Phantom Menace\",\n    \"Attack of the Clones\",\n    \"Revenge of the Sith\",\n    \"A New Hope\",\n    \"The Empire Strikes Back\",\n    \"Return of the Jedi\",\n]\n\nseen_all = df_filtered[seen_columns].all(axis=1)\ndf_seen_all = df_filtered[seen_all]\n\nfav_counts = [df_seen_all[col].value_counts().get(1, 0) for col in rank_columns]\n\ntotal_resp = df_seen_all.shape[0]\npercent_fav = [(count / total_resp) * 100 for count in fav_counts]\n\ndf_fav = pd.DataFrame({\n  'movie': movie_titles,\n  'percentage': percent_fav\n})\ndf_fav['percent_label'] = df_fav['percentage'].round(0).astype(int).astype(str) + '%'\ndf_fav['movie'] = pd.Categorical(df_fav['movie'], categories=movie_titles[::-1], ordered=True)\n\np5 = (ggplot(df_fav, aes(x='movie', y='percentage')) + geom_bar(stat='identity', fill='blue') + geom_text(aes(label='percent_label'), nudge_y=2, size=10) + labs(title='Favorite Star Wars Movies', subtitle='Of respondents who have seen all six films', x='Percentage of Respondents', y='Star Wars Movies') + coord_flip() + theme(axis_text_x=element_text(hjust=1)))\n\np5.show()",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project2.html",
    "href": "Cleansing_Projects/project2.html",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Using the data collected from the Bureau of Transportation Statistics I have determined what airports are the best to fly out of and when to fly in general to have the best chance of avoiding delays for several reasons altogether. Chicago O’Hare International Airport (ORD) was determined to be the worst airport based on the average length of delay (1.13 Hours). ORD also had the second highest proportion of delays due to weather out of all the airports I looked at. The highest proportion of delays due to weather was San Francisco International Airport (SFO). I also determined that September has the lowest proportion of total flights delayed. The holiday months (January, July, November, and December) have the highest likelihood of experiencing delays\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\"\ndf = pd.read_json(url)\n\nAirport_nan = df['airport_name'].isna().sum()\nyear_nan = df['year'].isna().sum()\nmin_delay_nan = df['minutes_delayed_carrier'].isna().sum()\nmin_delay_nas_nan = df['minutes_delayed_nas'].isna().sum()\n\nyear_nan\n\n\nnp.int64(23)\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project2.html#elevator-pitch",
    "href": "Cleansing_Projects/project2.html#elevator-pitch",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Using the data collected from the Bureau of Transportation Statistics I have determined what airports are the best to fly out of and when to fly in general to have the best chance of avoiding delays for several reasons altogether. Chicago O’Hare International Airport (ORD) was determined to be the worst airport based on the average length of delay (1.13 Hours). ORD also had the second highest proportion of delays due to weather out of all the airports I looked at. The highest proportion of delays due to weather was San Francisco International Airport (SFO). I also determined that September has the lowest proportion of total flights delayed. The holiday months (January, July, November, and December) have the highest likelihood of experiencing delays\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\"\ndf = pd.read_json(url)\n\nAirport_nan = df['airport_name'].isna().sum()\nyear_nan = df['year'].isna().sum()\nmin_delay_nan = df['minutes_delayed_carrier'].isna().sum()\nmin_delay_nas_nan = df['minutes_delayed_nas'].isna().sum()\n\nyear_nan\n\n\nnp.int64(23)\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project2.html#questiontask-1",
    "href": "Cleansing_Projects/project2.html#questiontask-1",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”).\nI went through and made a new data frame for me to preform some cleaning. I then went through and searched for all the blanks, 1500+ minutes, and values that had -999 in them. then used the “.replace” to get them out and replace them with “NaN”.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n::: {#cell-Q1-Clean Data .cell execution_count=4}\n\nCleaned data\n# Include and execute your code here\ndf_clean = df\n\ndf_clean.replace([\"\", \"1500+\", -999], np.nan, inplace=True)\n\nexample = df_clean.iloc[2].to_json()\nexample\n\n\n'{\"airport_code\":\"IAD\",\"airport_name\":null,\"month\":\"January\",\"year\":2005.0,\"num_of_flights_total\":12381,\"num_of_delays_carrier\":\"414\",\"num_of_delays_late_aircraft\":1058.0,\"num_of_delays_nas\":895,\"num_of_delays_security\":4,\"num_of_delays_weather\":61,\"num_of_delays_total\":2430,\"minutes_delayed_carrier\":null,\"minutes_delayed_late_aircraft\":70919,\"minutes_delayed_nas\":35660.0,\"minutes_delayed_security\":208,\"minutes_delayed_weather\":4497,\"minutes_delayed_total\":134881}'\nCleaned Data\n\n:::",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project2.html#questiontask-2",
    "href": "Cleansing_Projects/project2.html#questiontask-2",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays?\nI chose to look at the average time each flight is delayed determining the worst airport delays. In the table you will find the total delayed flights and total delayed minutes for each airport. The portion of delayed flights measure tells us what percentage of flights out of that airport are delayed. The last column displays the average hours each flight is delayed. This helped me determine that Chicago O’Hare International Airport (ORD) is the worst airport to fly out of.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nThe orst airport based on average delay hours\n# Include and execute your code here\n\ndf_clean_group = df_clean\n\ngrouped_data = df_clean_group.groupby('airport_code').agg(\n  total_flights=('num_of_flights_total', 'sum'),\n  total_delays=('num_of_delays_total', 'sum'),\n  total_delay_minutes=('minutes_delayed_total', 'sum')\n).reset_index()\n\ngrouped_data['proportion_delayed_flights'] = (grouped_data['total_delays'] / grouped_data['total_flights']) * 100\n\ngrouped_data['average_delay_hours'] = (grouped_data['total_delay_minutes'] / grouped_data['total_delays']) / 60\n\nworst_airport = grouped_data.loc[grouped_data['average_delay_hours'].idxmax()]\n\n\nworst_airport\n\n\nairport_code                        ORD\ntotal_flights                   3597588\ntotal_delays                     830825\ntotal_delay_minutes            56356129\nproportion_delayed_flights    23.093945\naverage_delay_hours            1.130525\nName: 3, dtype: object\nWorst Airport\n\n\n::: {#cell-Q2-Airport Tables .cell .tbl-cap-location-top tbl-cap=‘Airport Delays’ execution_count=7}\n\nAirport delays table\n# Include and execute your code here\ngrouped_data\n\n\n\n\n\n\n\n\n\nairport_code\ntotal_flights\ntotal_delays\ntotal_delay_minutes\nproportion_delayed_flights\naverage_delay_hours\n\n\n\n\n0\nATL\n4430047\n902443\n53983926\n20.370958\n0.996996\n\n\n1\nDEN\n2513974\n468519\n25173381\n18.636589\n0.895495\n\n\n2\nIAD\n851571\n168467\n10283478\n19.783083\n1.017358\n\n\n3\nORD\n3597588\n830825\n56356129\n23.093945\n1.130525\n\n\n4\nSAN\n917862\n175132\n8276248\n19.080428\n0.787620\n\n\n5\nSFO\n1630945\n425604\n26550493\n26.095546\n1.039718\n\n\n6\nSLC\n1403384\n205160\n10123371\n14.618950\n0.822396\n\n\n\n\n\n\n:::",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project2.html#questiontask-3",
    "href": "Cleansing_Projects/project2.html#questiontask-3",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length?\nI chose to look at the proportion of delayed flights to determine what month is the best month to fly. I chose this as my metric because I feel this paints an accurate picture about what the chances are of your flight being delayed dependent on month. From what I found, the best month to fly is September. 16.5% of all the flights that occur in September end up being delayed.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nProportion of delayed flights by month\n# Include and execute your code here\ndf_mo = df_clean\n\ndf_mo['month'] = df_mo['month'].replace('Febuary', 'February')\n\ndf_clean_month = df_mo[df['month'].notna()]\n\nmonth_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n\ndf_clean_month['month'] = pd.Categorical(df_clean_month['month'], categories=month_order, ordered=True)\n\nmonthly_delays = df_clean_month.groupby('month').agg(\n  total_flights=('num_of_flights_total', 'sum'),\n  total_delays=('num_of_delays_total', 'sum'),\n  total_delay_minutes=('minutes_delayed_total', 'sum')\n).reset_index()\n\nmonthly_delays['proportion_delayed_flights'] = (monthly_delays['total_delays'] / monthly_delays['total_flights']) * 100\n\n(\n  ggplot(monthly_delays, aes(x='month', y='proportion_delayed_flights')) +\n  geom_bar(stat='identity') +\n  ggtitle('Proportion of Delayed Flights by Month') +\n  xlab('Month') +\n  ylab('Proportion of Delayed Flights (%)') +\n  theme(axis_text_x=element_text(angle=45, hjust=1))\n)\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nProportion of Delayed Flights by Month",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project2.html#questiontask-4",
    "href": "Cleansing_Projects/project2.html#questiontask-4",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nYour job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild).\nI was able to create the new row and add it to the table. The column “weather_related_delays” will be used for the next question.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWeather delays\n# Include and execute your code here\n\ndf_clean_two = df_clean\n\ndf_clean_two['num_of_delays_late_aircraft'] = df_clean_two['num_of_delays_late_aircraft'].replace(-999, np.nan)\n\ndf_clean_two['num_of_delays_late_aircraft'].fillna(df['num_of_delays_late_aircraft'].mean(), inplace=True)\n\ndef weather_nas_delays(row):\n  if row['month'] in ['April', 'May', 'June', 'July', 'August']:\n    return 0.4 * round(row['num_of_delays_nas'], 2)\n  else:\n    return 0.65 * round(row['num_of_delays_nas'], 2)\n\ndf_clean_two['weather_related_delays'] = (\n  df_clean_two['num_of_delays_weather'] +\n  round(0.3 * df_clean['num_of_delays_late_aircraft'], 2) +\n  df_clean_two.apply(weather_nas_delays, axis=1)\n)\n\ndf_clean_two.head(5)\n\n\n\n\n\n\n\n\n\nairport_code\nairport_name\nmonth\nyear\nnum_of_flights_total\nnum_of_delays_carrier\nnum_of_delays_late_aircraft\nnum_of_delays_nas\nnum_of_delays_security\nnum_of_delays_weather\nnum_of_delays_total\nminutes_delayed_carrier\nminutes_delayed_late_aircraft\nminutes_delayed_nas\nminutes_delayed_security\nminutes_delayed_weather\nminutes_delayed_total\nweather_related_delays\n\n\n\n\n0\nATL\nAtlanta, GA: Hartsfield-Jackson Atlanta Intern...\nJanuary\n2005.0\n35048\nNaN\n1109.104072\n4598\n10\n448\n8355\n116423.0\n104415\n207467.0\n297\n36931\n465533\n3769.43\n\n\n1\nDEN\nDenver, CO: Denver International\nJanuary\n2005.0\n12687\n1041\n928.000000\n935\n11\n233\n3153\n53537.0\n70301\n36817.0\n363\n21779\n182797\n1119.15\n\n\n2\nIAD\nNaN\nJanuary\n2005.0\n12381\n414\n1058.000000\n895\n4\n61\n2430\nNaN\n70919\n35660.0\n208\n4497\n134881\n960.15\n\n\n3\nORD\nChicago, IL: Chicago O'Hare International\nJanuary\n2005.0\n28194\n1197\n2255.000000\n5415\n5\n306\n9178\n88691.0\n160811\n364382.0\n151\n24859\n638894\n4502.25\n\n\n4\nSAN\nSan Diego, CA: San Diego International\nJanuary\n2005.0\n7283\n572\n680.000000\n638\n7\n56\n1952\n27436.0\n38445\n21127.0\n218\n4326\n91552\n674.70\n\n\n\n\n\nWeather Delays",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project2.html#questiontask-5",
    "href": "Cleansing_Projects/project2.html#questiontask-5",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Describe what you learn from this graph.\nAccording the the data San Francisco International Airport (SFO) experiences the highest proportion of delays due to bad weather days. Approximately 9.81% of the flights from SFO are delayed due to weather. Upon further research I found that San Francisco experiences frequent oceanic fog that causes delays and groundings for outbound flights. The next airport to avoid flying out of is Chicago O’Hare International Airport (ORD) with 8.5% of their flights become delayed.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWeather delays by airport\n# Include and execute your code here\ndf_clean_three = df_clean_two\n\ndf_clean_three['proportion_weather_delays'] = (df_clean_three['weather_related_delays'] / df_clean_three['num_of_flights_total']) * 100\n\nairport_weather_delays = df_clean_three.groupby('airport_code').agg(proportion_weather_delays=('proportion_weather_delays', 'mean')\n).reset_index()\n\n(\n  ggplot(airport_weather_delays, aes(x='airport_code', y='proportion_weather_delays')) +\n  geom_bar(stat='identity') +\n  ggtitle('Proportion of Weather Delays by Airport') +\n  xlab('Airport Code') +\n  ylab('Proportion of Weather Delays (%)') +\n  theme(axis_text_x=element_text(angle=45, hjust=1))\n)\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nProportion of Weather Delays by Airport",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project3.html#elevator-pitch",
    "href": "Cleansing_Projects/project3.html#elevator-pitch",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nIn analyzing all the baseball data, we were able to gain a few valuable key insights. First, we analyzed all the players that had BYU-Idaho listed as their college. We looked at what team they played for, what year they played for the team, and how much their salary was. Next, we identified and raked players batting averages using three different parameters. We found that as we include more data over years of players careers, we can get closer to the true batting average. Lastly, we compared two teams, the Los Angeles Dodgers and the New York Yankees, total wins from 1985 to the latest year of the data. We observed that the New York Yankees had a higher frequency of wins than the Los Angeles Dodgers.\n\n\nRead and format project data\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)",
    "crumbs": [
      "Data Cleansing",
      "Project 3"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project3.html#questiontask-1",
    "href": "Cleansing_Projects/project3.html#questiontask-1",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nThe table displays the highest salaries reported by the players that came from BYU-Idaho. Matt Lindstrom (Lindsma01) had the most success out of all the baseball players that went to school in BYU-Idaho. Matt Lindstrom’s highest paid year on record was with the Chicago White Sox in 2014. Matt had a salary with the White Sox of $4 million.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\n\n\nBYU-Idaho Players Salaries\n# Include and execute your code here\nq_1 = \"\"\"\nselect distinct p.playerID, c.schoolID, printf('$%,.0f',s.salary) as 'Salary', s.yearID, s.teamID\nfrom collegeplaying c\njoin Salaries s on c.playerID = s.playerID\njoin People p on c.playerID = p.playerID\nwhere c.schoolID = 'idbyuid'\norder by s.salary desc\nlimit 10\n\"\"\"\nsalary_df = pd.read_sql_query(q_1,con)\ndisplay(salary_df)\n\n\n\n\n\n\n\n\n\nplayerID\nschoolID\nSalary\nyearID\nteamID\n\n\n\n\n0\nlindsma01\nidbyuid\n$4000000\n2014\nCHA\n\n\n1\nlindsma01\nidbyuid\n$3600000\n2012\nBAL\n\n\n2\nlindsma01\nidbyuid\n$2800000\n2011\nCOL\n\n\n3\nlindsma01\nidbyuid\n$2300000\n2013\nCHA\n\n\n4\nlindsma01\nidbyuid\n$1625000\n2010\nHOU\n\n\n5\nstephga01\nidbyuid\n$1025000\n2001\nSLN\n\n\n6\nstephga01\nidbyuid\n$900000\n2002\nSLN\n\n\n7\nstephga01\nidbyuid\n$800000\n2003\nSLN\n\n\n8\nstephga01\nidbyuid\n$550000\n2000\nSLN\n\n\n9\nlindsma01\nidbyuid\n$410000\n2009\nFLO\n\n\n\n\n\nBYU-Idaho Players Salaries",
    "crumbs": [
      "Data Cleansing",
      "Project 3"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project3.html#questiontask-2",
    "href": "Cleansing_Projects/project3.html#questiontask-2",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\nPart 1: Write an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\nFrom part one we can see that when we include all those players who had at least at bat for the year their batting average for the year was perfect. All the top 5 players from part one had a batting average of 100%.\n::: {#cell-Q2 Part 1 .cell execution_count=6}\n\nBatting average\n# Include and execute your code here\nq2_1 = \"\"\"\nselect playerID, yearID, (h*1.0)/ab as 'BA'\nfrom batting\nwhere ab &gt;= 1\norder by (h*1.0)/ab desc, playerID asc\nlimit 5\n\"\"\"\nba_part_1 = pd.read_sql_query(q2_1,con)\ndisplay(ba_part_1)\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nBA\n\n\n\n\n0\naberal01\n1957\n1.0\n\n\n1\nabernte02\n1960\n1.0\n\n\n2\nabramge01\n1923\n1.0\n\n\n3\nacklefr01\n1964\n1.0\n\n\n4\nalanirj01\n2019\n1.0\n\n\n\n\n\nTop 5 Batting Averages\n\n:::\nPart 2: Use the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\nFor part two we start to whittle down the results, instead of players with at least one at bat we increase the threshold to at least 10 at bats. Now, our best batting average was only 64% instead of players have a 100% batting average.\n::: {#cell-Q2 Part 2 .cell execution_count=7}\n\nBatting average\n# Include and execute your code here\nq2_2 = \"\"\"\nselect playerID, yearID, (h*1.0)/ab as 'BA'\nfrom batting\nwhere ab &gt;= 10\norder by (h*1.0)/ab desc, playerID\nlimit 5\n\"\"\"\nba_part_2 = pd.read_sql_query(q2_2,con)\ndisplay(ba_part_2)\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nBA\n\n\n\n\n0\nnymanny01\n1974\n0.642857\n\n\n1\ncarsoma01\n2013\n0.636364\n\n\n2\naltizda01\n1910\n0.600000\n\n\n3\njohnsde01\n1975\n0.600000\n\n\n4\nsilvech01\n1948\n0.571429\n\n\n\n\n\nTop 5 Batting Averages\n\n:::\nPart 3: Now calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\nFor part three we whittled down even further by obtaining players lifetime batting average across all the years they played. The highest player had a batting average of only 35.8%. Following the progression of the parameters. When we widen the number of at bats and hits, it becomes clear that the best players do not always have a 100% batting average like we saw in part one\n::: {#cell-Q2 Part 3 .cell execution_count=8}\n\nBatting average\n# Include and execute your code here\nq2_3 = \"\"\"\nselect playerID, sum(h*1.0)/sum(ab) as 'BA'\nfrom batting\nwhere ab &gt;= 100\ngroup by playerID\norder by (h*1.0)/ab desc, playerID\nlimit 5\n\"\"\"\nba_part_3 = pd.read_sql_query(q2_3,con)\ndisplay(ba_part_3)\n\n\n\n\n\n\n\n\n\nplayerID\nBA\n\n\n\n\n0\nmeyerle01\n0.357542\n\n\n1\nmcveyca01\n0.345802\n\n\n2\njacksjo01\n0.357009\n\n\n3\nhazlebo01\n0.402985\n\n\n4\nbarnero01\n0.363201\n\n\n\n\n\nTop 5 Batting Averages\n\n:::",
    "crumbs": [
      "Data Cleansing",
      "Project 3"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project3.html#questiontask-3",
    "href": "Cleansing_Projects/project3.html#questiontask-3",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Lets-Plot to visualize the comparison. What do you learn?\nI chose to use total wins for each year to compare the Los Angeles Dodgers (LAN) to the total wins for the New York Yankees (NYA). I chose this measure to compare the teams because I feel as though looking at the wins of the two teams gives a great overall comparison of the success of the team. In examining the data and graph I concluded that the New York Yankees had successful year more frequently than the Los Angeles Dodgers. Both teams had their good years and bad years. Overall, the New York Yankees won more frequently than the Los Angeles Dodgers.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\nfrom lets_plot import *\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nTable of team wins\n# Include and execute your code here\n\nq_3 = \"\"\"\nselect teamID, yearID, sum(w) as 'Total_Wins'\nfrom Teams\nwhere teamID in ('NYA', 'LAN') and yearID &gt;= 1985\ngroup by teamID, yearID\norder by yearID asc\n\"\"\"\n\nwins_df = pd.read_sql_query(q_3,con)\nwins_df.tail(14)\n\n\n\n\n\n\n\n\n\nteamID\nyearID\nTotal_Wins\n\n\n\n\n56\nLAN\n2013\n92\n\n\n57\nNYA\n2013\n85\n\n\n58\nLAN\n2014\n94\n\n\n59\nNYA\n2014\n84\n\n\n60\nLAN\n2015\n92\n\n\n61\nNYA\n2015\n87\n\n\n62\nLAN\n2016\n91\n\n\n63\nNYA\n2016\n84\n\n\n64\nLAN\n2017\n104\n\n\n65\nNYA\n2017\n91\n\n\n66\nLAN\n2018\n92\n\n\n67\nNYA\n2018\n100\n\n\n68\nLAN\n2019\n106\n\n\n69\nNYA\n2019\n103\n\n\n\n\n\nWins by Team\n\n\n\n\nGraph of wins\n# Include and execute your code here\n\n(ggplot(wins_df, aes(x='yearID', y='Total_Wins', color='teamID')) + geom_line() + ggtitle('Total Wins by Team') + xlab('Year') + ylab('Total Wins') + scale_x_continuous(format='d') + theme(plot_title=element_text(hjust=0.5))\n)",
    "crumbs": [
      "Data Cleansing",
      "Project 3"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project3.html#questiontaskstretch-4",
    "href": "Cleansing_Projects/project3.html#questiontaskstretch-4",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK(STRETCH) 4",
    "text": "QUESTION|TASK(STRETCH) 4\nAdvanced Salary Distribution by Position\nFrom this we can see that the on average the highest paid position is “Designated Hitter”. While this position has the highest average salary, when we look at the higest (max) salary for each position we find that “Third Baseman” and “Pitcher” are tied for the higest salary at $33 million. All positions are categorized as “High Salary”.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\n\n\nAverage salary by position\n# Include and execute your code here\n\nq_4 = \"\"\"\nselect \n  p.position, printf('$%,.0f',avg(s.salary)) as 'Average_Salary', count(distinct s.playerID) as 'Total_Players',\n  coalesce(printf('$%,.0f',max(s.salary)), 'N/A') as 'Highest_Salary',\n  case \n    when avg(s.salary) &gt; 1000000 then 'High Salary'\n    when avg(s.salary) between 5000000 and 1000000 then 'Medium salary'\n    else 'Low Salary'\n  end as salary_category\nfrom Salaries s\njoin appearances a on s.playerID = a.playerID\njoin(\n  select playerID, max(position) as position\n  from (\n    select playerID,\n      case\n        when G_p &gt; 0 then 'Pitcher'\n        when G_c &gt; 0 then 'Catcher'\n        when G_1b &gt; 0 then 'First Baseman'\n        when G_2b &gt; 0 then 'Second Baseman'\n        when G_3b &gt; 0 then 'Third Baseman'\n        when G_ss &gt; 0 then 'Shortstop'\n        when G_lf &gt; 0 then 'Left Fielder'\n        when G_cf &gt; 0 then 'Center Fielder'\n        when G_rf &gt; 0 then 'Right Fielder'\n        when G_of &gt; 0 then 'Outfielder'\n        when G_dh &gt; 0 then 'Designated Hitter'\n        when G_ph &gt; 0 then 'Pinch Hitter'\n        when G_pr &gt; 0 then 'Pinch Runner'\n        \n      end as position\n    from appearances\n  )\n  group by playerID\n) p on s.playerID = p.playerID\ngroup by p.position\norder by avg(s.salary) desc\nlimit 10\n\"\"\"\n\nsalary_df = pd.read_sql_query(q_4,con)\ndisplay(salary_df)\n\n\n\n\n\n\n\n\n\nposition\nAverage_Salary\nTotal_Players\nHighest_Salary\nsalary_category\n\n\n\n\n0\nDesignated Hitter\n$6625195\n7\n$16071429\nHigh Salary\n\n\n1\nFirst Baseman\n$4086740\n248\n$25000000\nHigh Salary\n\n\n2\nRight Fielder\n$3713566\n233\n$23854494\nHigh Salary\n\n\n3\nThird Baseman\n$2687984\n487\n$33000000\nHigh Salary\n\n\n4\nLeft Fielder\n$2640393\n521\n$27328046\nHigh Salary\n\n\n5\nShortstop\n$2500582\n169\n$22600000\nHigh Salary\n\n\n6\nPitcher\n$2364202\n2745\n$33000000\nHigh Salary\n\n\n7\nSecond Baseman\n$2020024\n370\n$24000000\nHigh Salary\n\n\n8\nCatcher\n$1617095\n298\n$20777778\nHigh Salary\n\n\n9\nCenter Fielder\n$1556184\n15\n$3900000\nHigh Salary\n\n\n\n\n\nAverage Salary by Position",
    "crumbs": [
      "Data Cleansing",
      "Project 3"
    ]
  },
  {
    "objectID": "Story_Telling/project1.html",
    "href": "Story_Telling/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 1"
    ]
  },
  {
    "objectID": "Story_Telling/project4.html",
    "href": "Story_Telling/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 4"
    ]
  },
  {
    "objectID": "Story_Telling/project5.html",
    "href": "Story_Telling/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 5"
    ]
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "About Me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "full_stack.html#my-data-science-portfolio",
    "href": "full_stack.html#my-data-science-portfolio",
    "title": "About Me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Braxton McCandless’s CV",
    "section": "",
    "text": "Student, Employed at UPS and Smith Kunz & Associates.\n\n\nAttending Brigham Young University - Idaho\n\n\nTax Accounting, Data Science Certificate\n\n\n\nTax Accounting, Python, Power BI\n\n\n\n\n2018 Graduated Madison High School\nApril 2020 - now Brigham Young University - Idaho\n\n\n\n2016-2018 Ibex Ind., Rigby\n\nShop Hand\nMinted coins\n\n2018-2020 Service Missionary for Churh of Jesus Christ, Kiribati Islands\n2020-Present United Parcal Service, Rexburg\n\nPDS Supervisor\nDisbatch UPS Routes"
  },
  {
    "objectID": "resume.html#currently",
    "href": "resume.html#currently",
    "title": "Braxton McCandless’s CV",
    "section": "",
    "text": "Attending Brigham Young University - Idaho\n\n\nTax Accounting, Data Science Certificate\n\n\n\nTax Accounting, Python, Power BI"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Braxton McCandless’s CV",
    "section": "",
    "text": "2018 Graduated Madison High School\nApril 2020 - now Brigham Young University - Idaho"
  },
  {
    "objectID": "resume.html#occupation",
    "href": "resume.html#occupation",
    "title": "Braxton McCandless’s CV",
    "section": "",
    "text": "2016-2018 Ibex Ind., Rigby\n\nShop Hand\nMinted coins\n\n2018-2020 Service Missionary for Churh of Jesus Christ, Kiribati Islands\n2020-Present United Parcal Service, Rexburg\n\nPDS Supervisor\nDisbatch UPS Routes"
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "About Me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "exploration.html#my-data-science-portfolio",
    "href": "exploration.html#my-data-science-portfolio",
    "title": "About Me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "Story_Telling/project3.html",
    "href": "Story_Telling/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 3"
    ]
  },
  {
    "objectID": "Story_Telling/project2.html",
    "href": "Story_Telling/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html",
    "href": "Cleansing_Projects/project5.html",
    "title": "Client Report - The War with Star Wars",
    "section": "",
    "text": "After cleansing the Star Wars survey data for better handling. The cleaning consisted of shorting column names, handling missing values, and flited the raw data to extract the correct and useable survey responses. I then successfully recreated two of the visuals featured on the article provided to validate our cleaned data. The visual output and measurements were identical to those of the article. After this we then used the cleaned data to create a machine learning model to predict if someone makes over $50,000 a year based on their survey responses. Our final model was 65% accurate at predicting if a participant makes over $50k a year. To provide extra validation we then successfully recreated one more visual from the article.\n\n\nRead and format project data\nurl = 'https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv'\n\ndf_cols = pd.read_csv(url, encoding = \"ISO-8859-1\", nrows = 1).melt()\ndf = pd.read_csv(url, encoding = \"ISO-8859-1\", skiprows =2, header = None )",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html#elevator-pitch",
    "href": "Cleansing_Projects/project5.html#elevator-pitch",
    "title": "Client Report - The War with Star Wars",
    "section": "",
    "text": "After cleansing the Star Wars survey data for better handling. The cleaning consisted of shorting column names, handling missing values, and flited the raw data to extract the correct and useable survey responses. I then successfully recreated two of the visuals featured on the article provided to validate our cleaned data. The visual output and measurements were identical to those of the article. After this we then used the cleaned data to create a machine learning model to predict if someone makes over $50,000 a year based on their survey responses. Our final model was 65% accurate at predicting if a participant makes over $50k a year. To provide extra validation we then successfully recreated one more visual from the article.\n\n\nRead and format project data\nurl = 'https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv'\n\ndf_cols = pd.read_csv(url, encoding = \"ISO-8859-1\", nrows = 1).melt()\ndf = pd.read_csv(url, encoding = \"ISO-8859-1\", skiprows =2, header = None )",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html#questiontask-1",
    "href": "Cleansing_Projects/project5.html#questiontask-1",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nShorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.\nThe column names were long and very difficult to work with what we wanted to accomplish in this product. I cleaned up some of the data and shorted the column names to a more unified method. The new column names can be seen below.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\n\n\n\n\nTable of column names\n# Include and execute your code here\n\nvar_replace = {\n    'Which of the following Star Wars films have you seen\\\\? Please select all that apply\\\\.':'seen',\n    'Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.':'rank',\n    'Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.':'view',\n    'Do you consider yourself to be a fan of the Star Trek franchise\\\\?':'star_trek_fan',\n    'Do you consider yourself to be a fan of the Expanded Universe\\\\?\\x8cæ':'expanded_fan',\n    'Are you familiar with the Expanded Universe\\\\?':'know_expanded',\n    'Have you seen any of the 6 films in the Star Wars franchise\\\\?':'seen_any',\n    'Do you consider yourself to be a fan of the Star Wars film franchise\\\\?':'star_wars_fans',\n    'Which character shot first\\\\?':'shot_first',\n    'Unnamed: \\d{1,2}':np.nan,\n    ' ':'_',\n}\n\nval_replace = {\n    'Response':'',\n    'Star Wars: Episode ':'',\n    ' ':'_'\n}\n\ndf_cols_new = (df_cols\n    .assign(\n        val_replace = lambda x:  x.value.str.strip().replace(val_replace, regex=True),\n        var_replace = lambda x: x.variable.str.strip().replace(var_replace, regex=True)\n    )\n    .fillna(method = 'ffill')\n    .fillna(value = \"\")\n    .assign(column_names = lambda x: x.var_replace.str.cat(x.val_replace, sep = \"_\").str.strip('__').str.lower())\n    )\n\ndf.columns = df_cols_new.column_names.to_list()\n\ndf.columns\n\n\nIndex(['respondentid', 'seen_any', 'star_wars_fans',\n       'seen_i__the_phantom_menace', 'seen_ii__attack_of_the_clones',\n       'seen_iii__revenge_of_the_sith', 'seen_iv__a_new_hope',\n       'seen_v_the_empire_strikes_back', 'seen_vi_return_of_the_jedi',\n       'rank_i__the_phantom_menace', 'rank_ii__attack_of_the_clones',\n       'rank_iii__revenge_of_the_sith', 'rank_iv__a_new_hope',\n       'rank_v_the_empire_strikes_back', 'rank_vi_return_of_the_jedi',\n       'view_han_solo', 'view_luke_skywalker', 'view_princess_leia_organa',\n       'view_anakin_skywalker', 'view_obi_wan_kenobi',\n       'view_emperor_palpatine', 'view_darth_vader', 'view_lando_calrissian',\n       'view_boba_fett', 'view_c-3p0', 'view_r2_d2', 'view_jar_jar_binks',\n       'view_padme_amidala', 'view_yoda', 'shot_first', 'know_expanded',\n       'expanded_fan', 'star_trek_fan', 'gender', 'age', 'household_income',\n       'education', 'location_(census_region)'],\n      dtype='object')\nColumn Names\n\n\nBelow is an example of what the column names were before and what I changed them to. The “variable” column represents what the original column name was. The “value” column original values in response to the question for the column. The “val_replace” and “var_replace” columns represent the cleaned values and variables used to replace the older, harder to use data and names. The last column “column_names” represents the cleaned and shortened column names we used for easier computer handling.\n\n\nTable of column names\n# Include and execute your code here\ndf_cols_new.head()\n\n\n\n\n\n\nColumn Names\n\n\n\nvariable\nvalue\nval_replace\nvar_replace\ncolumn_names\n\n\n\n\n0\nRespondentID\n\n\nRespondentID\nrespondentid\n\n\n1\nHave you seen any of the 6 films in the Star W...\nResponse\n\nseen_any\nseen_any\n\n\n2\nDo you consider yourself to be a fan of the St...\nResponse\n\nstar_wars_fans\nstar_wars_fans\n\n\n3\nWhich of the following Star Wars films have yo...\nStar Wars: Episode I The Phantom Menace\nI__The_Phantom_Menace\nseen\nseen_i__the_phantom_menace\n\n\n4\nUnnamed: 4\nStar Wars: Episode II Attack of the Clones\nII__Attack_of_the_Clones\nseen\nseen_ii__attack_of_the_clones",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html#questiontask-2",
    "href": "Cleansing_Projects/project5.html#questiontask-2",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nClean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made.\nIn order to properly work with machine learning, we have to do some more data cleaning. The following output and data represents the steps I tool to clean the data.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\n\n\nA. Filter the dataset to respondents that have seen at least one film. I also altered this code to filter out any responses that said they had seen any Star Wars movie but did not mark what movies they had seen.\n\n\nFilter data\n# Include and execute your code here\n\nseen_columns = [\n    'seen_i__the_phantom_menace',\n    'seen_ii__attack_of_the_clones',\n    'seen_iii__revenge_of_the_sith',\n    'seen_iv__a_new_hope',\n    'seen_v_the_empire_strikes_back',\n    'seen_vi_return_of_the_jedi',\n]\n\ndf_filtered = df[df['seen_any'] == 'Yes']\n\nfor col in seen_columns:\n    df_filtered[col] = df_filtered[col].notna()\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nage\nhousehold_income\neducation\nlocation_(census_region)\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\n18-29\nNaN\nHigh school degree\nSouth Atlantic\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n18-29\n$0 - $24,999\nHigh school degree\nWest North Central\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n18-29\n$25,000 - $49,999\nBachelor degree\nMiddle Atlantic\n\n\n\n\n5 rows × 38 columns\n\nSeen Any Star Wars Movie\n\n\nB. Create a new column that converts the age ranges to a single number. Drop the age range categorical column\n\n\nChange Age Field\n# Include and execute your code here\n\nage_map = {\n    '18-29': 24,\n    '30-44': 37,\n    '45-60': 52,\n    '&gt;60': 65\n}\n\ndf_filtered['age_num'] = df_filtered['age'].map(age_map)\ndf_filtered.drop('age', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\nAge Field\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nhousehold_income\neducation\nlocation_(census_region)\nage_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nNaN\nHigh school degree\nSouth Atlantic\n24.0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n$0 - $24,999\nHigh school degree\nWest North Central\n24.0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n24.0\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n24.0\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n$25,000 - $49,999\nBachelor degree\nMiddle Atlantic\n24.0\n\n\n\n\n5 rows × 38 columns\n\n\n\nC. Create a new column that converts the education groupings to a single number. Drop the school categorical column\n\n\nFilter data\n# Include and execute your code here\n\neducation_map = {\n  'Less than high school degree': 9,\n  'High school degree': 12,\n  'Some college or Associate degree': 14,\n  'Bachelor degree': 16,\n  'Graduate degree': 20\n}\n\ndf_filtered['education_num'] = df_filtered['education'].map(education_map)\ndf_filtered.drop('education', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nhousehold_income\nlocation_(census_region)\nage_num\neducation_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nNaN\nSouth Atlantic\n24.0\n12.0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n$0 - $24,999\nWest North Central\n24.0\n12.0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n$100,000 - $149,999\nWest North Central\n24.0\n14.0\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n$100,000 - $149,999\nWest North Central\n24.0\n14.0\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n$25,000 - $49,999\nMiddle Atlantic\n24.0\n16.0\n\n\n\n\n5 rows × 38 columns\n\nEducation Field\n\n\nD. Create a new column that converts the income ranges to a single number. Drop the income range categorical column\n\n\nFilter data\n# Include and execute your code here\n\ndf_filtered['household_income'] = df_filtered['household_income'].fillna('unknown')\n\nincome_map = {\n  '$0 - $24,999': 12500,\n  '$25,000 - $49,999': 37500,\n  '$50,000 - $99,999': 75000,\n  '$100,000 - $149,999': 125000,\n  '$150,000+': 150000,\n  'unknown': 0\n}\n\ndf_filtered['income_num'] = df_filtered['household_income'].map(income_map)\ndf_filtered.drop('household_income', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nlocation_(census_region)\nage_num\neducation_num\nincome_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nSouth Atlantic\n24.0\n12.0\n0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\nWest North Central\n24.0\n12.0\n12500\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\nWest North Central\n24.0\n14.0\n125000\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\nWest North Central\n24.0\n14.0\n125000\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\nMiddle Atlantic\n24.0\n16.0\n37500\n\n\n\n\n5 rows × 38 columns\n\nIncome Field\n\n\nE. Create your target (also known as “y” or “label”) column based on the new income range column\n\n\nFilter data\n# Include and execute your code here\n\ndf_filtered['income_over_50k'] = (df_filtered['income_num'] &gt; 50000).astype(int)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nlocation_(census_region)\nage_num\neducation_num\nincome_num\nincome_over_50k\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nI don't understand this question\nYes\nNo\nNo\nMale\nSouth Atlantic\n24.0\n12.0\n0\n0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nI don't understand this question\nNo\nNaN\nNo\nMale\nWest North Central\n24.0\n12.0\n12500\n0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nI don't understand this question\nNo\nNaN\nYes\nMale\nWest North Central\n24.0\n14.0\n125000\n1\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nGreedo\nYes\nNo\nNo\nMale\nWest North Central\n24.0\n14.0\n125000\n1\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nHan\nYes\nNo\nYes\nMale\nMiddle Atlantic\n24.0\n16.0\n37500\n0\n\n\n\n\n5 rows × 39 columns\n\nIncome Over 50k\n\n\nF. One-hot encode all remaining categorical columns\n\n\nFilter data\n# Include and execute your code here\n\ndf_encoded = pd.get_dummies(df_filtered, drop_first=True)\n\ndf_encoded.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\nrank_ii__attack_of_the_clones\nrank_iii__revenge_of_the_sith\n...\nstar_trek_fan_Yes\ngender_Male\nlocation_(census_region)_East South Central\nlocation_(census_region)_Middle Atlantic\nlocation_(census_region)_Mountain\nlocation_(census_region)_New England\nlocation_(census_region)_Pacific\nlocation_(census_region)_South Atlantic\nlocation_(census_region)_West North Central\nlocation_(census_region)_West South Central\n\n\n\n\n0\n3292879998\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n2.0\n1.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2\n3292765271\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n2.0\n3.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n3292763116\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n6.0\n1.0\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\n3292731220\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n4.0\n6.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n5\n3292719380\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n4.0\n3.0\n...\nTrue\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 102 columns\n\nOne-hot encoding",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html#questiontask-3",
    "href": "Cleansing_Projects/project5.html#questiontask-3",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nValidate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article\nI recreated the following visuals from the article to validate our data clensing: 1. “Which ‘Star Wars’ Movies Have You Seen?” and 2. “Who Shot First?”\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWhich Star Wars Movies Have You Seen?\n# Include and execute your code here\n\nseen_columns = [\n    'seen_i__the_phantom_menace',\n    'seen_ii__attack_of_the_clones',\n    'seen_iii__revenge_of_the_sith',\n    'seen_iv__a_new_hope',\n    'seen_v_the_empire_strikes_back',\n    'seen_vi_return_of_the_jedi',\n]\n\nmovie_titles = [\n    \"The Phantom Menace\",\n    \"Attack of the Clones\",\n    \"Revenge of the Sith\",\n    \"A New Hope\",\n    \"The Empire Strikes Back\",\n    \"Return of the Jedi\",\n]\n\ngood_resp = df_filtered[seen_columns].sum(axis=1) &gt; 0\n\ntotal_seen_any = good_resp.sum()\nprob_seen = [(df_filtered[col].sum() / total_seen_any) for col in seen_columns]\n\n\ndf_seen = pd.DataFrame({\n  'movie': movie_titles,\n  'probability': prob_seen\n})\n\np = (ggplot(df_seen, aes(x='movie', y='probability')) + geom_bar(stat='identity', fill='blue') + labs(title='Which Star Wars Movies Have You Seen?', subtitle='Percentage of respondents who have seen each film', x='Percentage of Respondents', y='Star Wars Movies') + theme(axis_text_x=element_text(angle=45, hjust=1)))\n\np.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nWhich Star Wars Movies Have You Seen?\n\n\n\n\nWho Shot First?\n# Include and execute your code here\n\nshot_first_counts = df_filtered['shot_first'].value_counts()\n\nshot_first_percent = (shot_first_counts / shot_first_counts.sum()) * 100\n\ndf_shot_first = pd.DataFrame({\n  'response': shot_first_percent.index,\n  'percentage': shot_first_percent.values\n})\n\ndf_shot_first['response'] = df_shot_first['response'].replace({\"I don't understand this question\": \"Don't Understand\"})\ndf_shot_first['response'] = pd.Categorical(df_shot_first['response'], categories=['Don\\'t Understand', 'Greedo', 'Han'], ordered=True)\ndf_shot_first['percent_label'] = df_shot_first['percentage'].round(0).astype(int).astype(str) + '%'\n\n\np2 = (ggplot(df_shot_first, aes(x='response', y='percentage')) + geom_bar(stat='identity', fill='blue') + geom_text(aes(label='percent_label'), nudge_y=2, size=10) + labs(title='Who Shot First?', subtitle='According to 834 respondents', x='Response', y='Percentage') + coord_flip() + theme(axis_text_x=element_text(hjust=1)))\n\np2.show()",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html#questiontask-4",
    "href": "Cleansing_Projects/project5.html#questiontask-4",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nBuild a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.\nFor my model, I chose to use all the columns in the DataFrame except any of the columns that would directly give up the answer (e.g. household_income, income_num, location, and income_over_50k). After I filtered out any undesirable data, I split the data into training and testing sets, and trained the model to make predictions on if a person makes more or less than $50k a year. I then ran a accuracy score and classification report to display how successful my model was. The accuracy of my model 0.655 using the “RandomForestClassifier”. This means that our model can predict if a person makes over or under $50k with a 67% accuracy.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n\n\n\nRandom Forest Model\n# Include and execute your code here\n\nX_pred = df_encoded.drop(columns=df_encoded.filter(regex=(\"(household_income|income_num|income_over_50k|location_\\(census_region\\)_East South Central|location_\\(census_region\\)_Middle Atlantic|location_\\(census_region\\)_Mountain|location_\\(census_region\\)_New England|location_\\(census_region\\)_Pacific|location_\\(census_region\\)_South Atlantic|location_\\(census_region\\)_West North Central|location_\\(census_region\\)_West South Central|respondentid)\")).columns)\ny_pred = df_encoded['income_over_50k']\n\nX_train, X_test, y_train, y_test = train_test_split(X_pred, y_pred, test_size=0.35, random_state=76)\n\nrf = RandomForestClassifier(n_estimators=300 ,random_state=76, max_depth=20, min_samples_split=2)\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\nprint('Accuracy:', accuracy_score(y_test, y_pred))\nprint('\\nClassification Report:\\n', classification_report(y_test, y_pred, target_names=['Under 50k', 'Over 50k']))\n\n\nAccuracy: 0.6432926829268293\n\nClassification Report:\n               precision    recall  f1-score   support\n\n   Under 50k       0.69      0.64      0.67       183\n    Over 50k       0.59      0.64      0.61       145\n\n    accuracy                           0.64       328\n   macro avg       0.64      0.64      0.64       328\nweighted avg       0.65      0.64      0.64       328",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html#stretchtask-5",
    "href": "Cleansing_Projects/project5.html#stretchtask-5",
    "title": "Client Report - The War with Star Wars",
    "section": "STRETCH|TASK 5",
    "text": "STRETCH|TASK 5\nValidate the data provided on GitHub lines up with the article by recreating a 3rd visual from the article.\nFor this stretch challenge we were asked to recreate one more visual from the article to double check the validity of our data. I chose to recreate the “What’s the Best ‘Star Wars’ Movie?” In this visual we first had to filter the data further to those who have seen all the Star Wars movies. We then had to calculate and display the share of respondents who rated each fil as their favorite.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWhat is the best Star Wars movie?\n# Include and execute your code here\n\nrank_columns = [\n    'rank_i__the_phantom_menace',\n    'rank_ii__attack_of_the_clones',\n    'rank_iii__revenge_of_the_sith',\n    'rank_iv__a_new_hope',\n    'rank_v_the_empire_strikes_back',\n    'rank_vi_return_of_the_jedi',\n]\n\nmovie_titles = [\n    \"The Phantom Menace\",\n    \"Attack of the Clones\",\n    \"Revenge of the Sith\",\n    \"A New Hope\",\n    \"The Empire Strikes Back\",\n    \"Return of the Jedi\",\n]\n\nseen_all = df_filtered[seen_columns].all(axis=1)\ndf_seen_all = df_filtered[seen_all]\n\nfav_counts = [df_seen_all[col].value_counts().get(1, 0) for col in rank_columns]\n\ntotal_resp = df_seen_all.shape[0]\npercent_fav = [(count / total_resp) * 100 for count in fav_counts]\n\ndf_fav = pd.DataFrame({\n  'movie': movie_titles,\n  'percentage': percent_fav\n})\ndf_fav['percent_label'] = df_fav['percentage'].round(0).astype(int).astype(str) + '%'\ndf_fav['movie'] = pd.Categorical(df_fav['movie'], categories=movie_titles[::-1], ordered=True)\n\np5 = (ggplot(df_fav, aes(x='movie', y='percentage')) + geom_bar(stat='identity', fill='blue') + geom_text(aes(label='percent_label'), nudge_y=2, size=10) + labs(title='Favorite Star Wars Movies', subtitle='Of respondents who have seen all six films', x='Percentage of Respondents', y='Star Wars Movies') + coord_flip() + theme(axis_text_x=element_text(hjust=1)))\n\np5.show()",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html",
    "href": "Cleansing_Projects/project4.html",
    "title": "Client Report - Can You Predict That?",
    "section": "",
    "text": "Using the power of advanced machine learning, we developed a model that successfully predicted whether a home was built before or after 1980. Through model tuning we were able to get the accuracy of our model’s prediction at the 90% threshold making our model relatively reliable in predicting the year a home was built. We first Identified possible relationships between homes built before and after 1980. Our model found that one story architecture, garage type, and quality ratings of C were most correlated with homes from built before 1980. Our model was then explained and justified using a classification report, confusion matrix and an ROC curve confirming that our model was accurate and reliable. I also built a model to predict the year a home was built using the “GradientBoostingRegressor.” This model was able to successfully predict 89% of its results. For Justification we look at Mean Absolute Error (MAE), Mean Squared Error (MSE), R-Squared value, and a confusion matrix. \n\n\nRead and format project data\n# Include and execute your code here\n\ndwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html#elevator-pitch",
    "href": "Cleansing_Projects/project4.html#elevator-pitch",
    "title": "Client Report - Can You Predict That?",
    "section": "",
    "text": "Using the power of advanced machine learning, we developed a model that successfully predicted whether a home was built before or after 1980. Through model tuning we were able to get the accuracy of our model’s prediction at the 90% threshold making our model relatively reliable in predicting the year a home was built. We first Identified possible relationships between homes built before and after 1980. Our model found that one story architecture, garage type, and quality ratings of C were most correlated with homes from built before 1980. Our model was then explained and justified using a classification report, confusion matrix and an ROC curve confirming that our model was accurate and reliable. I also built a model to predict the year a home was built using the “GradientBoostingRegressor.” This model was able to successfully predict 89% of its results. For Justification we look at Mean Absolute Error (MAE), Mean Squared Error (MSE), R-Squared value, and a confusion matrix. \n\n\nRead and format project data\n# Include and execute your code here\n\ndwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html#questiontask-1",
    "href": "Cleansing_Projects/project4.html#questiontask-1",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np \nfrom lets_plot import * \nLetsPlot.setup_html(isolated_frame=True)\n\ndf_subset = dwellings_ml.filter(\n    ['livearea', 'finbsmnt', 'basement', \n    'yearbuilt', 'nocars', 'numbdrm', 'numbaths', 'before1980',\n    'stories', 'yrbuilt']).sample(500)\n\ndf_subset = df_subset.dropna(subset=['livearea', 'finbsmnt', 'before1980', 'basement', 'numbaths', 'numbdrm'])\n\ndf_subset['before1980'] = df_subset['before1980'].astype(bool)\n\n\nThis scatter plot shows us the possible relationship between the number of bedrooms built in the house and the total living space, or square footage, of the home. There seems to be a linear relationship between house size and number of rooms built. As the number of rooms increase, so does the total livable square footage.\n\n\nLiving Area vs Number of Bedrooms\n# Include and execute your code here\nc_1 = ggplot(df_subset, aes(x='livearea', y='numbdrm', color='before1980')) + geom_point(alpha=0.5) + ggtitle(\"Living Area vs Number of Bedrooms\") + xlab(\"Living Area\") + ylab(\"Number of Bedrooms\") + scale_color_manual(name='Built Before 1980', labels=['Yes (Blue)', 'No (Red)'], values=['blue', 'red']) + theme(legend_position='right') \n\nc_1.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nLiving Area vs Number of Bedrooms\n\n\nThis box and whisker plot shows is the relationship between basement sizes on homes built before 1980 and after 1980. From this we can see that homes that were built before 1980 had slightly larger mean basement size than homes built after 1980.\n\n\nFinished Basement Area by Before1980\n# Include and execute your code here\n\nc_2 = ggplot(df_subset, aes(x='before1980', y='finbsmnt', fill='before1980')) + geom_boxplot() + ggtitle('Finished Basement Area by Before1980') + xlab('Built Before 1980') + ylab('Finished Basement Area') + scale_fill_discrete(name=\"Built Before 1980\")\nc_2.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThis stacked bar chart tells us the relationship between the number of bathrooms built in a house based on if the house was built before or after 1980. It looks like the majority of houses built before 1980 had two bathrooms. \n\n\nNumber of Bathrooms by Before1980\n# Include and execute your code here\n\nc_3 = ggplot(df_subset, aes(x='numbaths', fill='before1980')) + geom_bar(position='stack') + ggtitle('Number of Bathrooms by Before1980') + xlab('Number of Bathrooms') + ylab('Count') + scale_fill_discrete(name=\"Built Before 1980\")\nc_3.show()",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html#questiontask-2",
    "href": "Cleansing_Projects/project4.html#questiontask-2",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\nI ended up using the DecisionTreeClassifier to create my classification model. I settled on this because this led to me getting the highest accuracy in the quickest time. I tried to use the Gradient Boosting, but it ended up taking longer to get the results. I also tried to add the following parameters: max_depth, min_sample_split, and min_samples_leaf. I found that this took a lot longer to get the results and it did not improve the accuracy rating at all when compared to the DecisionTreeClassifier with no parameters. I was able to achieve a 90% accuracy rating.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\n\n\n\n\nClassification Report\nX = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny = dwellings_ml.filter(regex = \"before1980\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .35, random_state = 76)\n\nmodel_1 = tree.DecisionTreeClassifier(random_state=76, max_depth=10, min_samples_split=10, min_samples_leaf=2)\nmodel_1.fit(X_train, y_train)\n\ny_pred = model_1.predict(X_test)\n\nprint('\\nTest Accuracy:', metrics.accuracy_score(y_test, y_pred))\nprint('\\nClassification Report: \\n', metrics.classification_report(y_test, y_pred))\n\n\n\nTest Accuracy: 0.9007481296758105\n\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       0.86      0.87      0.87      2980\n           1       0.92      0.92      0.92      5040\n\n    accuracy                           0.90      8020\n   macro avg       0.89      0.89      0.89      8020\nweighted avg       0.90      0.90      0.90      8020",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html#questiontask-3",
    "href": "Cleansing_Projects/project4.html#questiontask-3",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a feature importance chart and a description of the features.\nThe most important features identified in my model were “arcstyle_ONE_STORY”, “gartype_Att”, and “quality_C.” The model puts the most importance on how many stories the house is using “arcstyle_ONE_STORY.” One story homes were more popular before 1980 so there were more built of these types of homes. The second most important feature my model used was whether or not the house had an attached, or detached garage, “gartype_Att.” Most of the houses built before 1980 had detached garages. The third greatest importance the model used was the quality of the home from “quality_C.” Homes with this quality rating were more likely to be older homes. The model put these three fields together to best predict if the house was built before or after 1980.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nFeature Importance Chart\n# Include and execute your code here\nimportance = model_1.feature_importances_\nfeature_names = X.columns\n\nfeature_importance_df = pd.DataFrame({'feature': feature_names, 'Importance': importance})\nfeature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n\nfeature_importance_df_top = feature_importance_df.head(10)\n\nc_4 = ggplot(feature_importance_df_top, aes(x='feature', y='Importance', fill= 'Importance')) + geom_bar(stat='identity') + theme(axis_text_x=element_text(angle=90, hjust=1)) + ggtitle('Feature Importance') + xlab('Feature') + ylab('Importance')\nc_4.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nFeature Importance",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html#questiontask-4",
    "href": "Cleansing_Projects/project4.html#questiontask-4",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\nTo justify the results and quality of my model, I chose to look at the following three metrics: 1 - Classification report containing the accuracy, 2 - Confusion matrix, and 3 - The Receiver-operating characteristic (ROC) curve. Based on the results of this test, I can conclude that the quality of my model is strong and can be relied upon with an acceptable accuracy and true positive rate. Under each of the reports I will explain the significance and how to interpret the results.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nPerformance Metrics\n# Include and execute your code here\nprint('\\nTest Accuracy:', metrics.accuracy_score(y_test, y_pred))\nprint('\\nClassification Report: \\n', metrics.classification_report(y_test, y_pred))\n\n\n\nTest Accuracy: 0.9007481296758105\n\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       0.86      0.87      0.87      2980\n           1       0.92      0.92      0.92      5040\n\n    accuracy                           0.90      8020\n   macro avg       0.89      0.89      0.89      8020\nweighted avg       0.90      0.90      0.90      8020\n\n\n\nThe classification report and accuracy measure the percentage of correctly classified predictions compared to the total results. The higher the accuracy percentage is the more accurate the model is. A high accuracy rating is anything greater than or equal to 90% (0.90). The precision measures the percentage of total correct guesses divided by the number of true positives + false positives. For “0” this means that 86% of the home predicted to be built after 1980 are correct, and for the “1” 96% of the homes predicted to be built before 1980 are correct. Recall measures the total true positives divided by true positives + false negatives. For “0”, the model correctly identified 87% of the homes built after 1980 and for “1” correctly identified 92% of the homes built before 1980. The F-1 Score is calculated as 2 * (Precision * Recall / Precision + Recall). The F-1 score provides a balanced metric to look at our model performance. Our model is slightly stronger at predicting if a home was built before 1980 than after 1980 but overall reliable at predicting if it was built before or after 1980.\n\n\nConfusion Matrix\n# Include and execute your code here\n\nconf_matrix = metrics.confusion_matrix(y_test, y_pred)\nconf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted: (After 1980)', 'Predicted: (Before 1980)'], index=['Actual: (After 1980)', 'Actual: (Before 1980)'])\n\nconf_matrix_plot = ggplot(conf_matrix_df.reset_index().melt(id_vars='index'), aes(x='index', y='variable', fill='value')) + geom_tile() + geom_text(aes(label='value'), color='black', size=6) + scale_fill_brewer(palette='RdYlBu') + ggtitle('Confusion Matrix') + xlab('Predicted') + ylab('Actual')\n\nconf_matrix_plot.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThe confusion matrix is a table that is used to describe the performance of a classification model. The matrix shows the number of true positives (4630), false positives (386), true negatives (2594), and false negatives (410). From the confusion matrix we can see that our model had relatively few false positives / negatives in relation to the correct predictions. This proves how well my model was at predicting if a home was built before or after 1980.\n\n\nROC Curve\n# Include and execute your code here\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\n\nroc_df = pd.DataFrame({'FPR': fpr, 'TPR': tpr, 'Thresholds': thresholds})\n\nroc_chart = ggplot(roc_df, aes(x='FPR', y='TPR')) + geom_line(color='blue') + ggtitle('ROC Curve') + xlab('False Positive Rate') + ylab('True Positive Rate')\n\nroc_chart.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThe ROC curve is a graphical representation of the true positive rate against the false positive rate. The closer the curve is to the top left corner, the better the model is. The area under the curve (AUC) is used to measure the performance of the model. The higher the AUC, the better the model is. Our model had an AUC of 0.92 which shows us it performed very well at predicting when a house was built.",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html#stretchtask-5",
    "href": "Cleansing_Projects/project4.html#stretchtask-5",
    "title": "Client Report - Can You Predict That?",
    "section": "STRETCH|TASK 5",
    "text": "STRETCH|TASK 5\nCan you build a model that predicts the year a house was built? Explain the model and the evaluation metrics you would use to determine if the model is good.\nFor the model I chose to use the “GradientBoostingRegressor” because of its ability to sift through complex, non-linear relationships to get to the target prediction. In this case our goal was to predict what year a house was built which requires the model to look through the data and attempt to find patterns that could help predict when a house was built. The “GradientBoostingRegressor” is also very good at not over filtering the data like other models. This model type also very flexible and forgiving when it comes to tuning the model. Below are the figures and print outs I used to evaluate the accuracy of my model.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nRead and format data\n# Include and execute your code here\n\nX_5 = dwellings_ml.drop(dwellings_ml.filter(regex = 'yrbuilt|parcel').columns, axis = 1)\ny_5 = dwellings_ml[\"yrbuilt\"]\n\nX_5 = X_5.dropna()\ny_5 = y_5[X_5.index]\n\nX_train, X_test, y5_train, y5_test = train_test_split(X_5, y_5, test_size = .35, random_state = 76)\n\nrf_model = GradientBoostingRegressor(random_state=76, n_estimators=500, learning_rate=0.1, max_depth=15, min_samples_split=10, min_samples_leaf=2)\nrf_model.fit(X_train, y5_train)\n\ny5_pred = rf_model.predict(X_test)\n\nmae = metrics.mean_absolute_error(y5_test, y5_pred)\nmse = metrics.mean_squared_error(y5_test, y5_pred)\nr2 = metrics.r2_score(y5_test, y5_pred)\n\nprint(f'Mean Absolute Error (MAE): {mae:.2f}')\nprint(f'Mean Squared Error (MSE): {mse:.2f}')\nprint(f'R-Squared (R^2): {r2:.2f}')\n\n\nMean Absolute Error (MAE): 7.49\nMean Squared Error (MSE): 155.45\nR-Squared (R^2): 0.89\n\n\n\n\nClassification Report\n# Include and execute your code here\n\nbins = [0, 1980, 1995, 2010, 2023]\nlables = ['Pre 1980', '1980-1995', '1996-2010', 'Post 2010']\n\ny5_test_bins = pd.cut(y5_test, bins=bins, labels=lables)\ny5_pred_bins = pd.cut(y5_pred, bins=bins, labels=lables)\n\nprint('\\nTest Accuracy:', metrics.accuracy_score(y5_test_bins, y5_pred_bins))\nprint('\\nClassification Report: \\n', metrics.classification_report(y5_test_bins, y5_pred_bins))\n\n\n\nTest Accuracy: 0.9337905236907731\n\nClassification Report: \n               precision    recall  f1-score   support\n\n   1980-1995       0.67      0.68      0.68       418\n   1996-2010       0.84      0.91      0.87      1832\n   Post 2010       0.88      0.73      0.80       671\n    Pre 1980       1.00      0.99      0.99      5099\n\n    accuracy                           0.93      8020\n   macro avg       0.85      0.83      0.84      8020\nweighted avg       0.94      0.93      0.93      8020\n\n\n\nThe Mean Absolute Error (MAE) tells us that on average, our model’s predictions are off by only 7.5 years. With regression tasks like this a MAE of 10 years is typically considered very good. The Mean Squared Error (MSE) is similar to the MAE, but the MSE penalizes the model’s larger errors heavier than smaller errors. Our score of 156.62 equates to about 12.5 years of difference with a range of years that spans over 100 years. When we compare our R-Squared result with the Accuracy test below we can see that overall, our model is right within the acceptable margin of error.\n\n\nConfusion Matrix\n# Include and execute your code here\n\nconf5_matrix = metrics.confusion_matrix(y5_test_bins, y5_pred_bins, labels=lables)\nconf5_matrix_df = pd.DataFrame(conf5_matrix, columns=['Pred:(Pre 80)', 'Pred:(80-95)', 'Pred:(96-10)', 'Pred:(Post 10)'], index=['Actl:(Pre 80)', 'Actl:(80-95)', 'Actl:(96-10)', 'Actl:(Post 10)'])\n\nconf5_matrix_norm = conf5_matrix.astype('float') / conf5_matrix.sum(axis=1)[:, np.newaxis]\n\nconf5_matrix_norm_df = pd.DataFrame(conf5_matrix_norm, columns=['Pred:(Pre 80)', 'Pred:(80-95)', 'Pred:(96-10)', 'Pred:(Post 10)'], index=['Actl:(Pre 80)', 'Actl:(80-95)', 'Actl:(96-10)', 'Actl:(Post 10)'])\n\nconf5_matrix_norm_df_melted = conf5_matrix_norm_df.reset_index().melt(id_vars='index', var_name='Predicted', value_name='Accuracy')\n\nlbl_plot = (\n  pd.DataFrame(100 * conf5_matrix_norm)\n  .stack()\n  .reset_index(drop=True)\n  .round(1)\n  .astype(str) + '%'\n)\n\nconf5_matrix_plot = ggplot(conf5_matrix_norm_df_melted, aes(x='Predicted', y='index', fill='Accuracy')) + geom_tile() + geom_text(aes(label=lbl_plot), color='white', size=6) + scale_fill_gradient(low='red', high='blue', name='Accuracy') + ggtitle('Confusion Matrix') + xlab('Predicted') + ylab('Actual') + theme(axis_text_x=element_text(angle=90, hjust=1))\n\nconf5_matrix_plot.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nConfusion Matrix\n\n\nThe confusion matrix is a table that is used to describe the performance of a classification model. The matrix shows the number of true positives, false positives, true negatives, and false negatives. From the confusion matrix we can see that our model had relatively few false positives / negatives in relation to the correct predictions. This proves how well my model was at predicting when a house was built.",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html",
    "href": "Cleansing_Projects/project1.html",
    "title": "Client Report - What’s in a Name?",
    "section": "",
    "text": "What’s in a Name?\n\n\n\n\nElevator pitch\nIn comparing baby names and birth years looking for trends, I have learned that my name “Braxton” was not all that popular when I was born. If you were to get a call from a “Brittney” on the phone, it would be safe to assume that her age would be 34 years old. I also analyzed the effect of certain events on popularity of names.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\ndf \n\n\n\n\n\n\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n0\nAaden\n2005\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n1\nAaden\n2007\n0.0\n5.0\n0.0\n5.0\n20.0\n6.0\n0.0\n0.0\n...\n5.0\n14.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n98.0\n\n\n2\nAaden\n2008\n0.0\n15.0\n13.0\n20.0\n135.0\n10.0\n9.0\n0.0\n...\n15.0\n99.0\n8.0\n26.0\n0.0\n11.0\n19.0\n5.0\n0.0\n939.0\n\n\n3\nAaden\n2009\n0.0\n28.0\n20.0\n23.0\n158.0\n22.0\n12.0\n0.0\n...\n33.0\n140.0\n6.0\n17.0\n0.0\n31.0\n23.0\n14.0\n0.0\n1242.0\n\n\n4\nAaden\n2010\n0.0\n8.0\n6.0\n12.0\n62.0\n9.0\n5.0\n0.0\n...\n9.0\n56.0\n0.0\n11.0\n0.0\n7.0\n12.0\n0.0\n0.0\n414.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n393379\nZyon\n2011\n0.0\n0.0\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n...\n5.0\n9.0\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n117.0\n\n\n393380\nZyon\n2012\n0.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n7.5\n0.0\n12.0\n0.0\n0.0\n0.0\n0.0\n0.0\n118.5\n\n\n393381\nZyon\n2013\n0.0\n0.0\n0.0\n0.0\n6.0\n0.0\n0.0\n0.0\n...\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n93.0\n\n\n393382\nZyon\n2014\n0.0\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n...\n0.0\n7.0\n0.0\n9.0\n0.0\n0.0\n0.0\n0.0\n0.0\n73.0\n\n\n393383\nZyon\n2015\n0.0\n0.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n0.0\n10.0\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n87.5\n\n\n\n\n393384 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 1\nHow does your name at your birth year compare to its use historically?\nHistoricly it looks like my name was given after a slight rise in popularity. However, I was given my name before the largest spike after about 2002.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nPlot of the name Braxton\n# Include and execute your code here\nmy_name_df = df[df['name'] == 'Braxton']\nyear_2000_data = my_name_df[my_name_df['year'] == 2000]\n\n(ggplot(my_name_df, aes(x='year', y='Total')) +\n    geom_point(color = 'blue') +\n    ggtitle('The Use of Braxton') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5)) +\n    geom_text(aes(x=year_2000_data['year'], y=year_2000_data['Total']), label='782', nudge_x=-0.5, nudge_y=400) +\n    geom_segment(aes(x=year_2000_data['year'], xend=year_2000_data['year'], y=year_2000_data['Total'] - 10, yend=year_2000_data['Total']+350), arrow=arrow(length=5, type='closed'), color='red')\n)\n\n\n   \n   \nThe Use of Braxton\n\n\n\n\ntable of the name Braxton\ndisplay(my_name_df)\n\n\n\n\n\n\nBraxton’s Name Usage\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n50029\nBraxton\n1914\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n13.0\n\n\n50030\nBraxton\n1915\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n11.0\n\n\n50031\nBraxton\n1916\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12.0\n\n\n50032\nBraxton\n1917\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n50033\nBraxton\n1918\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n10.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50095\nBraxton\n2011\n6.0\n123.0\n84.0\n37.0\n62.0\n43.0\n0.0\n0.0\n...\n131.0\n188.0\n93.0\n40.0\n0.0\n33.0\n35.0\n68.0\n16.0\n2491.0\n\n\n50096\nBraxton\n2012\n5.0\n111.0\n79.0\n37.0\n61.0\n39.0\n5.0\n0.0\n...\n151.0\n227.0\n65.0\n58.0\n0.0\n41.0\n62.0\n95.0\n22.0\n2986.0\n\n\n50097\nBraxton\n2013\n0.0\n100.0\n84.0\n53.0\n92.0\n49.0\n7.0\n0.0\n...\n158.0\n226.0\n63.0\n64.0\n0.0\n42.0\n62.0\n74.0\n9.0\n3085.0\n\n\n50098\nBraxton\n2014\n10.0\n121.0\n86.0\n45.0\n106.0\n43.0\n6.0\n0.0\n...\n150.0\n147.0\n79.0\n60.0\n0.0\n39.0\n56.0\n74.0\n20.0\n3186.0\n\n\n50099\nBraxton\n2015\n0.0\n103.0\n76.0\n48.0\n102.0\n38.0\n6.0\n0.0\n...\n173.0\n278.0\n54.0\n61.0\n0.0\n57.0\n74.0\n67.0\n9.0\n3265.0\n\n\n\n\n71 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nIt appears that the most popular year for the use of the name “Brittney” was 1990. This means that we could make a educated guess that a person named Britteny would be 34 years old.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nBar chart of the name Brittany\nbrittany_df = df[df['name'] == 'Brittany']\n\n(ggplot(brittany_df, aes(x='year', y='Total')) +\n    geom_bar(stat='identity') +\n    ggtitle('The Popularity of Brittney Over Time') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nThe Popularity of Brittney Over Time\n\n\n\n\nTable of the name Brittany\n# Include and execute your code here\n\ndisplay(brittany_df)\n\n\n\n\n\n\nBrittany’s Name Usage\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n53205\nBrittany\n1968\n0.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n53206\nBrittany\n1969\n0.0\n0.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12.0\n\n\n53207\nBrittany\n1970\n0.0\n0.0\n0.0\n0.0\n5.0\n5.0\n0.0\n0.0\n...\n0.0\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n32.0\n\n\n53208\nBrittany\n1971\n0.0\n0.0\n0.0\n5.0\n17.0\n0.0\n0.0\n0.0\n...\n0.0\n14.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n81.0\n\n\n53209\nBrittany\n1972\n0.0\n0.0\n0.0\n0.0\n11.0\n10.0\n0.0\n0.0\n...\n8.0\n14.0\n16.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n158.0\n\n\n53210\nBrittany\n1973\n0.0\n0.0\n0.0\n6.0\n17.0\n0.0\n0.0\n0.0\n...\n0.0\n18.0\n13.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n166.0\n\n\n53211\nBrittany\n1974\n0.0\n0.0\n7.0\n0.0\n19.0\n0.0\n0.0\n0.0\n...\n0.0\n17.0\n28.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n198.0\n\n\n53212\nBrittany\n1975\n0.0\n0.0\n0.0\n0.0\n28.0\n0.0\n0.0\n0.0\n...\n10.0\n23.0\n28.0\n0.0\n0.0\n9.0\n5.0\n0.0\n0.0\n277.0\n\n\n53213\nBrittany\n1976\n0.0\n0.0\n6.0\n0.0\n36.0\n5.0\n0.0\n0.0\n...\n15.0\n35.0\n21.0\n7.0\n0.0\n7.0\n11.0\n0.0\n0.0\n304.0\n\n\n53214\nBrittany\n1977\n0.0\n9.0\n8.0\n5.0\n39.0\n10.0\n0.0\n0.0\n...\n16.0\n48.0\n55.0\n5.0\n0.0\n0.0\n7.0\n0.0\n0.0\n448.0\n\n\n53215\nBrittany\n1978\n0.0\n8.0\n12.0\n8.0\n50.0\n17.0\n0.0\n0.0\n...\n15.0\n48.0\n67.0\n7.0\n0.0\n11.0\n5.0\n0.0\n0.0\n592.0\n\n\n53216\nBrittany\n1979\n0.0\n6.0\n13.0\n0.0\n47.0\n19.0\n10.0\n0.0\n...\n20.0\n76.0\n63.0\n14.0\n0.0\n13.0\n10.0\n7.0\n8.0\n764.0\n\n\n53217\nBrittany\n1980\n0.0\n11.0\n20.0\n13.0\n121.0\n41.0\n5.0\n0.0\n...\n39.0\n142.0\n110.0\n18.0\n0.0\n36.0\n28.0\n10.0\n10.0\n1383.0\n\n\n53218\nBrittany\n1981\n11.0\n23.0\n32.0\n22.0\n127.0\n37.0\n14.0\n0.0\n...\n54.0\n166.0\n116.0\n21.0\n0.0\n47.0\n24.0\n10.0\n14.0\n1701.0\n\n\n53219\nBrittany\n1982\n12.0\n50.0\n53.0\n47.0\n290.0\n88.0\n18.0\n5.0\n...\n102.0\n300.0\n152.0\n50.0\n7.0\n73.0\n58.0\n25.0\n16.0\n3093.0\n\n\n53220\nBrittany\n1983\n20.0\n71.0\n80.0\n66.0\n406.0\n131.0\n26.0\n19.0\n...\n133.0\n415.0\n156.0\n83.0\n7.0\n95.0\n68.0\n42.0\n19.0\n4377.0\n\n\n53221\nBrittany\n1984\n34.0\n158.0\n111.0\n112.0\n698.0\n225.0\n59.0\n22.0\n...\n243.0\n735.0\n159.0\n148.0\n6.0\n134.0\n114.0\n78.0\n36.0\n7664.0\n\n\n53222\nBrittany\n1985\n53.0\n351.0\n224.0\n217.0\n715.5\n319.0\n118.0\n48.0\n...\n500.0\n1256.0\n182.0\n390.0\n25.0\n231.0\n260.0\n235.0\n40.0\n14010.0\n\n\n53223\nBrittany\n1986\n49.0\n241.5\n269.0\n309.0\n886.5\n389.0\n165.0\n79.0\n...\n586.0\n792.5\n183.0\n533.0\n45.0\n287.0\n361.0\n229.0\n48.0\n17856.5\n\n\n53224\nBrittany\n1987\n56.0\n528.0\n267.0\n322.0\n977.0\n365.0\n196.0\n97.0\n...\n633.0\n795.5\n193.0\n628.0\n34.0\n289.0\n351.0\n241.0\n42.0\n18825.5\n\n\n53225\nBrittany\n1988\n48.0\n585.0\n304.0\n383.0\n1214.5\n370.0\n271.0\n126.0\n...\n409.5\n925.0\n167.0\n884.0\n43.0\n368.0\n455.0\n270.0\n41.0\n21952.0\n\n\n53226\nBrittany\n1989\n53.0\n837.0\n457.0\n549.0\n1513.0\n439.0\n371.0\n86.5\n...\n537.5\n1266.0\n185.0\n1247.0\n71.0\n500.0\n724.0\n402.0\n50.0\n30848.0\n\n\n53227\nBrittany\n1990\n75.0\n788.0\n440.0\n483.0\n1372.5\n450.0\n345.0\n90.0\n...\n546.5\n1181.5\n173.0\n1229.0\n86.0\n470.0\n671.0\n379.0\n38.0\n32562.5\n\n\n53228\nBrittany\n1991\n55.0\n585.0\n327.0\n379.0\n1155.5\n361.0\n293.0\n132.0\n...\n872.0\n983.0\n140.0\n959.0\n47.0\n363.0\n556.0\n352.0\n43.0\n26963.5\n\n\n53229\nBrittany\n1992\n51.0\n526.0\n282.0\n292.0\n915.0\n325.0\n223.0\n87.0\n...\n726.0\n1785.0\n119.0\n854.0\n57.0\n307.0\n465.0\n292.0\n40.0\n23416.5\n\n\n53230\nBrittany\n1993\n37.0\n511.0\n229.0\n298.0\n1597.0\n275.0\n199.0\n62.0\n...\n637.0\n1547.0\n123.0\n697.0\n44.0\n286.0\n389.0\n242.0\n26.0\n21728.0\n\n\n53231\nBrittany\n1994\n47.0\n422.0\n237.0\n227.0\n666.5\n227.0\n168.0\n72.0\n...\n599.0\n1359.0\n91.0\n641.0\n43.0\n229.0\n351.0\n228.0\n32.0\n17808.5\n\n\n53232\nBrittany\n1995\n34.0\n342.0\n190.0\n202.0\n605.5\n179.0\n156.0\n49.0\n...\n516.0\n1293.0\n110.0\n537.0\n40.0\n216.0\n271.0\n194.0\n19.0\n15875.5\n\n\n53233\nBrittany\n1996\n29.0\n318.0\n163.0\n195.0\n977.0\n163.0\n133.0\n31.0\n...\n400.0\n1147.0\n107.0\n455.0\n25.0\n166.0\n239.0\n147.0\n18.0\n13796.0\n\n\n53234\nBrittany\n1997\n19.0\n253.0\n166.0\n138.0\n825.0\n141.0\n111.0\n24.0\n...\n342.0\n966.0\n85.0\n334.0\n19.0\n147.0\n230.0\n102.0\n7.0\n11527.0\n\n\n53235\nBrittany\n1998\n29.0\n190.0\n109.0\n165.0\n735.0\n134.0\n85.0\n22.0\n...\n324.0\n828.0\n75.0\n310.0\n23.0\n121.0\n170.0\n114.0\n11.0\n9843.0\n\n\n53236\nBrittany\n1999\n18.0\n163.0\n97.0\n114.0\n607.0\n135.0\n69.0\n20.0\n...\n265.0\n698.0\n70.0\n246.0\n13.0\n104.0\n126.0\n83.0\n11.0\n7942.0\n\n\n53237\nBrittany\n2000\n14.0\n111.0\n63.0\n88.0\n354.0\n71.0\n33.0\n14.0\n...\n199.0\n432.0\n41.0\n176.0\n9.0\n59.0\n91.0\n71.0\n6.0\n5183.0\n\n\n53238\nBrittany\n2001\n6.0\n92.0\n43.0\n60.0\n223.0\n35.0\n27.0\n9.0\n...\n109.0\n287.0\n21.0\n83.0\n5.0\n33.0\n33.0\n50.0\n0.0\n2915.0\n\n\n53239\nBrittany\n2002\n0.0\n39.0\n28.0\n32.0\n144.0\n32.0\n9.0\n7.0\n...\n66.0\n206.0\n22.0\n70.0\n5.0\n16.0\n30.0\n29.0\n0.0\n1912.0\n\n\n53240\nBrittany\n2003\n0.0\n32.0\n22.0\n25.0\n148.0\n23.0\n16.0\n5.0\n...\n54.0\n177.0\n16.0\n42.0\n0.0\n15.0\n25.0\n19.0\n0.0\n1559.0\n\n\n53241\nBrittany\n2004\n0.0\n31.0\n23.0\n24.0\n139.0\n20.0\n0.0\n5.0\n...\n50.0\n137.0\n16.0\n37.0\n0.0\n14.0\n18.0\n28.0\n6.0\n1323.5\n\n\n53242\nBrittany\n2005\n0.0\n28.0\n18.0\n29.0\n116.0\n24.0\n7.0\n0.0\n...\n45.0\n148.0\n18.0\n35.0\n0.0\n9.0\n17.0\n13.0\n0.0\n1168.0\n\n\n53243\nBrittany\n2006\n0.0\n20.0\n13.0\n20.0\n121.0\n12.0\n6.0\n0.0\n...\n41.0\n103.0\n9.0\n30.0\n0.0\n10.0\n14.0\n9.0\n0.0\n1009.0\n\n\n53244\nBrittany\n2007\n0.0\n14.0\n12.0\n26.0\n126.0\n14.0\n0.0\n0.0\n...\n45.0\n96.0\n7.0\n27.0\n0.0\n14.0\n9.0\n9.0\n0.0\n891.0\n\n\n53245\nBrittany\n2008\n0.0\n15.0\n0.0\n14.0\n102.0\n5.0\n0.0\n0.0\n...\n30.0\n92.0\n9.0\n19.0\n0.0\n9.0\n16.0\n5.0\n0.0\n749.0\n\n\n53246\nBrittany\n2009\n0.0\n7.0\n6.0\n10.0\n105.0\n12.0\n0.0\n0.0\n...\n18.0\n83.0\n6.0\n26.0\n0.0\n10.0\n7.0\n0.0\n0.0\n644.0\n\n\n53247\nBrittany\n2010\n0.0\n9.0\n0.0\n20.0\n116.0\n9.0\n0.0\n0.0\n...\n31.0\n94.0\n11.0\n22.0\n0.0\n8.0\n6.0\n0.0\n0.0\n698.0\n\n\n53248\nBrittany\n2011\n0.0\n12.0\n7.0\n17.0\n109.0\n10.0\n9.0\n0.0\n...\n18.0\n91.0\n6.0\n32.0\n0.0\n7.0\n11.0\n0.0\n0.0\n717.0\n\n\n53249\nBrittany\n2012\n0.0\n12.0\n13.0\n9.0\n137.0\n11.0\n8.0\n6.0\n...\n23.0\n94.0\n6.0\n24.0\n0.0\n7.0\n10.0\n0.0\n0.0\n745.0\n\n\n53250\nBrittany\n2013\n0.0\n13.0\n0.0\n14.0\n110.0\n7.0\n9.0\n5.0\n...\n11.0\n113.0\n7.0\n25.0\n0.0\n12.0\n8.0\n0.0\n0.0\n699.0\n\n\n53251\nBrittany\n2014\n0.0\n11.0\n5.0\n8.0\n112.0\n9.0\n10.0\n0.0\n...\n21.0\n110.0\n8.0\n15.0\n0.0\n10.0\n6.0\n0.0\n0.0\n660.0\n\n\n53252\nBrittany\n2015\n0.0\n9.0\n6.0\n10.0\n109.0\n11.0\n0.0\n0.0\n...\n16.0\n124.0\n9.0\n26.0\n0.0\n9.0\n0.0\n0.0\n0.0\n636.0\n\n\n\n\n48 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names in a single chart. What trends do you notice?\nIt seems to be that ‘Mary’ was the most common name for most of the early 1920’s. Once we get to the 1970’s however, all four names seem to take a steep decline and the doesn’t climb back up. I am lead to assume that as the world became less religious, biblical names became less popular.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nLine chart of the names Mary, Martha, Peter, and Paul\nnew_names_df = df[df['name'].isin(['Mary', 'Martha', 'Peter', 'Paul']) & (df['year'] &gt;= 1920) & (df['year'] &lt;= 2000)]\n\n(ggplot (new_names_df, aes(x='year', y='Total', color='name')) +\ngeom_line() + \n    ggtitle('Mary, Martha, Peter, and Paul Over Time') + xlab('Year') + ylab('Total') + \n    scale_x_continuous(format='d') + \n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nMary, Martha, Peter, and Paul Over Time\n\n\n\n\nMary, Martha, Peter, and Paul table\ndisplay(new_names_df)\n\n\n\n\n\n\nMary, Martha, Peter, and Paul Over Time\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n264124\nMartha\n1920\n11.0\n258.0\n224.0\n21.0\n131.0\n66.0\n55.0\n20.0\n...\n416.0\n418.0\n16.0\n269.0\n11.0\n73.0\n116.0\n172.0\n11.0\n8705.0\n\n\n264125\nMartha\n1921\n0.0\n307.0\n216.0\n18.0\n161.0\n70.0\n57.0\n34.0\n...\n420.0\n478.0\n20.0\n297.0\n13.0\n57.0\n101.0\n204.0\n9.0\n9254.0\n\n\n264126\nMartha\n1922\n9.0\n326.0\n219.0\n23.0\n126.0\n67.0\n56.0\n17.0\n...\n421.0\n501.0\n18.0\n273.0\n14.0\n39.0\n82.0\n225.0\n5.0\n9018.0\n\n\n264127\nMartha\n1923\n0.0\n341.0\n236.0\n27.0\n159.0\n63.0\n38.0\n24.0\n...\n442.0\n233.0\n22.0\n293.0\n11.0\n45.0\n72.0\n210.0\n10.0\n8731.0\n\n\n264128\nMartha\n1924\n0.0\n342.0\n257.0\n39.0\n166.0\n58.0\n49.0\n23.0\n...\n507.0\n487.0\n15.0\n155.0\n15.0\n41.0\n72.0\n196.0\n20.0\n9163.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n303691\nPeter\n1996\n0.0\n22.0\n8.0\n42.0\n501.0\n55.0\n134.0\n22.0\n...\n30.0\n183.0\n27.0\n103.0\n20.0\n109.0\n114.0\n8.0\n0.0\n4069.0\n\n\n303692\nPeter\n1997\n17.0\n19.0\n5.0\n47.0\n476.0\n44.0\n111.0\n25.0\n...\n24.0\n175.0\n27.0\n73.0\n17.0\n74.0\n98.0\n0.0\n0.0\n3821.0\n\n\n303693\nPeter\n1998\n14.0\n21.0\n8.0\n42.0\n471.0\n42.0\n104.0\n14.0\n...\n27.0\n134.0\n41.0\n78.0\n12.0\n98.0\n89.0\n0.0\n0.0\n3377.0\n\n\n303694\nPeter\n1999\n15.0\n24.0\n9.0\n48.0\n402.0\n66.0\n101.0\n23.0\n...\n30.0\n138.0\n26.0\n105.0\n10.0\n77.0\n81.0\n0.0\n0.0\n3430.0\n\n\n303695\nPeter\n2000\n11.0\n14.0\n11.0\n40.0\n379.0\n66.0\n72.0\n20.0\n...\n26.0\n147.0\n24.0\n62.0\n12.0\n68.0\n70.0\n0.0\n0.0\n3137.0\n\n\n\n\n324 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nLooking at the graph it looks as though there was a little rise in the use of the name after 1993. The rise after 1993 is slight and would be hard to conclude that the increase is correlated with the release of “The Sandlot”.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nLine chart of the name Benjamin\nbenjamin_df = df[df['name'] == 'Benjamin']\n\n(ggplot(benjamin_df, aes(x='year', y='Total')) +\n    geom_line(color = 'blue') +\n    geom_vline(xintercept = 1993, linetype = 'dashed', color = 'red') +\n    geom_text(x = 1992, y = 15000, label = 'Sandlot was Released', hjust = 1) +\n    geom_text(x = 1960, y = 14200, label = '(1993)', hjust = 0) +\n    ggtitle('Benjamin from The Sandlot') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nBenjamin from The Sandlot\n\n\n\n\nBenjamin from The Sandlot table\ndisplay(benjamin_df)\n\n\n\n\n\n\nBenjamin from The Sandlot\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n41526\nBenjamin\n1910\n0.0\n13.0\n10.0\n5.0\n5.0\n0.0\n7.0\n0.0\n...\n0.0\n22.0\n0.0\n23.0\n0.0\n0.0\n5.0\n0.0\n0.0\n497.0\n\n\n41527\nBenjamin\n1911\n0.0\n14.0\n10.0\n0.0\n7.0\n7.0\n22.0\n7.0\n...\n7.0\n16.0\n0.0\n26.0\n0.0\n0.0\n7.0\n0.0\n0.0\n645.0\n\n\n41528\nBenjamin\n1912\n0.0\n25.0\n13.0\n0.0\n18.0\n7.0\n25.0\n7.0\n...\n24.0\n41.0\n0.0\n47.0\n0.0\n7.0\n19.0\n13.0\n0.0\n1245.0\n\n\n41529\nBenjamin\n1913\n0.0\n29.0\n19.0\n0.0\n16.0\n9.0\n31.0\n15.0\n...\n22.0\n57.0\n5.0\n49.0\n0.0\n14.0\n19.0\n15.0\n0.0\n1451.0\n\n\n41530\nBenjamin\n1914\n0.0\n50.0\n31.0\n5.0\n29.0\n12.0\n37.0\n8.0\n...\n35.0\n71.0\n0.0\n59.0\n0.0\n11.0\n21.0\n11.0\n0.0\n1802.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41627\nBenjamin\n2011\n23.0\n165.0\n91.0\n281.0\n1592.0\n247.0\n157.0\n57.0\n...\n244.0\n988.0\n193.0\n403.0\n33.0\n308.0\n271.0\n49.0\n17.0\n12743.0\n\n\n41628\nBenjamin\n2012\n31.0\n141.0\n87.0\n264.0\n1676.0\n212.0\n157.0\n52.0\n...\n252.0\n1035.0\n158.0\n364.0\n29.0\n336.0\n238.0\n57.0\n20.0\n12411.5\n\n\n41629\nBenjamin\n2013\n34.0\n125.0\n81.0\n300.0\n1845.0\n242.0\n145.0\n37.0\n...\n279.0\n1079.0\n178.0\n363.0\n21.0\n333.0\n262.0\n56.0\n18.0\n13460.0\n\n\n41630\nBenjamin\n2014\n36.0\n128.0\n93.0\n270.0\n1873.0\n252.0\n179.0\n64.0\n...\n249.0\n1152.0\n153.0\n352.0\n26.0\n370.0\n214.0\n50.0\n25.0\n13761.0\n\n\n41631\nBenjamin\n2015\n34.0\n151.0\n110.0\n275.0\n1809.0\n248.0\n178.0\n57.0\n...\n239.0\n1154.0\n168.0\n383.0\n21.0\n356.0\n232.0\n72.0\n29.0\n13608.0\n\n\n\n\n106 rows × 54 columns\n\n\n\n–&gt; –&gt;\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Machine_Learning/project3.html#elevator-pitch",
    "href": "Machine_Learning/project3.html#elevator-pitch",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nIn analyzing all the baseball data, we were able to gain a few valuable key insights. First, we analyzed all the players that had BYU-Idaho listed as their college. We looked at what team they played for, what year they played for the team, and how much their salary was. Next, we identified and raked players batting averages using three different parameters. We found that as we include more data over years of players careers, we can get closer to the true batting average. Lastly, we compared two teams, the Los Angeles Dodgers and the New York Yankees, total wins from 1985 to the latest year of the data. We observed that the New York Yankees had a higher frequency of wins than the Los Angeles Dodgers.\n\n\nRead and format project data\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)",
    "crumbs": [
      "Machine Learning",
      "Project 3"
    ]
  },
  {
    "objectID": "Machine_Learning/project3.html#questiontask-1",
    "href": "Machine_Learning/project3.html#questiontask-1",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nThe table displays the highest salaries reported by the players that came from BYU-Idaho. Matt Lindstrom (Lindsma01) had the most success out of all the baseball players that went to school in BYU-Idaho. Matt Lindstrom’s highest paid year on record was with the Chicago White Sox in 2014. Matt had a salary with the White Sox of $4 million.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\n\n\nBYU-Idaho Players Salaries\n# Include and execute your code here\nq_1 = \"\"\"\nselect distinct p.playerID, c.schoolID, printf('$%,.0f',s.salary) as 'Salary', s.yearID, s.teamID\nfrom collegeplaying c\njoin Salaries s on c.playerID = s.playerID\njoin People p on c.playerID = p.playerID\nwhere c.schoolID = 'idbyuid'\norder by s.salary desc\nlimit 10\n\"\"\"\nsalary_df = pd.read_sql_query(q_1,con)\ndisplay(salary_df)\n\n\n\n\n\n\n\n\n\nplayerID\nschoolID\nSalary\nyearID\nteamID\n\n\n\n\n0\nlindsma01\nidbyuid\n$4000000\n2014\nCHA\n\n\n1\nlindsma01\nidbyuid\n$3600000\n2012\nBAL\n\n\n2\nlindsma01\nidbyuid\n$2800000\n2011\nCOL\n\n\n3\nlindsma01\nidbyuid\n$2300000\n2013\nCHA\n\n\n4\nlindsma01\nidbyuid\n$1625000\n2010\nHOU\n\n\n5\nstephga01\nidbyuid\n$1025000\n2001\nSLN\n\n\n6\nstephga01\nidbyuid\n$900000\n2002\nSLN\n\n\n7\nstephga01\nidbyuid\n$800000\n2003\nSLN\n\n\n8\nstephga01\nidbyuid\n$550000\n2000\nSLN\n\n\n9\nlindsma01\nidbyuid\n$410000\n2009\nFLO\n\n\n\n\n\nBYU-Idaho Players Salaries",
    "crumbs": [
      "Machine Learning",
      "Project 3"
    ]
  },
  {
    "objectID": "Machine_Learning/project3.html#questiontask-2",
    "href": "Machine_Learning/project3.html#questiontask-2",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\nPart 1: Write an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\nFrom part one we can see that when we include all those players who had at least at bat for the year their batting average for the year was perfect. All the top 5 players from part one had a batting average of 100%.\n::: {#cell-Q2 Part 1 .cell execution_count=6}\n\nBatting average\n# Include and execute your code here\nq2_1 = \"\"\"\nselect playerID, yearID, (h*1.0)/ab as 'BA'\nfrom batting\nwhere ab &gt;= 1\norder by (h*1.0)/ab desc, playerID asc\nlimit 5\n\"\"\"\nba_part_1 = pd.read_sql_query(q2_1,con)\ndisplay(ba_part_1)\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nBA\n\n\n\n\n0\naberal01\n1957\n1.0\n\n\n1\nabernte02\n1960\n1.0\n\n\n2\nabramge01\n1923\n1.0\n\n\n3\nacklefr01\n1964\n1.0\n\n\n4\nalanirj01\n2019\n1.0\n\n\n\n\n\nTop 5 Batting Averages\n\n:::\nPart 2: Use the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\nFor part two we start to whittle down the results, instead of players with at least one at bat we increase the threshold to at least 10 at bats. Now, our best batting average was only 64% instead of players have a 100% batting average.\n::: {#cell-Q2 Part 2 .cell execution_count=7}\n\nBatting average\n# Include and execute your code here\nq2_2 = \"\"\"\nselect playerID, yearID, (h*1.0)/ab as 'BA'\nfrom batting\nwhere ab &gt;= 10\norder by (h*1.0)/ab desc, playerID\nlimit 5\n\"\"\"\nba_part_2 = pd.read_sql_query(q2_2,con)\ndisplay(ba_part_2)\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nBA\n\n\n\n\n0\nnymanny01\n1974\n0.642857\n\n\n1\ncarsoma01\n2013\n0.636364\n\n\n2\naltizda01\n1910\n0.600000\n\n\n3\njohnsde01\n1975\n0.600000\n\n\n4\nsilvech01\n1948\n0.571429\n\n\n\n\n\nTop 5 Batting Averages\n\n:::\nPart 3: Now calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\nFor part three we whittled down even further by obtaining players lifetime batting average across all the years they played. The highest player had a batting average of only 35.8%. Following the progression of the parameters. When we widen the number of at bats and hits, it becomes clear that the best players do not always have a 100% batting average like we saw in part one\n::: {#cell-Q2 Part 3 .cell execution_count=8}\n\nBatting average\n# Include and execute your code here\nq2_3 = \"\"\"\nselect playerID, sum(h*1.0)/sum(ab) as 'BA'\nfrom batting\nwhere ab &gt;= 100\ngroup by playerID\norder by (h*1.0)/ab desc, playerID\nlimit 5\n\"\"\"\nba_part_3 = pd.read_sql_query(q2_3,con)\ndisplay(ba_part_3)\n\n\n\n\n\n\n\n\n\nplayerID\nBA\n\n\n\n\n0\nmeyerle01\n0.357542\n\n\n1\nmcveyca01\n0.345802\n\n\n2\njacksjo01\n0.357009\n\n\n3\nhazlebo01\n0.402985\n\n\n4\nbarnero01\n0.363201\n\n\n\n\n\nTop 5 Batting Averages\n\n:::",
    "crumbs": [
      "Machine Learning",
      "Project 3"
    ]
  },
  {
    "objectID": "Machine_Learning/project3.html#questiontask-3",
    "href": "Machine_Learning/project3.html#questiontask-3",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Lets-Plot to visualize the comparison. What do you learn?\nI chose to use total wins for each year to compare the Los Angeles Dodgers (LAN) to the total wins for the New York Yankees (NYA). I chose this measure to compare the teams because I feel as though looking at the wins of the two teams gives a great overall comparison of the success of the team. In examining the data and graph I concluded that the New York Yankees had successful year more frequently than the Los Angeles Dodgers. Both teams had their good years and bad years. Overall, the New York Yankees won more frequently than the Los Angeles Dodgers.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\nfrom lets_plot import *\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nTable of team wins\n# Include and execute your code here\n\nq_3 = \"\"\"\nselect teamID, yearID, sum(w) as 'Total_Wins'\nfrom Teams\nwhere teamID in ('NYA', 'LAN') and yearID &gt;= 1985\ngroup by teamID, yearID\norder by yearID asc\n\"\"\"\n\nwins_df = pd.read_sql_query(q_3,con)\nwins_df.tail(14)\n\n\n\n\n\n\n\n\n\nteamID\nyearID\nTotal_Wins\n\n\n\n\n56\nLAN\n2013\n92\n\n\n57\nNYA\n2013\n85\n\n\n58\nLAN\n2014\n94\n\n\n59\nNYA\n2014\n84\n\n\n60\nLAN\n2015\n92\n\n\n61\nNYA\n2015\n87\n\n\n62\nLAN\n2016\n91\n\n\n63\nNYA\n2016\n84\n\n\n64\nLAN\n2017\n104\n\n\n65\nNYA\n2017\n91\n\n\n66\nLAN\n2018\n92\n\n\n67\nNYA\n2018\n100\n\n\n68\nLAN\n2019\n106\n\n\n69\nNYA\n2019\n103\n\n\n\n\n\nWins by Team\n\n\n\n\nGraph of wins\n# Include and execute your code here\n\n(ggplot(wins_df, aes(x='yearID', y='Total_Wins', color='teamID')) + geom_line() + ggtitle('Total Wins by Team') + xlab('Year') + ylab('Total Wins') + scale_x_continuous(format='d') + theme(plot_title=element_text(hjust=0.5))\n)",
    "crumbs": [
      "Machine Learning",
      "Project 3"
    ]
  },
  {
    "objectID": "Machine_Learning/project3.html#questiontaskstretch-4",
    "href": "Machine_Learning/project3.html#questiontaskstretch-4",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK(STRETCH) 4",
    "text": "QUESTION|TASK(STRETCH) 4\nAdvanced Salary Distribution by Position\nFrom this we can see that the on average the highest paid position is “Designated Hitter”. While this position has the highest average salary, when we look at the higest (max) salary for each position we find that “Third Baseman” and “Pitcher” are tied for the higest salary at $33 million. All positions are categorized as “High Salary”.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\n\n\nAverage salary by position\n# Include and execute your code here\n\nq_4 = \"\"\"\nselect \n  p.position, printf('$%,.0f',avg(s.salary)) as 'Average_Salary', count(distinct s.playerID) as 'Total_Players',\n  coalesce(printf('$%,.0f',max(s.salary)), 'N/A') as 'Highest_Salary',\n  case \n    when avg(s.salary) &gt; 1000000 then 'High Salary'\n    when avg(s.salary) between 5000000 and 1000000 then 'Medium salary'\n    else 'Low Salary'\n  end as salary_category\nfrom Salaries s\njoin appearances a on s.playerID = a.playerID\njoin(\n  select playerID, max(position) as position\n  from (\n    select playerID,\n      case\n        when G_p &gt; 0 then 'Pitcher'\n        when G_c &gt; 0 then 'Catcher'\n        when G_1b &gt; 0 then 'First Baseman'\n        when G_2b &gt; 0 then 'Second Baseman'\n        when G_3b &gt; 0 then 'Third Baseman'\n        when G_ss &gt; 0 then 'Shortstop'\n        when G_lf &gt; 0 then 'Left Fielder'\n        when G_cf &gt; 0 then 'Center Fielder'\n        when G_rf &gt; 0 then 'Right Fielder'\n        when G_of &gt; 0 then 'Outfielder'\n        when G_dh &gt; 0 then 'Designated Hitter'\n        when G_ph &gt; 0 then 'Pinch Hitter'\n        when G_pr &gt; 0 then 'Pinch Runner'\n        \n      end as position\n    from appearances\n  )\n  group by playerID\n) p on s.playerID = p.playerID\ngroup by p.position\norder by avg(s.salary) desc\nlimit 10\n\"\"\"\n\nsalary_df = pd.read_sql_query(q_4,con)\ndisplay(salary_df)\n\n\n\n\n\n\n\n\n\nposition\nAverage_Salary\nTotal_Players\nHighest_Salary\nsalary_category\n\n\n\n\n0\nDesignated Hitter\n$6625195\n7\n$16071429\nHigh Salary\n\n\n1\nFirst Baseman\n$4086740\n248\n$25000000\nHigh Salary\n\n\n2\nRight Fielder\n$3713566\n233\n$23854494\nHigh Salary\n\n\n3\nThird Baseman\n$2687984\n487\n$33000000\nHigh Salary\n\n\n4\nLeft Fielder\n$2640393\n521\n$27328046\nHigh Salary\n\n\n5\nShortstop\n$2500582\n169\n$22600000\nHigh Salary\n\n\n6\nPitcher\n$2364202\n2745\n$33000000\nHigh Salary\n\n\n7\nSecond Baseman\n$2020024\n370\n$24000000\nHigh Salary\n\n\n8\nCatcher\n$1617095\n298\n$20777778\nHigh Salary\n\n\n9\nCenter Fielder\n$1556184\n15\n$3900000\nHigh Salary\n\n\n\n\n\nAverage Salary by Position",
    "crumbs": [
      "Machine Learning",
      "Project 3"
    ]
  },
  {
    "objectID": "Machine_Learning/project2.html",
    "href": "Machine_Learning/project2.html",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Using the data collected from the Bureau of Transportation Statistics I have determined what airports are the best to fly out of and when to fly in general to have the best chance of avoiding delays for several reasons altogether. Chicago O’Hare International Airport (ORD) was determined to be the worst airport based on the average length of delay (1.13 Hours). ORD also had the second highest proportion of delays due to weather out of all the airports I looked at. The highest proportion of delays due to weather was San Francisco International Airport (SFO). I also determined that September has the lowest proportion of total flights delayed. The holiday months (January, July, November, and December) have the highest likelihood of experiencing delays\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\"\ndf = pd.read_json(url)\n\nAirport_nan = df['airport_name'].isna().sum()\nyear_nan = df['year'].isna().sum()\nmin_delay_nan = df['minutes_delayed_carrier'].isna().sum()\nmin_delay_nas_nan = df['minutes_delayed_nas'].isna().sum()\n\nyear_nan\n\n\nnp.int64(23)\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "Machine_Learning/project2.html#elevator-pitch",
    "href": "Machine_Learning/project2.html#elevator-pitch",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Using the data collected from the Bureau of Transportation Statistics I have determined what airports are the best to fly out of and when to fly in general to have the best chance of avoiding delays for several reasons altogether. Chicago O’Hare International Airport (ORD) was determined to be the worst airport based on the average length of delay (1.13 Hours). ORD also had the second highest proportion of delays due to weather out of all the airports I looked at. The highest proportion of delays due to weather was San Francisco International Airport (SFO). I also determined that September has the lowest proportion of total flights delayed. The holiday months (January, July, November, and December) have the highest likelihood of experiencing delays\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\"\ndf = pd.read_json(url)\n\nAirport_nan = df['airport_name'].isna().sum()\nyear_nan = df['year'].isna().sum()\nmin_delay_nan = df['minutes_delayed_carrier'].isna().sum()\nmin_delay_nas_nan = df['minutes_delayed_nas'].isna().sum()\n\nyear_nan\n\n\nnp.int64(23)\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "Machine_Learning/project2.html#questiontask-1",
    "href": "Machine_Learning/project2.html#questiontask-1",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”).\nI went through and made a new data frame for me to preform some cleaning. I then went through and searched for all the blanks, 1500+ minutes, and values that had -999 in them. then used the “.replace” to get them out and replace them with “NaN”.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n::: {#cell-Q1-Clean Data .cell execution_count=4}\n\nCleaned data\n# Include and execute your code here\ndf_clean = df\n\ndf_clean.replace([\"\", \"1500+\", -999], np.nan, inplace=True)\n\nexample = df_clean.iloc[2].to_json()\nexample\n\n\n'{\"airport_code\":\"IAD\",\"airport_name\":null,\"month\":\"January\",\"year\":2005.0,\"num_of_flights_total\":12381,\"num_of_delays_carrier\":\"414\",\"num_of_delays_late_aircraft\":1058.0,\"num_of_delays_nas\":895,\"num_of_delays_security\":4,\"num_of_delays_weather\":61,\"num_of_delays_total\":2430,\"minutes_delayed_carrier\":null,\"minutes_delayed_late_aircraft\":70919,\"minutes_delayed_nas\":35660.0,\"minutes_delayed_security\":208,\"minutes_delayed_weather\":4497,\"minutes_delayed_total\":134881}'\nCleaned Data\n\n:::",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "Machine_Learning/project2.html#questiontask-2",
    "href": "Machine_Learning/project2.html#questiontask-2",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays?\nI chose to look at the average time each flight is delayed determining the worst airport delays. In the table you will find the total delayed flights and total delayed minutes for each airport. The portion of delayed flights measure tells us what percentage of flights out of that airport are delayed. The last column displays the average hours each flight is delayed. This helped me determine that Chicago O’Hare International Airport (ORD) is the worst airport to fly out of.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nThe orst airport based on average delay hours\n# Include and execute your code here\n\ndf_clean_group = df_clean\n\ngrouped_data = df_clean_group.groupby('airport_code').agg(\n  total_flights=('num_of_flights_total', 'sum'),\n  total_delays=('num_of_delays_total', 'sum'),\n  total_delay_minutes=('minutes_delayed_total', 'sum')\n).reset_index()\n\ngrouped_data['proportion_delayed_flights'] = (grouped_data['total_delays'] / grouped_data['total_flights']) * 100\n\ngrouped_data['average_delay_hours'] = (grouped_data['total_delay_minutes'] / grouped_data['total_delays']) / 60\n\nworst_airport = grouped_data.loc[grouped_data['average_delay_hours'].idxmax()]\n\n\nworst_airport\n\n\nairport_code                        ORD\ntotal_flights                   3597588\ntotal_delays                     830825\ntotal_delay_minutes            56356129\nproportion_delayed_flights    23.093945\naverage_delay_hours            1.130525\nName: 3, dtype: object\nWorst Airport\n\n\n::: {#cell-Q2-Airport Tables .cell .tbl-cap-location-top tbl-cap=‘Airport Delays’ execution_count=7}\n\nAirport delays table\n# Include and execute your code here\ngrouped_data\n\n\n\n\n\n\n\n\n\nairport_code\ntotal_flights\ntotal_delays\ntotal_delay_minutes\nproportion_delayed_flights\naverage_delay_hours\n\n\n\n\n0\nATL\n4430047\n902443\n53983926\n20.370958\n0.996996\n\n\n1\nDEN\n2513974\n468519\n25173381\n18.636589\n0.895495\n\n\n2\nIAD\n851571\n168467\n10283478\n19.783083\n1.017358\n\n\n3\nORD\n3597588\n830825\n56356129\n23.093945\n1.130525\n\n\n4\nSAN\n917862\n175132\n8276248\n19.080428\n0.787620\n\n\n5\nSFO\n1630945\n425604\n26550493\n26.095546\n1.039718\n\n\n6\nSLC\n1403384\n205160\n10123371\n14.618950\n0.822396\n\n\n\n\n\n\n:::",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "Machine_Learning/project2.html#questiontask-3",
    "href": "Machine_Learning/project2.html#questiontask-3",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length?\nI chose to look at the proportion of delayed flights to determine what month is the best month to fly. I chose this as my metric because I feel this paints an accurate picture about what the chances are of your flight being delayed dependent on month. From what I found, the best month to fly is September. 16.5% of all the flights that occur in September end up being delayed.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nProportion of delayed flights by month\n# Include and execute your code here\ndf_mo = df_clean\n\ndf_mo['month'] = df_mo['month'].replace('Febuary', 'February')\n\ndf_clean_month = df_mo[df['month'].notna()]\n\nmonth_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n\ndf_clean_month['month'] = pd.Categorical(df_clean_month['month'], categories=month_order, ordered=True)\n\nmonthly_delays = df_clean_month.groupby('month').agg(\n  total_flights=('num_of_flights_total', 'sum'),\n  total_delays=('num_of_delays_total', 'sum'),\n  total_delay_minutes=('minutes_delayed_total', 'sum')\n).reset_index()\n\nmonthly_delays['proportion_delayed_flights'] = (monthly_delays['total_delays'] / monthly_delays['total_flights']) * 100\n\n(\n  ggplot(monthly_delays, aes(x='month', y='proportion_delayed_flights')) +\n  geom_bar(stat='identity') +\n  ggtitle('Proportion of Delayed Flights by Month') +\n  xlab('Month') +\n  ylab('Proportion of Delayed Flights (%)') +\n  theme(axis_text_x=element_text(angle=45, hjust=1))\n)\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nProportion of Delayed Flights by Month",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "Machine_Learning/project2.html#questiontask-4",
    "href": "Machine_Learning/project2.html#questiontask-4",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nYour job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild).\nI was able to create the new row and add it to the table. The column “weather_related_delays” will be used for the next question.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWeather delays\n# Include and execute your code here\n\ndf_clean_two = df_clean\n\ndf_clean_two['num_of_delays_late_aircraft'] = df_clean_two['num_of_delays_late_aircraft'].replace(-999, np.nan)\n\ndf_clean_two['num_of_delays_late_aircraft'].fillna(df['num_of_delays_late_aircraft'].mean(), inplace=True)\n\ndef weather_nas_delays(row):\n  if row['month'] in ['April', 'May', 'June', 'July', 'August']:\n    return 0.4 * round(row['num_of_delays_nas'], 2)\n  else:\n    return 0.65 * round(row['num_of_delays_nas'], 2)\n\ndf_clean_two['weather_related_delays'] = (\n  df_clean_two['num_of_delays_weather'] +\n  round(0.3 * df_clean['num_of_delays_late_aircraft'], 2) +\n  df_clean_two.apply(weather_nas_delays, axis=1)\n)\n\ndf_clean_two.head(5)\n\n\n\n\n\n\n\n\n\nairport_code\nairport_name\nmonth\nyear\nnum_of_flights_total\nnum_of_delays_carrier\nnum_of_delays_late_aircraft\nnum_of_delays_nas\nnum_of_delays_security\nnum_of_delays_weather\nnum_of_delays_total\nminutes_delayed_carrier\nminutes_delayed_late_aircraft\nminutes_delayed_nas\nminutes_delayed_security\nminutes_delayed_weather\nminutes_delayed_total\nweather_related_delays\n\n\n\n\n0\nATL\nAtlanta, GA: Hartsfield-Jackson Atlanta Intern...\nJanuary\n2005.0\n35048\nNaN\n1109.104072\n4598\n10\n448\n8355\n116423.0\n104415\n207467.0\n297\n36931\n465533\n3769.43\n\n\n1\nDEN\nDenver, CO: Denver International\nJanuary\n2005.0\n12687\n1041\n928.000000\n935\n11\n233\n3153\n53537.0\n70301\n36817.0\n363\n21779\n182797\n1119.15\n\n\n2\nIAD\nNaN\nJanuary\n2005.0\n12381\n414\n1058.000000\n895\n4\n61\n2430\nNaN\n70919\n35660.0\n208\n4497\n134881\n960.15\n\n\n3\nORD\nChicago, IL: Chicago O'Hare International\nJanuary\n2005.0\n28194\n1197\n2255.000000\n5415\n5\n306\n9178\n88691.0\n160811\n364382.0\n151\n24859\n638894\n4502.25\n\n\n4\nSAN\nSan Diego, CA: San Diego International\nJanuary\n2005.0\n7283\n572\n680.000000\n638\n7\n56\n1952\n27436.0\n38445\n21127.0\n218\n4326\n91552\n674.70\n\n\n\n\n\nWeather Delays",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "Machine_Learning/project2.html#questiontask-5",
    "href": "Machine_Learning/project2.html#questiontask-5",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Describe what you learn from this graph.\nAccording the the data San Francisco International Airport (SFO) experiences the highest proportion of delays due to bad weather days. Approximately 9.81% of the flights from SFO are delayed due to weather. Upon further research I found that San Francisco experiences frequent oceanic fog that causes delays and groundings for outbound flights. The next airport to avoid flying out of is Chicago O’Hare International Airport (ORD) with 8.5% of their flights become delayed.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWeather delays by airport\n# Include and execute your code here\ndf_clean_three = df_clean_two\n\ndf_clean_three['proportion_weather_delays'] = (df_clean_three['weather_related_delays'] / df_clean_three['num_of_flights_total']) * 100\n\nairport_weather_delays = df_clean_three.groupby('airport_code').agg(proportion_weather_delays=('proportion_weather_delays', 'mean')\n).reset_index()\n\n(\n  ggplot(airport_weather_delays, aes(x='airport_code', y='proportion_weather_delays')) +\n  geom_bar(stat='identity') +\n  ggtitle('Proportion of Weather Delays by Airport') +\n  xlab('Airport Code') +\n  ylab('Proportion of Weather Delays (%)') +\n  theme(axis_text_x=element_text(angle=45, hjust=1))\n)\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nProportion of Weather Delays by Airport",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "Full_Stack/project5.html",
    "href": "Full_Stack/project5.html",
    "title": "Client Report - The War with Star Wars",
    "section": "",
    "text": "After cleansing the Star Wars survey data for better handling. The cleaning consisted of shorting column names, handling missing values, and flited the raw data to extract the correct and useable survey responses. I then successfully recreated two of the visuals featured on the article provided to validate our cleaned data. The visual output and measurements were identical to those of the article. After this we then used the cleaned data to create a machine learning model to predict if someone makes over $50,000 a year based on their survey responses. Our final model was 65% accurate at predicting if a participant makes over $50k a year. To provide extra validation we then successfully recreated one more visual from the article.\n\n\nRead and format project data\nurl = 'https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv'\n\ndf_cols = pd.read_csv(url, encoding = \"ISO-8859-1\", nrows = 1).melt()\ndf = pd.read_csv(url, encoding = \"ISO-8859-1\", skiprows =2, header = None )",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "Full_Stack/project5.html#elevator-pitch",
    "href": "Full_Stack/project5.html#elevator-pitch",
    "title": "Client Report - The War with Star Wars",
    "section": "",
    "text": "After cleansing the Star Wars survey data for better handling. The cleaning consisted of shorting column names, handling missing values, and flited the raw data to extract the correct and useable survey responses. I then successfully recreated two of the visuals featured on the article provided to validate our cleaned data. The visual output and measurements were identical to those of the article. After this we then used the cleaned data to create a machine learning model to predict if someone makes over $50,000 a year based on their survey responses. Our final model was 65% accurate at predicting if a participant makes over $50k a year. To provide extra validation we then successfully recreated one more visual from the article.\n\n\nRead and format project data\nurl = 'https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv'\n\ndf_cols = pd.read_csv(url, encoding = \"ISO-8859-1\", nrows = 1).melt()\ndf = pd.read_csv(url, encoding = \"ISO-8859-1\", skiprows =2, header = None )",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "Full_Stack/project5.html#questiontask-1",
    "href": "Full_Stack/project5.html#questiontask-1",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nShorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.\nThe column names were long and very difficult to work with what we wanted to accomplish in this product. I cleaned up some of the data and shorted the column names to a more unified method. The new column names can be seen below.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\n\n\n\n\nTable of column names\n# Include and execute your code here\n\nvar_replace = {\n    'Which of the following Star Wars films have you seen\\\\? Please select all that apply\\\\.':'seen',\n    'Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.':'rank',\n    'Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.':'view',\n    'Do you consider yourself to be a fan of the Star Trek franchise\\\\?':'star_trek_fan',\n    'Do you consider yourself to be a fan of the Expanded Universe\\\\?\\x8cæ':'expanded_fan',\n    'Are you familiar with the Expanded Universe\\\\?':'know_expanded',\n    'Have you seen any of the 6 films in the Star Wars franchise\\\\?':'seen_any',\n    'Do you consider yourself to be a fan of the Star Wars film franchise\\\\?':'star_wars_fans',\n    'Which character shot first\\\\?':'shot_first',\n    'Unnamed: \\d{1,2}':np.nan,\n    ' ':'_',\n}\n\nval_replace = {\n    'Response':'',\n    'Star Wars: Episode ':'',\n    ' ':'_'\n}\n\ndf_cols_new = (df_cols\n    .assign(\n        val_replace = lambda x:  x.value.str.strip().replace(val_replace, regex=True),\n        var_replace = lambda x: x.variable.str.strip().replace(var_replace, regex=True)\n    )\n    .fillna(method = 'ffill')\n    .fillna(value = \"\")\n    .assign(column_names = lambda x: x.var_replace.str.cat(x.val_replace, sep = \"_\").str.strip('__').str.lower())\n    )\n\ndf.columns = df_cols_new.column_names.to_list()\n\ndf.columns\n\n\nIndex(['respondentid', 'seen_any', 'star_wars_fans',\n       'seen_i__the_phantom_menace', 'seen_ii__attack_of_the_clones',\n       'seen_iii__revenge_of_the_sith', 'seen_iv__a_new_hope',\n       'seen_v_the_empire_strikes_back', 'seen_vi_return_of_the_jedi',\n       'rank_i__the_phantom_menace', 'rank_ii__attack_of_the_clones',\n       'rank_iii__revenge_of_the_sith', 'rank_iv__a_new_hope',\n       'rank_v_the_empire_strikes_back', 'rank_vi_return_of_the_jedi',\n       'view_han_solo', 'view_luke_skywalker', 'view_princess_leia_organa',\n       'view_anakin_skywalker', 'view_obi_wan_kenobi',\n       'view_emperor_palpatine', 'view_darth_vader', 'view_lando_calrissian',\n       'view_boba_fett', 'view_c-3p0', 'view_r2_d2', 'view_jar_jar_binks',\n       'view_padme_amidala', 'view_yoda', 'shot_first', 'know_expanded',\n       'expanded_fan', 'star_trek_fan', 'gender', 'age', 'household_income',\n       'education', 'location_(census_region)'],\n      dtype='object')\nColumn Names\n\n\nBelow is an example of what the column names were before and what I changed them to. The “variable” column represents what the original column name was. The “value” column original values in response to the question for the column. The “val_replace” and “var_replace” columns represent the cleaned values and variables used to replace the older, harder to use data and names. The last column “column_names” represents the cleaned and shortened column names we used for easier computer handling.\n\n\nTable of column names\n# Include and execute your code here\ndf_cols_new.head()\n\n\n\n\n\n\nColumn Names\n\n\n\nvariable\nvalue\nval_replace\nvar_replace\ncolumn_names\n\n\n\n\n0\nRespondentID\n\n\nRespondentID\nrespondentid\n\n\n1\nHave you seen any of the 6 films in the Star W...\nResponse\n\nseen_any\nseen_any\n\n\n2\nDo you consider yourself to be a fan of the St...\nResponse\n\nstar_wars_fans\nstar_wars_fans\n\n\n3\nWhich of the following Star Wars films have yo...\nStar Wars: Episode I The Phantom Menace\nI__The_Phantom_Menace\nseen\nseen_i__the_phantom_menace\n\n\n4\nUnnamed: 4\nStar Wars: Episode II Attack of the Clones\nII__Attack_of_the_Clones\nseen\nseen_ii__attack_of_the_clones",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "Full_Stack/project5.html#questiontask-2",
    "href": "Full_Stack/project5.html#questiontask-2",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nClean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made.\nIn order to properly work with machine learning, we have to do some more data cleaning. The following output and data represents the steps I tool to clean the data.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\n\n\nA. Filter the dataset to respondents that have seen at least one film. I also altered this code to filter out any responses that said they had seen any Star Wars movie but did not mark what movies they had seen.\n\n\nFilter data\n# Include and execute your code here\n\nseen_columns = [\n    'seen_i__the_phantom_menace',\n    'seen_ii__attack_of_the_clones',\n    'seen_iii__revenge_of_the_sith',\n    'seen_iv__a_new_hope',\n    'seen_v_the_empire_strikes_back',\n    'seen_vi_return_of_the_jedi',\n]\n\ndf_filtered = df[df['seen_any'] == 'Yes']\n\nfor col in seen_columns:\n    df_filtered[col] = df_filtered[col].notna()\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nage\nhousehold_income\neducation\nlocation_(census_region)\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\n18-29\nNaN\nHigh school degree\nSouth Atlantic\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n18-29\n$0 - $24,999\nHigh school degree\nWest North Central\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n18-29\n$25,000 - $49,999\nBachelor degree\nMiddle Atlantic\n\n\n\n\n5 rows × 38 columns\n\nSeen Any Star Wars Movie\n\n\nB. Create a new column that converts the age ranges to a single number. Drop the age range categorical column\n\n\nChange Age Field\n# Include and execute your code here\n\nage_map = {\n    '18-29': 24,\n    '30-44': 37,\n    '45-60': 52,\n    '&gt;60': 65\n}\n\ndf_filtered['age_num'] = df_filtered['age'].map(age_map)\ndf_filtered.drop('age', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\nAge Field\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nhousehold_income\neducation\nlocation_(census_region)\nage_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nNaN\nHigh school degree\nSouth Atlantic\n24.0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n$0 - $24,999\nHigh school degree\nWest North Central\n24.0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n24.0\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n24.0\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n$25,000 - $49,999\nBachelor degree\nMiddle Atlantic\n24.0\n\n\n\n\n5 rows × 38 columns\n\n\n\nC. Create a new column that converts the education groupings to a single number. Drop the school categorical column\n\n\nFilter data\n# Include and execute your code here\n\neducation_map = {\n  'Less than high school degree': 9,\n  'High school degree': 12,\n  'Some college or Associate degree': 14,\n  'Bachelor degree': 16,\n  'Graduate degree': 20\n}\n\ndf_filtered['education_num'] = df_filtered['education'].map(education_map)\ndf_filtered.drop('education', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nhousehold_income\nlocation_(census_region)\nage_num\neducation_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nNaN\nSouth Atlantic\n24.0\n12.0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n$0 - $24,999\nWest North Central\n24.0\n12.0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n$100,000 - $149,999\nWest North Central\n24.0\n14.0\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n$100,000 - $149,999\nWest North Central\n24.0\n14.0\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n$25,000 - $49,999\nMiddle Atlantic\n24.0\n16.0\n\n\n\n\n5 rows × 38 columns\n\nEducation Field\n\n\nD. Create a new column that converts the income ranges to a single number. Drop the income range categorical column\n\n\nFilter data\n# Include and execute your code here\n\ndf_filtered['household_income'] = df_filtered['household_income'].fillna('unknown')\n\nincome_map = {\n  '$0 - $24,999': 12500,\n  '$25,000 - $49,999': 37500,\n  '$50,000 - $99,999': 75000,\n  '$100,000 - $149,999': 125000,\n  '$150,000+': 150000,\n  'unknown': 0\n}\n\ndf_filtered['income_num'] = df_filtered['household_income'].map(income_map)\ndf_filtered.drop('household_income', axis=1, inplace = True)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nview_yoda\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nlocation_(census_region)\nage_num\neducation_num\nincome_num\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\nSouth Atlantic\n24.0\n12.0\n0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\nWest North Central\n24.0\n12.0\n12500\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\nWest North Central\n24.0\n14.0\n125000\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\nWest North Central\n24.0\n14.0\n125000\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\nMiddle Atlantic\n24.0\n16.0\n37500\n\n\n\n\n5 rows × 38 columns\n\nIncome Field\n\n\nE. Create your target (also known as “y” or “label”) column based on the new income range column\n\n\nFilter data\n# Include and execute your code here\n\ndf_filtered['income_over_50k'] = (df_filtered['income_num'] &gt; 50000).astype(int)\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_any\nstar_wars_fans\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\n...\nshot_first\nknow_expanded\nexpanded_fan\nstar_trek_fan\ngender\nlocation_(census_region)\nage_num\neducation_num\nincome_num\nincome_over_50k\n\n\n\n\n0\n3292879998\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n...\nI don't understand this question\nYes\nNo\nNo\nMale\nSouth Atlantic\n24.0\n12.0\n0\n0\n\n\n2\n3292765271\nYes\nNo\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n...\nI don't understand this question\nNo\nNaN\nNo\nMale\nWest North Central\n24.0\n12.0\n12500\n0\n\n\n3\n3292763116\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nI don't understand this question\nNo\nNaN\nYes\nMale\nWest North Central\n24.0\n14.0\n125000\n1\n\n\n4\n3292731220\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n...\nGreedo\nYes\nNo\nNo\nMale\nWest North Central\n24.0\n14.0\n125000\n1\n\n\n5\n3292719380\nYes\nYes\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n...\nHan\nYes\nNo\nYes\nMale\nMiddle Atlantic\n24.0\n16.0\n37500\n0\n\n\n\n\n5 rows × 39 columns\n\nIncome Over 50k\n\n\nF. One-hot encode all remaining categorical columns\n\n\nFilter data\n# Include and execute your code here\n\ndf_encoded = pd.get_dummies(df_filtered, drop_first=True)\n\ndf_encoded.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nseen_i__the_phantom_menace\nseen_ii__attack_of_the_clones\nseen_iii__revenge_of_the_sith\nseen_iv__a_new_hope\nseen_v_the_empire_strikes_back\nseen_vi_return_of_the_jedi\nrank_i__the_phantom_menace\nrank_ii__attack_of_the_clones\nrank_iii__revenge_of_the_sith\n...\nstar_trek_fan_Yes\ngender_Male\nlocation_(census_region)_East South Central\nlocation_(census_region)_Middle Atlantic\nlocation_(census_region)_Mountain\nlocation_(census_region)_New England\nlocation_(census_region)_Pacific\nlocation_(census_region)_South Atlantic\nlocation_(census_region)_West North Central\nlocation_(census_region)_West South Central\n\n\n\n\n0\n3292879998\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n3.0\n2.0\n1.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2\n3292765271\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n1.0\n2.0\n3.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n3292763116\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n6.0\n1.0\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\n3292731220\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n5.0\n4.0\n6.0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n5\n3292719380\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n1.0\n4.0\n3.0\n...\nTrue\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 102 columns\n\nOne-hot encoding",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "Full_Stack/project5.html#questiontask-3",
    "href": "Full_Stack/project5.html#questiontask-3",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nValidate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article\nI recreated the following visuals from the article to validate our data clensing: 1. “Which ‘Star Wars’ Movies Have You Seen?” and 2. “Who Shot First?”\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWhich Star Wars Movies Have You Seen?\n# Include and execute your code here\n\nseen_columns = [\n    'seen_i__the_phantom_menace',\n    'seen_ii__attack_of_the_clones',\n    'seen_iii__revenge_of_the_sith',\n    'seen_iv__a_new_hope',\n    'seen_v_the_empire_strikes_back',\n    'seen_vi_return_of_the_jedi',\n]\n\nmovie_titles = [\n    \"The Phantom Menace\",\n    \"Attack of the Clones\",\n    \"Revenge of the Sith\",\n    \"A New Hope\",\n    \"The Empire Strikes Back\",\n    \"Return of the Jedi\",\n]\n\ngood_resp = df_filtered[seen_columns].sum(axis=1) &gt; 0\n\ntotal_seen_any = good_resp.sum()\nprob_seen = [(df_filtered[col].sum() / total_seen_any) for col in seen_columns]\n\n\ndf_seen = pd.DataFrame({\n  'movie': movie_titles,\n  'probability': prob_seen\n})\n\np = (ggplot(df_seen, aes(x='movie', y='probability')) + geom_bar(stat='identity', fill='blue') + labs(title='Which Star Wars Movies Have You Seen?', subtitle='Percentage of respondents who have seen each film', x='Percentage of Respondents', y='Star Wars Movies') + theme(axis_text_x=element_text(angle=45, hjust=1)))\n\np.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nWhich Star Wars Movies Have You Seen?\n\n\n\n\nWho Shot First?\n# Include and execute your code here\n\nshot_first_counts = df_filtered['shot_first'].value_counts()\n\nshot_first_percent = (shot_first_counts / shot_first_counts.sum()) * 100\n\ndf_shot_first = pd.DataFrame({\n  'response': shot_first_percent.index,\n  'percentage': shot_first_percent.values\n})\n\ndf_shot_first['response'] = df_shot_first['response'].replace({\"I don't understand this question\": \"Don't Understand\"})\ndf_shot_first['response'] = pd.Categorical(df_shot_first['response'], categories=['Don\\'t Understand', 'Greedo', 'Han'], ordered=True)\ndf_shot_first['percent_label'] = df_shot_first['percentage'].round(0).astype(int).astype(str) + '%'\n\n\np2 = (ggplot(df_shot_first, aes(x='response', y='percentage')) + geom_bar(stat='identity', fill='blue') + geom_text(aes(label='percent_label'), nudge_y=2, size=10) + labs(title='Who Shot First?', subtitle='According to 834 respondents', x='Response', y='Percentage') + coord_flip() + theme(axis_text_x=element_text(hjust=1)))\n\np2.show()",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "Full_Stack/project5.html#questiontask-4",
    "href": "Full_Stack/project5.html#questiontask-4",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nBuild a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.\nFor my model, I chose to use all the columns in the DataFrame except any of the columns that would directly give up the answer (e.g. household_income, income_num, location, and income_over_50k). After I filtered out any undesirable data, I split the data into training and testing sets, and trained the model to make predictions on if a person makes more or less than $50k a year. I then ran a accuracy score and classification report to display how successful my model was. The accuracy of my model 0.655 using the “RandomForestClassifier”. This means that our model can predict if a person makes over or under $50k with a 67% accuracy.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n\n\n\nRandom Forest Model\n# Include and execute your code here\n\nX_pred = df_encoded.drop(columns=df_encoded.filter(regex=(\"(household_income|income_num|income_over_50k|location_\\(census_region\\)_East South Central|location_\\(census_region\\)_Middle Atlantic|location_\\(census_region\\)_Mountain|location_\\(census_region\\)_New England|location_\\(census_region\\)_Pacific|location_\\(census_region\\)_South Atlantic|location_\\(census_region\\)_West North Central|location_\\(census_region\\)_West South Central|respondentid)\")).columns)\ny_pred = df_encoded['income_over_50k']\n\nX_train, X_test, y_train, y_test = train_test_split(X_pred, y_pred, test_size=0.35, random_state=76)\n\nrf = RandomForestClassifier(n_estimators=300 ,random_state=76, max_depth=20, min_samples_split=2)\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\nprint('Accuracy:', accuracy_score(y_test, y_pred))\nprint('\\nClassification Report:\\n', classification_report(y_test, y_pred, target_names=['Under 50k', 'Over 50k']))\n\n\nAccuracy: 0.6432926829268293\n\nClassification Report:\n               precision    recall  f1-score   support\n\n   Under 50k       0.69      0.64      0.67       183\n    Over 50k       0.59      0.64      0.61       145\n\n    accuracy                           0.64       328\n   macro avg       0.64      0.64      0.64       328\nweighted avg       0.65      0.64      0.64       328",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "Full_Stack/project5.html#stretchtask-5",
    "href": "Full_Stack/project5.html#stretchtask-5",
    "title": "Client Report - The War with Star Wars",
    "section": "STRETCH|TASK 5",
    "text": "STRETCH|TASK 5\nValidate the data provided on GitHub lines up with the article by recreating a 3rd visual from the article.\nFor this stretch challenge we were asked to recreate one more visual from the article to double check the validity of our data. I chose to recreate the “What’s the Best ‘Star Wars’ Movie?” In this visual we first had to filter the data further to those who have seen all the Star Wars movies. We then had to calculate and display the share of respondents who rated each fil as their favorite.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWhat is the best Star Wars movie?\n# Include and execute your code here\n\nrank_columns = [\n    'rank_i__the_phantom_menace',\n    'rank_ii__attack_of_the_clones',\n    'rank_iii__revenge_of_the_sith',\n    'rank_iv__a_new_hope',\n    'rank_v_the_empire_strikes_back',\n    'rank_vi_return_of_the_jedi',\n]\n\nmovie_titles = [\n    \"The Phantom Menace\",\n    \"Attack of the Clones\",\n    \"Revenge of the Sith\",\n    \"A New Hope\",\n    \"The Empire Strikes Back\",\n    \"Return of the Jedi\",\n]\n\nseen_all = df_filtered[seen_columns].all(axis=1)\ndf_seen_all = df_filtered[seen_all]\n\nfav_counts = [df_seen_all[col].value_counts().get(1, 0) for col in rank_columns]\n\ntotal_resp = df_seen_all.shape[0]\npercent_fav = [(count / total_resp) * 100 for count in fav_counts]\n\ndf_fav = pd.DataFrame({\n  'movie': movie_titles,\n  'percentage': percent_fav\n})\ndf_fav['percent_label'] = df_fav['percentage'].round(0).astype(int).astype(str) + '%'\ndf_fav['movie'] = pd.Categorical(df_fav['movie'], categories=movie_titles[::-1], ordered=True)\n\np5 = (ggplot(df_fav, aes(x='movie', y='percentage')) + geom_bar(stat='identity', fill='blue') + geom_text(aes(label='percent_label'), nudge_y=2, size=10) + labs(title='Favorite Star Wars Movies', subtitle='Of respondents who have seen all six films', x='Percentage of Respondents', y='Star Wars Movies') + coord_flip() + theme(axis_text_x=element_text(hjust=1)))\n\np5.show()",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "Full_Stack/project4.html",
    "href": "Full_Stack/project4.html",
    "title": "Client Report - Can You Predict That?",
    "section": "",
    "text": "Using the power of advanced machine learning, we developed a model that successfully predicted whether a home was built before or after 1980. Through model tuning we were able to get the accuracy of our model’s prediction at the 90% threshold making our model relatively reliable in predicting the year a home was built. We first Identified possible relationships between homes built before and after 1980. Our model found that one story architecture, garage type, and quality ratings of C were most correlated with homes from built before 1980. Our model was then explained and justified using a classification report, confusion matrix and an ROC curve confirming that our model was accurate and reliable. I also built a model to predict the year a home was built using the “GradientBoostingRegressor.” This model was able to successfully predict 89% of its results. For Justification we look at Mean Absolute Error (MAE), Mean Squared Error (MSE), R-Squared value, and a confusion matrix. \n\n\nRead and format project data\n# Include and execute your code here\n\ndwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project4.html#elevator-pitch",
    "href": "Full_Stack/project4.html#elevator-pitch",
    "title": "Client Report - Can You Predict That?",
    "section": "",
    "text": "Using the power of advanced machine learning, we developed a model that successfully predicted whether a home was built before or after 1980. Through model tuning we were able to get the accuracy of our model’s prediction at the 90% threshold making our model relatively reliable in predicting the year a home was built. We first Identified possible relationships between homes built before and after 1980. Our model found that one story architecture, garage type, and quality ratings of C were most correlated with homes from built before 1980. Our model was then explained and justified using a classification report, confusion matrix and an ROC curve confirming that our model was accurate and reliable. I also built a model to predict the year a home was built using the “GradientBoostingRegressor.” This model was able to successfully predict 89% of its results. For Justification we look at Mean Absolute Error (MAE), Mean Squared Error (MSE), R-Squared value, and a confusion matrix. \n\n\nRead and format project data\n# Include and execute your code here\n\ndwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project4.html#questiontask-1",
    "href": "Full_Stack/project4.html#questiontask-1",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np \nfrom lets_plot import * \nLetsPlot.setup_html(isolated_frame=True)\n\ndf_subset = dwellings_ml.filter(\n    ['livearea', 'finbsmnt', 'basement', \n    'yearbuilt', 'nocars', 'numbdrm', 'numbaths', 'before1980',\n    'stories', 'yrbuilt']).sample(500)\n\ndf_subset = df_subset.dropna(subset=['livearea', 'finbsmnt', 'before1980', 'basement', 'numbaths', 'numbdrm'])\n\ndf_subset['before1980'] = df_subset['before1980'].astype(bool)\n\n\nThis scatter plot shows us the possible relationship between the number of bedrooms built in the house and the total living space, or square footage, of the home. There seems to be a linear relationship between house size and number of rooms built. As the number of rooms increase, so does the total livable square footage.\n\n\nLiving Area vs Number of Bedrooms\n# Include and execute your code here\nc_1 = ggplot(df_subset, aes(x='livearea', y='numbdrm', color='before1980')) + geom_point(alpha=0.5) + ggtitle(\"Living Area vs Number of Bedrooms\") + xlab(\"Living Area\") + ylab(\"Number of Bedrooms\") + scale_color_manual(name='Built Before 1980', labels=['Yes (Blue)', 'No (Red)'], values=['blue', 'red']) + theme(legend_position='right') \n\nc_1.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nLiving Area vs Number of Bedrooms\n\n\nThis box and whisker plot shows is the relationship between basement sizes on homes built before 1980 and after 1980. From this we can see that homes that were built before 1980 had slightly larger mean basement size than homes built after 1980.\n\n\nFinished Basement Area by Before1980\n# Include and execute your code here\n\nc_2 = ggplot(df_subset, aes(x='before1980', y='finbsmnt', fill='before1980')) + geom_boxplot() + ggtitle('Finished Basement Area by Before1980') + xlab('Built Before 1980') + ylab('Finished Basement Area') + scale_fill_discrete(name=\"Built Before 1980\")\nc_2.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThis stacked bar chart tells us the relationship between the number of bathrooms built in a house based on if the house was built before or after 1980. It looks like the majority of houses built before 1980 had two bathrooms. \n\n\nNumber of Bathrooms by Before1980\n# Include and execute your code here\n\nc_3 = ggplot(df_subset, aes(x='numbaths', fill='before1980')) + geom_bar(position='stack') + ggtitle('Number of Bathrooms by Before1980') + xlab('Number of Bathrooms') + ylab('Count') + scale_fill_discrete(name=\"Built Before 1980\")\nc_3.show()",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project4.html#questiontask-2",
    "href": "Full_Stack/project4.html#questiontask-2",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\nI ended up using the DecisionTreeClassifier to create my classification model. I settled on this because this led to me getting the highest accuracy in the quickest time. I tried to use the Gradient Boosting, but it ended up taking longer to get the results. I also tried to add the following parameters: max_depth, min_sample_split, and min_samples_leaf. I found that this took a lot longer to get the results and it did not improve the accuracy rating at all when compared to the DecisionTreeClassifier with no parameters. I was able to achieve a 90% accuracy rating.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\n\n\n\n\nClassification Report\nX = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny = dwellings_ml.filter(regex = \"before1980\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .35, random_state = 76)\n\nmodel_1 = tree.DecisionTreeClassifier(random_state=76, max_depth=10, min_samples_split=10, min_samples_leaf=2)\nmodel_1.fit(X_train, y_train)\n\ny_pred = model_1.predict(X_test)\n\nprint('\\nTest Accuracy:', metrics.accuracy_score(y_test, y_pred))\nprint('\\nClassification Report: \\n', metrics.classification_report(y_test, y_pred))\n\n\n\nTest Accuracy: 0.9007481296758105\n\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       0.86      0.87      0.87      2980\n           1       0.92      0.92      0.92      5040\n\n    accuracy                           0.90      8020\n   macro avg       0.89      0.89      0.89      8020\nweighted avg       0.90      0.90      0.90      8020",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project4.html#questiontask-3",
    "href": "Full_Stack/project4.html#questiontask-3",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a feature importance chart and a description of the features.\nThe most important features identified in my model were “arcstyle_ONE_STORY”, “gartype_Att”, and “quality_C.” The model puts the most importance on how many stories the house is using “arcstyle_ONE_STORY.” One story homes were more popular before 1980 so there were more built of these types of homes. The second most important feature my model used was whether or not the house had an attached, or detached garage, “gartype_Att.” Most of the houses built before 1980 had detached garages. The third greatest importance the model used was the quality of the home from “quality_C.” Homes with this quality rating were more likely to be older homes. The model put these three fields together to best predict if the house was built before or after 1980.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nFeature Importance Chart\n# Include and execute your code here\nimportance = model_1.feature_importances_\nfeature_names = X.columns\n\nfeature_importance_df = pd.DataFrame({'feature': feature_names, 'Importance': importance})\nfeature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n\nfeature_importance_df_top = feature_importance_df.head(10)\n\nc_4 = ggplot(feature_importance_df_top, aes(x='feature', y='Importance', fill= 'Importance')) + geom_bar(stat='identity') + theme(axis_text_x=element_text(angle=90, hjust=1)) + ggtitle('Feature Importance') + xlab('Feature') + ylab('Importance')\nc_4.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nFeature Importance",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project4.html#questiontask-4",
    "href": "Full_Stack/project4.html#questiontask-4",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\nTo justify the results and quality of my model, I chose to look at the following three metrics: 1 - Classification report containing the accuracy, 2 - Confusion matrix, and 3 - The Receiver-operating characteristic (ROC) curve. Based on the results of this test, I can conclude that the quality of my model is strong and can be relied upon with an acceptable accuracy and true positive rate. Under each of the reports I will explain the significance and how to interpret the results.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nPerformance Metrics\n# Include and execute your code here\nprint('\\nTest Accuracy:', metrics.accuracy_score(y_test, y_pred))\nprint('\\nClassification Report: \\n', metrics.classification_report(y_test, y_pred))\n\n\n\nTest Accuracy: 0.9007481296758105\n\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       0.86      0.87      0.87      2980\n           1       0.92      0.92      0.92      5040\n\n    accuracy                           0.90      8020\n   macro avg       0.89      0.89      0.89      8020\nweighted avg       0.90      0.90      0.90      8020\n\n\n\nThe classification report and accuracy measure the percentage of correctly classified predictions compared to the total results. The higher the accuracy percentage is the more accurate the model is. A high accuracy rating is anything greater than or equal to 90% (0.90). The precision measures the percentage of total correct guesses divided by the number of true positives + false positives. For “0” this means that 86% of the home predicted to be built after 1980 are correct, and for the “1” 96% of the homes predicted to be built before 1980 are correct. Recall measures the total true positives divided by true positives + false negatives. For “0”, the model correctly identified 87% of the homes built after 1980 and for “1” correctly identified 92% of the homes built before 1980. The F-1 Score is calculated as 2 * (Precision * Recall / Precision + Recall). The F-1 score provides a balanced metric to look at our model performance. Our model is slightly stronger at predicting if a home was built before 1980 than after 1980 but overall reliable at predicting if it was built before or after 1980.\n\n\nConfusion Matrix\n# Include and execute your code here\n\nconf_matrix = metrics.confusion_matrix(y_test, y_pred)\nconf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted: (After 1980)', 'Predicted: (Before 1980)'], index=['Actual: (After 1980)', 'Actual: (Before 1980)'])\n\nconf_matrix_plot = ggplot(conf_matrix_df.reset_index().melt(id_vars='index'), aes(x='index', y='variable', fill='value')) + geom_tile() + geom_text(aes(label='value'), color='black', size=6) + scale_fill_brewer(palette='RdYlBu') + ggtitle('Confusion Matrix') + xlab('Predicted') + ylab('Actual')\n\nconf_matrix_plot.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThe confusion matrix is a table that is used to describe the performance of a classification model. The matrix shows the number of true positives (4630), false positives (386), true negatives (2594), and false negatives (410). From the confusion matrix we can see that our model had relatively few false positives / negatives in relation to the correct predictions. This proves how well my model was at predicting if a home was built before or after 1980.\n\n\nROC Curve\n# Include and execute your code here\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\n\nroc_df = pd.DataFrame({'FPR': fpr, 'TPR': tpr, 'Thresholds': thresholds})\n\nroc_chart = ggplot(roc_df, aes(x='FPR', y='TPR')) + geom_line(color='blue') + ggtitle('ROC Curve') + xlab('False Positive Rate') + ylab('True Positive Rate')\n\nroc_chart.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThe ROC curve is a graphical representation of the true positive rate against the false positive rate. The closer the curve is to the top left corner, the better the model is. The area under the curve (AUC) is used to measure the performance of the model. The higher the AUC, the better the model is. Our model had an AUC of 0.92 which shows us it performed very well at predicting when a house was built.",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project4.html#stretchtask-5",
    "href": "Full_Stack/project4.html#stretchtask-5",
    "title": "Client Report - Can You Predict That?",
    "section": "STRETCH|TASK 5",
    "text": "STRETCH|TASK 5\nCan you build a model that predicts the year a house was built? Explain the model and the evaluation metrics you would use to determine if the model is good.\nFor the model I chose to use the “GradientBoostingRegressor” because of its ability to sift through complex, non-linear relationships to get to the target prediction. In this case our goal was to predict what year a house was built which requires the model to look through the data and attempt to find patterns that could help predict when a house was built. The “GradientBoostingRegressor” is also very good at not over filtering the data like other models. This model type also very flexible and forgiving when it comes to tuning the model. Below are the figures and print outs I used to evaluate the accuracy of my model.\n\n\nRead and format data\n# Include and execute your code here\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nRead and format data\n# Include and execute your code here\n\nX_5 = dwellings_ml.drop(dwellings_ml.filter(regex = 'yrbuilt|parcel').columns, axis = 1)\ny_5 = dwellings_ml[\"yrbuilt\"]\n\nX_5 = X_5.dropna()\ny_5 = y_5[X_5.index]\n\nX_train, X_test, y5_train, y5_test = train_test_split(X_5, y_5, test_size = .35, random_state = 76)\n\nrf_model = GradientBoostingRegressor(random_state=76, n_estimators=500, learning_rate=0.1, max_depth=15, min_samples_split=10, min_samples_leaf=2)\nrf_model.fit(X_train, y5_train)\n\ny5_pred = rf_model.predict(X_test)\n\nmae = metrics.mean_absolute_error(y5_test, y5_pred)\nmse = metrics.mean_squared_error(y5_test, y5_pred)\nr2 = metrics.r2_score(y5_test, y5_pred)\n\nprint(f'Mean Absolute Error (MAE): {mae:.2f}')\nprint(f'Mean Squared Error (MSE): {mse:.2f}')\nprint(f'R-Squared (R^2): {r2:.2f}')\n\n\nMean Absolute Error (MAE): 7.49\nMean Squared Error (MSE): 155.45\nR-Squared (R^2): 0.89\n\n\n\n\nClassification Report\n# Include and execute your code here\n\nbins = [0, 1980, 1995, 2010, 2023]\nlables = ['Pre 1980', '1980-1995', '1996-2010', 'Post 2010']\n\ny5_test_bins = pd.cut(y5_test, bins=bins, labels=lables)\ny5_pred_bins = pd.cut(y5_pred, bins=bins, labels=lables)\n\nprint('\\nTest Accuracy:', metrics.accuracy_score(y5_test_bins, y5_pred_bins))\nprint('\\nClassification Report: \\n', metrics.classification_report(y5_test_bins, y5_pred_bins))\n\n\n\nTest Accuracy: 0.9337905236907731\n\nClassification Report: \n               precision    recall  f1-score   support\n\n   1980-1995       0.67      0.68      0.68       418\n   1996-2010       0.84      0.91      0.87      1832\n   Post 2010       0.88      0.73      0.80       671\n    Pre 1980       1.00      0.99      0.99      5099\n\n    accuracy                           0.93      8020\n   macro avg       0.85      0.83      0.84      8020\nweighted avg       0.94      0.93      0.93      8020\n\n\n\nThe Mean Absolute Error (MAE) tells us that on average, our model’s predictions are off by only 7.5 years. With regression tasks like this a MAE of 10 years is typically considered very good. The Mean Squared Error (MSE) is similar to the MAE, but the MSE penalizes the model’s larger errors heavier than smaller errors. Our score of 156.62 equates to about 12.5 years of difference with a range of years that spans over 100 years. When we compare our R-Squared result with the Accuracy test below we can see that overall, our model is right within the acceptable margin of error.\n\n\nConfusion Matrix\n# Include and execute your code here\n\nconf5_matrix = metrics.confusion_matrix(y5_test_bins, y5_pred_bins, labels=lables)\nconf5_matrix_df = pd.DataFrame(conf5_matrix, columns=['Pred:(Pre 80)', 'Pred:(80-95)', 'Pred:(96-10)', 'Pred:(Post 10)'], index=['Actl:(Pre 80)', 'Actl:(80-95)', 'Actl:(96-10)', 'Actl:(Post 10)'])\n\nconf5_matrix_norm = conf5_matrix.astype('float') / conf5_matrix.sum(axis=1)[:, np.newaxis]\n\nconf5_matrix_norm_df = pd.DataFrame(conf5_matrix_norm, columns=['Pred:(Pre 80)', 'Pred:(80-95)', 'Pred:(96-10)', 'Pred:(Post 10)'], index=['Actl:(Pre 80)', 'Actl:(80-95)', 'Actl:(96-10)', 'Actl:(Post 10)'])\n\nconf5_matrix_norm_df_melted = conf5_matrix_norm_df.reset_index().melt(id_vars='index', var_name='Predicted', value_name='Accuracy')\n\nlbl_plot = (\n  pd.DataFrame(100 * conf5_matrix_norm)\n  .stack()\n  .reset_index(drop=True)\n  .round(1)\n  .astype(str) + '%'\n)\n\nconf5_matrix_plot = ggplot(conf5_matrix_norm_df_melted, aes(x='Predicted', y='index', fill='Accuracy')) + geom_tile() + geom_text(aes(label=lbl_plot), color='white', size=6) + scale_fill_gradient(low='red', high='blue', name='Accuracy') + ggtitle('Confusion Matrix') + xlab('Predicted') + ylab('Actual') + theme(axis_text_x=element_text(angle=90, hjust=1))\n\nconf5_matrix_plot.show()\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nConfusion Matrix\n\n\nThe confusion matrix is a table that is used to describe the performance of a classification model. The matrix shows the number of true positives, false positives, true negatives, and false negatives. From the confusion matrix we can see that our model had relatively few false positives / negatives in relation to the correct predictions. This proves how well my model was at predicting when a house was built.",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project1.html",
    "href": "Full_Stack/project1.html",
    "title": "Client Report - What’s in a Name?",
    "section": "",
    "text": "What’s in a Name?\n\n\n\n\nElevator pitch\nIn comparing baby names and birth years looking for trends, I have learned that my name “Braxton” was not all that popular when I was born. If you were to get a call from a “Brittney” on the phone, it would be safe to assume that her age would be 34 years old. I also analyzed the effect of certain events on popularity of names.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\ndf \n\n\n\n\n\n\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n0\nAaden\n2005\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n1\nAaden\n2007\n0.0\n5.0\n0.0\n5.0\n20.0\n6.0\n0.0\n0.0\n...\n5.0\n14.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n98.0\n\n\n2\nAaden\n2008\n0.0\n15.0\n13.0\n20.0\n135.0\n10.0\n9.0\n0.0\n...\n15.0\n99.0\n8.0\n26.0\n0.0\n11.0\n19.0\n5.0\n0.0\n939.0\n\n\n3\nAaden\n2009\n0.0\n28.0\n20.0\n23.0\n158.0\n22.0\n12.0\n0.0\n...\n33.0\n140.0\n6.0\n17.0\n0.0\n31.0\n23.0\n14.0\n0.0\n1242.0\n\n\n4\nAaden\n2010\n0.0\n8.0\n6.0\n12.0\n62.0\n9.0\n5.0\n0.0\n...\n9.0\n56.0\n0.0\n11.0\n0.0\n7.0\n12.0\n0.0\n0.0\n414.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n393379\nZyon\n2011\n0.0\n0.0\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n...\n5.0\n9.0\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n117.0\n\n\n393380\nZyon\n2012\n0.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n7.5\n0.0\n12.0\n0.0\n0.0\n0.0\n0.0\n0.0\n118.5\n\n\n393381\nZyon\n2013\n0.0\n0.0\n0.0\n0.0\n6.0\n0.0\n0.0\n0.0\n...\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n93.0\n\n\n393382\nZyon\n2014\n0.0\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n...\n0.0\n7.0\n0.0\n9.0\n0.0\n0.0\n0.0\n0.0\n0.0\n73.0\n\n\n393383\nZyon\n2015\n0.0\n0.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n0.0\n10.0\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n87.5\n\n\n\n\n393384 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 1\nHow does your name at your birth year compare to its use historically?\nHistoricly it looks like my name was given after a slight rise in popularity. However, I was given my name before the largest spike after about 2002.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nPlot of the name Braxton\n# Include and execute your code here\nmy_name_df = df[df['name'] == 'Braxton']\nyear_2000_data = my_name_df[my_name_df['year'] == 2000]\n\n(ggplot(my_name_df, aes(x='year', y='Total')) +\n    geom_point(color = 'blue') +\n    ggtitle('The Use of Braxton') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5)) +\n    geom_text(aes(x=year_2000_data['year'], y=year_2000_data['Total']), label='782', nudge_x=-0.5, nudge_y=400) +\n    geom_segment(aes(x=year_2000_data['year'], xend=year_2000_data['year'], y=year_2000_data['Total'] - 10, yend=year_2000_data['Total']+350), arrow=arrow(length=5, type='closed'), color='red')\n)\n\n\n   \n   \nThe Use of Braxton\n\n\n\n\ntable of the name Braxton\ndisplay(my_name_df)\n\n\n\n\n\n\nBraxton’s Name Usage\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n50029\nBraxton\n1914\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n13.0\n\n\n50030\nBraxton\n1915\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n11.0\n\n\n50031\nBraxton\n1916\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12.0\n\n\n50032\nBraxton\n1917\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n50033\nBraxton\n1918\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n10.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50095\nBraxton\n2011\n6.0\n123.0\n84.0\n37.0\n62.0\n43.0\n0.0\n0.0\n...\n131.0\n188.0\n93.0\n40.0\n0.0\n33.0\n35.0\n68.0\n16.0\n2491.0\n\n\n50096\nBraxton\n2012\n5.0\n111.0\n79.0\n37.0\n61.0\n39.0\n5.0\n0.0\n...\n151.0\n227.0\n65.0\n58.0\n0.0\n41.0\n62.0\n95.0\n22.0\n2986.0\n\n\n50097\nBraxton\n2013\n0.0\n100.0\n84.0\n53.0\n92.0\n49.0\n7.0\n0.0\n...\n158.0\n226.0\n63.0\n64.0\n0.0\n42.0\n62.0\n74.0\n9.0\n3085.0\n\n\n50098\nBraxton\n2014\n10.0\n121.0\n86.0\n45.0\n106.0\n43.0\n6.0\n0.0\n...\n150.0\n147.0\n79.0\n60.0\n0.0\n39.0\n56.0\n74.0\n20.0\n3186.0\n\n\n50099\nBraxton\n2015\n0.0\n103.0\n76.0\n48.0\n102.0\n38.0\n6.0\n0.0\n...\n173.0\n278.0\n54.0\n61.0\n0.0\n57.0\n74.0\n67.0\n9.0\n3265.0\n\n\n\n\n71 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nIt appears that the most popular year for the use of the name “Brittney” was 1990. This means that we could make a educated guess that a person named Britteny would be 34 years old.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nBar chart of the name Brittany\nbrittany_df = df[df['name'] == 'Brittany']\n\n(ggplot(brittany_df, aes(x='year', y='Total')) +\n    geom_bar(stat='identity') +\n    ggtitle('The Popularity of Brittney Over Time') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nThe Popularity of Brittney Over Time\n\n\n\n\nTable of the name Brittany\n# Include and execute your code here\n\ndisplay(brittany_df)\n\n\n\n\n\n\nBrittany’s Name Usage\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n53205\nBrittany\n1968\n0.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n53206\nBrittany\n1969\n0.0\n0.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\n0.0\n5.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12.0\n\n\n53207\nBrittany\n1970\n0.0\n0.0\n0.0\n0.0\n5.0\n5.0\n0.0\n0.0\n...\n0.0\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n32.0\n\n\n53208\nBrittany\n1971\n0.0\n0.0\n0.0\n5.0\n17.0\n0.0\n0.0\n0.0\n...\n0.0\n14.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n81.0\n\n\n53209\nBrittany\n1972\n0.0\n0.0\n0.0\n0.0\n11.0\n10.0\n0.0\n0.0\n...\n8.0\n14.0\n16.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n158.0\n\n\n53210\nBrittany\n1973\n0.0\n0.0\n0.0\n6.0\n17.0\n0.0\n0.0\n0.0\n...\n0.0\n18.0\n13.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n166.0\n\n\n53211\nBrittany\n1974\n0.0\n0.0\n7.0\n0.0\n19.0\n0.0\n0.0\n0.0\n...\n0.0\n17.0\n28.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n198.0\n\n\n53212\nBrittany\n1975\n0.0\n0.0\n0.0\n0.0\n28.0\n0.0\n0.0\n0.0\n...\n10.0\n23.0\n28.0\n0.0\n0.0\n9.0\n5.0\n0.0\n0.0\n277.0\n\n\n53213\nBrittany\n1976\n0.0\n0.0\n6.0\n0.0\n36.0\n5.0\n0.0\n0.0\n...\n15.0\n35.0\n21.0\n7.0\n0.0\n7.0\n11.0\n0.0\n0.0\n304.0\n\n\n53214\nBrittany\n1977\n0.0\n9.0\n8.0\n5.0\n39.0\n10.0\n0.0\n0.0\n...\n16.0\n48.0\n55.0\n5.0\n0.0\n0.0\n7.0\n0.0\n0.0\n448.0\n\n\n53215\nBrittany\n1978\n0.0\n8.0\n12.0\n8.0\n50.0\n17.0\n0.0\n0.0\n...\n15.0\n48.0\n67.0\n7.0\n0.0\n11.0\n5.0\n0.0\n0.0\n592.0\n\n\n53216\nBrittany\n1979\n0.0\n6.0\n13.0\n0.0\n47.0\n19.0\n10.0\n0.0\n...\n20.0\n76.0\n63.0\n14.0\n0.0\n13.0\n10.0\n7.0\n8.0\n764.0\n\n\n53217\nBrittany\n1980\n0.0\n11.0\n20.0\n13.0\n121.0\n41.0\n5.0\n0.0\n...\n39.0\n142.0\n110.0\n18.0\n0.0\n36.0\n28.0\n10.0\n10.0\n1383.0\n\n\n53218\nBrittany\n1981\n11.0\n23.0\n32.0\n22.0\n127.0\n37.0\n14.0\n0.0\n...\n54.0\n166.0\n116.0\n21.0\n0.0\n47.0\n24.0\n10.0\n14.0\n1701.0\n\n\n53219\nBrittany\n1982\n12.0\n50.0\n53.0\n47.0\n290.0\n88.0\n18.0\n5.0\n...\n102.0\n300.0\n152.0\n50.0\n7.0\n73.0\n58.0\n25.0\n16.0\n3093.0\n\n\n53220\nBrittany\n1983\n20.0\n71.0\n80.0\n66.0\n406.0\n131.0\n26.0\n19.0\n...\n133.0\n415.0\n156.0\n83.0\n7.0\n95.0\n68.0\n42.0\n19.0\n4377.0\n\n\n53221\nBrittany\n1984\n34.0\n158.0\n111.0\n112.0\n698.0\n225.0\n59.0\n22.0\n...\n243.0\n735.0\n159.0\n148.0\n6.0\n134.0\n114.0\n78.0\n36.0\n7664.0\n\n\n53222\nBrittany\n1985\n53.0\n351.0\n224.0\n217.0\n715.5\n319.0\n118.0\n48.0\n...\n500.0\n1256.0\n182.0\n390.0\n25.0\n231.0\n260.0\n235.0\n40.0\n14010.0\n\n\n53223\nBrittany\n1986\n49.0\n241.5\n269.0\n309.0\n886.5\n389.0\n165.0\n79.0\n...\n586.0\n792.5\n183.0\n533.0\n45.0\n287.0\n361.0\n229.0\n48.0\n17856.5\n\n\n53224\nBrittany\n1987\n56.0\n528.0\n267.0\n322.0\n977.0\n365.0\n196.0\n97.0\n...\n633.0\n795.5\n193.0\n628.0\n34.0\n289.0\n351.0\n241.0\n42.0\n18825.5\n\n\n53225\nBrittany\n1988\n48.0\n585.0\n304.0\n383.0\n1214.5\n370.0\n271.0\n126.0\n...\n409.5\n925.0\n167.0\n884.0\n43.0\n368.0\n455.0\n270.0\n41.0\n21952.0\n\n\n53226\nBrittany\n1989\n53.0\n837.0\n457.0\n549.0\n1513.0\n439.0\n371.0\n86.5\n...\n537.5\n1266.0\n185.0\n1247.0\n71.0\n500.0\n724.0\n402.0\n50.0\n30848.0\n\n\n53227\nBrittany\n1990\n75.0\n788.0\n440.0\n483.0\n1372.5\n450.0\n345.0\n90.0\n...\n546.5\n1181.5\n173.0\n1229.0\n86.0\n470.0\n671.0\n379.0\n38.0\n32562.5\n\n\n53228\nBrittany\n1991\n55.0\n585.0\n327.0\n379.0\n1155.5\n361.0\n293.0\n132.0\n...\n872.0\n983.0\n140.0\n959.0\n47.0\n363.0\n556.0\n352.0\n43.0\n26963.5\n\n\n53229\nBrittany\n1992\n51.0\n526.0\n282.0\n292.0\n915.0\n325.0\n223.0\n87.0\n...\n726.0\n1785.0\n119.0\n854.0\n57.0\n307.0\n465.0\n292.0\n40.0\n23416.5\n\n\n53230\nBrittany\n1993\n37.0\n511.0\n229.0\n298.0\n1597.0\n275.0\n199.0\n62.0\n...\n637.0\n1547.0\n123.0\n697.0\n44.0\n286.0\n389.0\n242.0\n26.0\n21728.0\n\n\n53231\nBrittany\n1994\n47.0\n422.0\n237.0\n227.0\n666.5\n227.0\n168.0\n72.0\n...\n599.0\n1359.0\n91.0\n641.0\n43.0\n229.0\n351.0\n228.0\n32.0\n17808.5\n\n\n53232\nBrittany\n1995\n34.0\n342.0\n190.0\n202.0\n605.5\n179.0\n156.0\n49.0\n...\n516.0\n1293.0\n110.0\n537.0\n40.0\n216.0\n271.0\n194.0\n19.0\n15875.5\n\n\n53233\nBrittany\n1996\n29.0\n318.0\n163.0\n195.0\n977.0\n163.0\n133.0\n31.0\n...\n400.0\n1147.0\n107.0\n455.0\n25.0\n166.0\n239.0\n147.0\n18.0\n13796.0\n\n\n53234\nBrittany\n1997\n19.0\n253.0\n166.0\n138.0\n825.0\n141.0\n111.0\n24.0\n...\n342.0\n966.0\n85.0\n334.0\n19.0\n147.0\n230.0\n102.0\n7.0\n11527.0\n\n\n53235\nBrittany\n1998\n29.0\n190.0\n109.0\n165.0\n735.0\n134.0\n85.0\n22.0\n...\n324.0\n828.0\n75.0\n310.0\n23.0\n121.0\n170.0\n114.0\n11.0\n9843.0\n\n\n53236\nBrittany\n1999\n18.0\n163.0\n97.0\n114.0\n607.0\n135.0\n69.0\n20.0\n...\n265.0\n698.0\n70.0\n246.0\n13.0\n104.0\n126.0\n83.0\n11.0\n7942.0\n\n\n53237\nBrittany\n2000\n14.0\n111.0\n63.0\n88.0\n354.0\n71.0\n33.0\n14.0\n...\n199.0\n432.0\n41.0\n176.0\n9.0\n59.0\n91.0\n71.0\n6.0\n5183.0\n\n\n53238\nBrittany\n2001\n6.0\n92.0\n43.0\n60.0\n223.0\n35.0\n27.0\n9.0\n...\n109.0\n287.0\n21.0\n83.0\n5.0\n33.0\n33.0\n50.0\n0.0\n2915.0\n\n\n53239\nBrittany\n2002\n0.0\n39.0\n28.0\n32.0\n144.0\n32.0\n9.0\n7.0\n...\n66.0\n206.0\n22.0\n70.0\n5.0\n16.0\n30.0\n29.0\n0.0\n1912.0\n\n\n53240\nBrittany\n2003\n0.0\n32.0\n22.0\n25.0\n148.0\n23.0\n16.0\n5.0\n...\n54.0\n177.0\n16.0\n42.0\n0.0\n15.0\n25.0\n19.0\n0.0\n1559.0\n\n\n53241\nBrittany\n2004\n0.0\n31.0\n23.0\n24.0\n139.0\n20.0\n0.0\n5.0\n...\n50.0\n137.0\n16.0\n37.0\n0.0\n14.0\n18.0\n28.0\n6.0\n1323.5\n\n\n53242\nBrittany\n2005\n0.0\n28.0\n18.0\n29.0\n116.0\n24.0\n7.0\n0.0\n...\n45.0\n148.0\n18.0\n35.0\n0.0\n9.0\n17.0\n13.0\n0.0\n1168.0\n\n\n53243\nBrittany\n2006\n0.0\n20.0\n13.0\n20.0\n121.0\n12.0\n6.0\n0.0\n...\n41.0\n103.0\n9.0\n30.0\n0.0\n10.0\n14.0\n9.0\n0.0\n1009.0\n\n\n53244\nBrittany\n2007\n0.0\n14.0\n12.0\n26.0\n126.0\n14.0\n0.0\n0.0\n...\n45.0\n96.0\n7.0\n27.0\n0.0\n14.0\n9.0\n9.0\n0.0\n891.0\n\n\n53245\nBrittany\n2008\n0.0\n15.0\n0.0\n14.0\n102.0\n5.0\n0.0\n0.0\n...\n30.0\n92.0\n9.0\n19.0\n0.0\n9.0\n16.0\n5.0\n0.0\n749.0\n\n\n53246\nBrittany\n2009\n0.0\n7.0\n6.0\n10.0\n105.0\n12.0\n0.0\n0.0\n...\n18.0\n83.0\n6.0\n26.0\n0.0\n10.0\n7.0\n0.0\n0.0\n644.0\n\n\n53247\nBrittany\n2010\n0.0\n9.0\n0.0\n20.0\n116.0\n9.0\n0.0\n0.0\n...\n31.0\n94.0\n11.0\n22.0\n0.0\n8.0\n6.0\n0.0\n0.0\n698.0\n\n\n53248\nBrittany\n2011\n0.0\n12.0\n7.0\n17.0\n109.0\n10.0\n9.0\n0.0\n...\n18.0\n91.0\n6.0\n32.0\n0.0\n7.0\n11.0\n0.0\n0.0\n717.0\n\n\n53249\nBrittany\n2012\n0.0\n12.0\n13.0\n9.0\n137.0\n11.0\n8.0\n6.0\n...\n23.0\n94.0\n6.0\n24.0\n0.0\n7.0\n10.0\n0.0\n0.0\n745.0\n\n\n53250\nBrittany\n2013\n0.0\n13.0\n0.0\n14.0\n110.0\n7.0\n9.0\n5.0\n...\n11.0\n113.0\n7.0\n25.0\n0.0\n12.0\n8.0\n0.0\n0.0\n699.0\n\n\n53251\nBrittany\n2014\n0.0\n11.0\n5.0\n8.0\n112.0\n9.0\n10.0\n0.0\n...\n21.0\n110.0\n8.0\n15.0\n0.0\n10.0\n6.0\n0.0\n0.0\n660.0\n\n\n53252\nBrittany\n2015\n0.0\n9.0\n6.0\n10.0\n109.0\n11.0\n0.0\n0.0\n...\n16.0\n124.0\n9.0\n26.0\n0.0\n9.0\n0.0\n0.0\n0.0\n636.0\n\n\n\n\n48 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names in a single chart. What trends do you notice?\nIt seems to be that ‘Mary’ was the most common name for most of the early 1920’s. Once we get to the 1970’s however, all four names seem to take a steep decline and the doesn’t climb back up. I am lead to assume that as the world became less religious, biblical names became less popular.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nLine chart of the names Mary, Martha, Peter, and Paul\nnew_names_df = df[df['name'].isin(['Mary', 'Martha', 'Peter', 'Paul']) & (df['year'] &gt;= 1920) & (df['year'] &lt;= 2000)]\n\n(ggplot (new_names_df, aes(x='year', y='Total', color='name')) +\ngeom_line() + \n    ggtitle('Mary, Martha, Peter, and Paul Over Time') + xlab('Year') + ylab('Total') + \n    scale_x_continuous(format='d') + \n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nMary, Martha, Peter, and Paul Over Time\n\n\n\n\nMary, Martha, Peter, and Paul table\ndisplay(new_names_df)\n\n\n\n\n\n\nMary, Martha, Peter, and Paul Over Time\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n264124\nMartha\n1920\n11.0\n258.0\n224.0\n21.0\n131.0\n66.0\n55.0\n20.0\n...\n416.0\n418.0\n16.0\n269.0\n11.0\n73.0\n116.0\n172.0\n11.0\n8705.0\n\n\n264125\nMartha\n1921\n0.0\n307.0\n216.0\n18.0\n161.0\n70.0\n57.0\n34.0\n...\n420.0\n478.0\n20.0\n297.0\n13.0\n57.0\n101.0\n204.0\n9.0\n9254.0\n\n\n264126\nMartha\n1922\n9.0\n326.0\n219.0\n23.0\n126.0\n67.0\n56.0\n17.0\n...\n421.0\n501.0\n18.0\n273.0\n14.0\n39.0\n82.0\n225.0\n5.0\n9018.0\n\n\n264127\nMartha\n1923\n0.0\n341.0\n236.0\n27.0\n159.0\n63.0\n38.0\n24.0\n...\n442.0\n233.0\n22.0\n293.0\n11.0\n45.0\n72.0\n210.0\n10.0\n8731.0\n\n\n264128\nMartha\n1924\n0.0\n342.0\n257.0\n39.0\n166.0\n58.0\n49.0\n23.0\n...\n507.0\n487.0\n15.0\n155.0\n15.0\n41.0\n72.0\n196.0\n20.0\n9163.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n303691\nPeter\n1996\n0.0\n22.0\n8.0\n42.0\n501.0\n55.0\n134.0\n22.0\n...\n30.0\n183.0\n27.0\n103.0\n20.0\n109.0\n114.0\n8.0\n0.0\n4069.0\n\n\n303692\nPeter\n1997\n17.0\n19.0\n5.0\n47.0\n476.0\n44.0\n111.0\n25.0\n...\n24.0\n175.0\n27.0\n73.0\n17.0\n74.0\n98.0\n0.0\n0.0\n3821.0\n\n\n303693\nPeter\n1998\n14.0\n21.0\n8.0\n42.0\n471.0\n42.0\n104.0\n14.0\n...\n27.0\n134.0\n41.0\n78.0\n12.0\n98.0\n89.0\n0.0\n0.0\n3377.0\n\n\n303694\nPeter\n1999\n15.0\n24.0\n9.0\n48.0\n402.0\n66.0\n101.0\n23.0\n...\n30.0\n138.0\n26.0\n105.0\n10.0\n77.0\n81.0\n0.0\n0.0\n3430.0\n\n\n303695\nPeter\n2000\n11.0\n14.0\n11.0\n40.0\n379.0\n66.0\n72.0\n20.0\n...\n26.0\n147.0\n24.0\n62.0\n12.0\n68.0\n70.0\n0.0\n0.0\n3137.0\n\n\n\n\n324 rows × 54 columns\n\n\n\n\n\nQUESTION|TASK 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nLooking at the graph it looks as though there was a little rise in the use of the name after 1993. The rise after 1993 is slight and would be hard to conclude that the increase is correlated with the release of “The Sandlot”.\n\n\nRead and format data\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete \n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nLine chart of the name Benjamin\nbenjamin_df = df[df['name'] == 'Benjamin']\n\n(ggplot(benjamin_df, aes(x='year', y='Total')) +\n    geom_line(color = 'blue') +\n    geom_vline(xintercept = 1993, linetype = 'dashed', color = 'red') +\n    geom_text(x = 1992, y = 15000, label = 'Sandlot was Released', hjust = 1) +\n    geom_text(x = 1960, y = 14200, label = '(1993)', hjust = 0) +\n    ggtitle('Benjamin from The Sandlot') +\n    xlab('Year') +\n    ylab('Total') +\n    scale_x_continuous(format='d') +\n    theme(plot_title=element_text(hjust=0.5))\n)\n\n\n   \n   \nBenjamin from The Sandlot\n\n\n\n\nBenjamin from The Sandlot table\ndisplay(benjamin_df)\n\n\n\n\n\n\nBenjamin from The Sandlot\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nTN\nTX\nUT\nVA\nVT\nWA\nWI\nWV\nWY\nTotal\n\n\n\n\n41526\nBenjamin\n1910\n0.0\n13.0\n10.0\n5.0\n5.0\n0.0\n7.0\n0.0\n...\n0.0\n22.0\n0.0\n23.0\n0.0\n0.0\n5.0\n0.0\n0.0\n497.0\n\n\n41527\nBenjamin\n1911\n0.0\n14.0\n10.0\n0.0\n7.0\n7.0\n22.0\n7.0\n...\n7.0\n16.0\n0.0\n26.0\n0.0\n0.0\n7.0\n0.0\n0.0\n645.0\n\n\n41528\nBenjamin\n1912\n0.0\n25.0\n13.0\n0.0\n18.0\n7.0\n25.0\n7.0\n...\n24.0\n41.0\n0.0\n47.0\n0.0\n7.0\n19.0\n13.0\n0.0\n1245.0\n\n\n41529\nBenjamin\n1913\n0.0\n29.0\n19.0\n0.0\n16.0\n9.0\n31.0\n15.0\n...\n22.0\n57.0\n5.0\n49.0\n0.0\n14.0\n19.0\n15.0\n0.0\n1451.0\n\n\n41530\nBenjamin\n1914\n0.0\n50.0\n31.0\n5.0\n29.0\n12.0\n37.0\n8.0\n...\n35.0\n71.0\n0.0\n59.0\n0.0\n11.0\n21.0\n11.0\n0.0\n1802.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41627\nBenjamin\n2011\n23.0\n165.0\n91.0\n281.0\n1592.0\n247.0\n157.0\n57.0\n...\n244.0\n988.0\n193.0\n403.0\n33.0\n308.0\n271.0\n49.0\n17.0\n12743.0\n\n\n41628\nBenjamin\n2012\n31.0\n141.0\n87.0\n264.0\n1676.0\n212.0\n157.0\n52.0\n...\n252.0\n1035.0\n158.0\n364.0\n29.0\n336.0\n238.0\n57.0\n20.0\n12411.5\n\n\n41629\nBenjamin\n2013\n34.0\n125.0\n81.0\n300.0\n1845.0\n242.0\n145.0\n37.0\n...\n279.0\n1079.0\n178.0\n363.0\n21.0\n333.0\n262.0\n56.0\n18.0\n13460.0\n\n\n41630\nBenjamin\n2014\n36.0\n128.0\n93.0\n270.0\n1873.0\n252.0\n179.0\n64.0\n...\n249.0\n1152.0\n153.0\n352.0\n26.0\n370.0\n214.0\n50.0\n25.0\n13761.0\n\n\n41631\nBenjamin\n2015\n34.0\n151.0\n110.0\n275.0\n1809.0\n248.0\n178.0\n57.0\n...\n239.0\n1154.0\n168.0\n383.0\n21.0\n356.0\n232.0\n72.0\n29.0\n13608.0\n\n\n\n\n106 rows × 54 columns\n\n\n\n–&gt; –&gt;\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#my-data-science-portfolio",
    "href": "index.html#my-data-science-portfolio",
    "title": "About Me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Cleansing_Exploration/project3.html#elevator-pitch",
    "href": "Cleansing_Exploration/project3.html#elevator-pitch",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nIn analyzing all the baseball data, we were able to gain a few valuable key insights. First, we analyzed all the players that had BYU-Idaho listed as their college. We looked at what team they played for, what year they played for the team, and how much their salary was. Next, we identified and raked players batting averages using three different parameters. We found that as we include more data over years of players careers, we can get closer to the true batting average. Lastly, we compared two teams, the Los Angeles Dodgers and the New York Yankees, total wins from 1985 to the latest year of the data. We observed that the New York Yankees had a higher frequency of wins than the Los Angeles Dodgers.\n\n\nRead and format project data\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)"
  },
  {
    "objectID": "Cleansing_Exploration/project3.html#questiontask-1",
    "href": "Cleansing_Exploration/project3.html#questiontask-1",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nThe table displays the highest salaries reported by the players that came from BYU-Idaho. Matt Lindstrom (Lindsma01) had the most success out of all the baseball players that went to school in BYU-Idaho. Matt Lindstrom’s highest paid year on record was with the Chicago White Sox in 2014. Matt had a salary with the White Sox of $4 million.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\n\n\nBYU-Idaho Players Salaries\n# Include and execute your code here\nq_1 = \"\"\"\nselect distinct p.playerID, c.schoolID, printf('$%,.0f',s.salary) as 'Salary', s.yearID, s.teamID\nfrom collegeplaying c\njoin Salaries s on c.playerID = s.playerID\njoin People p on c.playerID = p.playerID\nwhere c.schoolID = 'idbyuid'\norder by s.salary desc\nlimit 10\n\"\"\"\nsalary_df = pd.read_sql_query(q_1,con)\ndisplay(salary_df)\n\n\n\n\n\n\n\n\n\nplayerID\nschoolID\nSalary\nyearID\nteamID\n\n\n\n\n0\nlindsma01\nidbyuid\n$4000000\n2014\nCHA\n\n\n1\nlindsma01\nidbyuid\n$3600000\n2012\nBAL\n\n\n2\nlindsma01\nidbyuid\n$2800000\n2011\nCOL\n\n\n3\nlindsma01\nidbyuid\n$2300000\n2013\nCHA\n\n\n4\nlindsma01\nidbyuid\n$1625000\n2010\nHOU\n\n\n5\nstephga01\nidbyuid\n$1025000\n2001\nSLN\n\n\n6\nstephga01\nidbyuid\n$900000\n2002\nSLN\n\n\n7\nstephga01\nidbyuid\n$800000\n2003\nSLN\n\n\n8\nstephga01\nidbyuid\n$550000\n2000\nSLN\n\n\n9\nlindsma01\nidbyuid\n$410000\n2009\nFLO\n\n\n\n\n\nBYU-Idaho Players Salaries"
  },
  {
    "objectID": "Cleansing_Exploration/project3.html#questiontask-2",
    "href": "Cleansing_Exploration/project3.html#questiontask-2",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\nPart 1: Write an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\nFrom part one we can see that when we include all those players who had at least at bat for the year their batting average for the year was perfect. All the top 5 players from part one had a batting average of 100%.\n::: {#cell-Q2 Part 1 .cell execution_count=6}\n\nBatting average\n# Include and execute your code here\nq2_1 = \"\"\"\nselect playerID, yearID, (h*1.0)/ab as 'BA'\nfrom batting\nwhere ab &gt;= 1\norder by (h*1.0)/ab desc, playerID asc\nlimit 5\n\"\"\"\nba_part_1 = pd.read_sql_query(q2_1,con)\ndisplay(ba_part_1)\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nBA\n\n\n\n\n0\naberal01\n1957\n1.0\n\n\n1\nabernte02\n1960\n1.0\n\n\n2\nabramge01\n1923\n1.0\n\n\n3\nacklefr01\n1964\n1.0\n\n\n4\nalanirj01\n2019\n1.0\n\n\n\n\n\nTop 5 Batting Averages\n\n:::\nPart 2: Use the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\nFor part two we start to whittle down the results, instead of players with at least one at bat we increase the threshold to at least 10 at bats. Now, our best batting average was only 64% instead of players have a 100% batting average.\n::: {#cell-Q2 Part 2 .cell execution_count=7}\n\nBatting average\n# Include and execute your code here\nq2_2 = \"\"\"\nselect playerID, yearID, (h*1.0)/ab as 'BA'\nfrom batting\nwhere ab &gt;= 10\norder by (h*1.0)/ab desc, playerID\nlimit 5\n\"\"\"\nba_part_2 = pd.read_sql_query(q2_2,con)\ndisplay(ba_part_2)\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nBA\n\n\n\n\n0\nnymanny01\n1974\n0.642857\n\n\n1\ncarsoma01\n2013\n0.636364\n\n\n2\naltizda01\n1910\n0.600000\n\n\n3\njohnsde01\n1975\n0.600000\n\n\n4\nsilvech01\n1948\n0.571429\n\n\n\n\n\nTop 5 Batting Averages\n\n:::\nPart 3: Now calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\nFor part three we whittled down even further by obtaining players lifetime batting average across all the years they played. The highest player had a batting average of only 35.8%. Following the progression of the parameters. When we widen the number of at bats and hits, it becomes clear that the best players do not always have a 100% batting average like we saw in part one\n::: {#cell-Q2 Part 3 .cell execution_count=8}\n\nBatting average\n# Include and execute your code here\nq2_3 = \"\"\"\nselect playerID, sum(h*1.0)/sum(ab) as 'BA'\nfrom batting\nwhere ab &gt;= 100\ngroup by playerID\norder by (h*1.0)/ab desc, playerID\nlimit 5\n\"\"\"\nba_part_3 = pd.read_sql_query(q2_3,con)\ndisplay(ba_part_3)\n\n\n\n\n\n\n\n\n\nplayerID\nBA\n\n\n\n\n0\nmeyerle01\n0.357542\n\n\n1\nmcveyca01\n0.345802\n\n\n2\njacksjo01\n0.357009\n\n\n3\nhazlebo01\n0.402985\n\n\n4\nbarnero01\n0.363201\n\n\n\n\n\nTop 5 Batting Averages\n\n:::"
  },
  {
    "objectID": "Cleansing_Exploration/project3.html#questiontask-3",
    "href": "Cleansing_Exploration/project3.html#questiontask-3",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Lets-Plot to visualize the comparison. What do you learn?\nI chose to use total wins for each year to compare the Los Angeles Dodgers (LAN) to the total wins for the New York Yankees (NYA). I chose this measure to compare the teams because I feel as though looking at the wins of the two teams gives a great overall comparison of the success of the team. In examining the data and graph I concluded that the New York Yankees had successful year more frequently than the Los Angeles Dodgers. Both teams had their good years and bad years. Overall, the New York Yankees won more frequently than the Los Angeles Dodgers.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\nfrom lets_plot import *\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nTable of team wins\n# Include and execute your code here\n\nq_3 = \"\"\"\nselect teamID, yearID, sum(w) as 'Total_Wins'\nfrom Teams\nwhere teamID in ('NYA', 'LAN') and yearID &gt;= 1985\ngroup by teamID, yearID\norder by yearID asc\n\"\"\"\n\nwins_df = pd.read_sql_query(q_3,con)\nwins_df.tail(14)\n\n\n\n\n\n\n\n\n\nteamID\nyearID\nTotal_Wins\n\n\n\n\n56\nLAN\n2013\n92\n\n\n57\nNYA\n2013\n85\n\n\n58\nLAN\n2014\n94\n\n\n59\nNYA\n2014\n84\n\n\n60\nLAN\n2015\n92\n\n\n61\nNYA\n2015\n87\n\n\n62\nLAN\n2016\n91\n\n\n63\nNYA\n2016\n84\n\n\n64\nLAN\n2017\n104\n\n\n65\nNYA\n2017\n91\n\n\n66\nLAN\n2018\n92\n\n\n67\nNYA\n2018\n100\n\n\n68\nLAN\n2019\n106\n\n\n69\nNYA\n2019\n103\n\n\n\n\n\nWins by Team\n\n\n\n\nGraph of wins\n# Include and execute your code here\n\n(ggplot(wins_df, aes(x='yearID', y='Total_Wins', color='teamID')) + geom_line() + ggtitle('Total Wins by Team') + xlab('Year') + ylab('Total Wins') + scale_x_continuous(format='d') + theme(plot_title=element_text(hjust=0.5))\n)"
  },
  {
    "objectID": "Cleansing_Exploration/project3.html#questiontaskstretch-4",
    "href": "Cleansing_Exploration/project3.html#questiontaskstretch-4",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK(STRETCH) 4",
    "text": "QUESTION|TASK(STRETCH) 4\nAdvanced Salary Distribution by Position\nFrom this we can see that the on average the highest paid position is “Designated Hitter”. While this position has the highest average salary, when we look at the higest (max) salary for each position we find that “Third Baseman” and “Pitcher” are tied for the higest salary at $33 million. All positions are categorized as “High Salary”.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\n\n\nAverage salary by position\n# Include and execute your code here\n\nq_4 = \"\"\"\nselect \n  p.position, printf('$%,.0f',avg(s.salary)) as 'Average_Salary', count(distinct s.playerID) as 'Total_Players',\n  coalesce(printf('$%,.0f',max(s.salary)), 'N/A') as 'Highest_Salary',\n  case \n    when avg(s.salary) &gt; 1000000 then 'High Salary'\n    when avg(s.salary) between 5000000 and 1000000 then 'Medium salary'\n    else 'Low Salary'\n  end as salary_category\nfrom Salaries s\njoin appearances a on s.playerID = a.playerID\njoin(\n  select playerID, max(position) as position\n  from (\n    select playerID,\n      case\n        when G_p &gt; 0 then 'Pitcher'\n        when G_c &gt; 0 then 'Catcher'\n        when G_1b &gt; 0 then 'First Baseman'\n        when G_2b &gt; 0 then 'Second Baseman'\n        when G_3b &gt; 0 then 'Third Baseman'\n        when G_ss &gt; 0 then 'Shortstop'\n        when G_lf &gt; 0 then 'Left Fielder'\n        when G_cf &gt; 0 then 'Center Fielder'\n        when G_rf &gt; 0 then 'Right Fielder'\n        when G_of &gt; 0 then 'Outfielder'\n        when G_dh &gt; 0 then 'Designated Hitter'\n        when G_ph &gt; 0 then 'Pinch Hitter'\n        when G_pr &gt; 0 then 'Pinch Runner'\n        \n      end as position\n    from appearances\n  )\n  group by playerID\n) p on s.playerID = p.playerID\ngroup by p.position\norder by avg(s.salary) desc\nlimit 10\n\"\"\"\n\nsalary_df = pd.read_sql_query(q_4,con)\ndisplay(salary_df)\n\n\n\n\n\n\n\n\n\nposition\nAverage_Salary\nTotal_Players\nHighest_Salary\nsalary_category\n\n\n\n\n0\nDesignated Hitter\n$6625195\n7\n$16071429\nHigh Salary\n\n\n1\nFirst Baseman\n$4086740\n248\n$25000000\nHigh Salary\n\n\n2\nRight Fielder\n$3713566\n233\n$23854494\nHigh Salary\n\n\n3\nThird Baseman\n$2687984\n487\n$33000000\nHigh Salary\n\n\n4\nLeft Fielder\n$2640393\n521\n$27328046\nHigh Salary\n\n\n5\nShortstop\n$2500582\n169\n$22600000\nHigh Salary\n\n\n6\nPitcher\n$2364202\n2745\n$33000000\nHigh Salary\n\n\n7\nSecond Baseman\n$2020024\n370\n$24000000\nHigh Salary\n\n\n8\nCatcher\n$1617095\n298\n$20777778\nHigh Salary\n\n\n9\nCenter Fielder\n$1556184\n15\n$3900000\nHigh Salary\n\n\n\n\n\nAverage Salary by Position"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html",
    "href": "Cleansing_Exploration/project2.html",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Using the data collected from the Bureau of Transportation Statistics I have determined what airports are the best to fly out of and when to fly in general to have the best chance of avoiding delays for several reasons altogether. Chicago O’Hare International Airport (ORD) was determined to be the worst airport based on the average length of delay (1.13 Hours). ORD also had the second highest proportion of delays due to weather out of all the airports I looked at. The highest proportion of delays due to weather was San Francisco International Airport (SFO). I also determined that September has the lowest proportion of total flights delayed. The holiday months (January, July, November, and December) have the highest likelihood of experiencing delays\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\"\ndf = pd.read_json(url)\n\nAirport_nan = df['airport_name'].isna().sum()\nyear_nan = df['year'].isna().sum()\nmin_delay_nan = df['minutes_delayed_carrier'].isna().sum()\nmin_delay_nas_nan = df['minutes_delayed_nas'].isna().sum()\n\nyear_nan\n\n\nnp.int64(23)\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html#elevator-pitch",
    "href": "Cleansing_Exploration/project2.html#elevator-pitch",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Using the data collected from the Bureau of Transportation Statistics I have determined what airports are the best to fly out of and when to fly in general to have the best chance of avoiding delays for several reasons altogether. Chicago O’Hare International Airport (ORD) was determined to be the worst airport based on the average length of delay (1.13 Hours). ORD also had the second highest proportion of delays due to weather out of all the airports I looked at. The highest proportion of delays due to weather was San Francisco International Airport (SFO). I also determined that September has the lowest proportion of total flights delayed. The holiday months (January, July, November, and December) have the highest likelihood of experiencing delays\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\"\ndf = pd.read_json(url)\n\nAirport_nan = df['airport_name'].isna().sum()\nyear_nan = df['year'].isna().sum()\nmin_delay_nan = df['minutes_delayed_carrier'].isna().sum()\nmin_delay_nas_nan = df['minutes_delayed_nas'].isna().sum()\n\nyear_nan\n\n\nnp.int64(23)\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html#questiontask-1",
    "href": "Cleansing_Exploration/project2.html#questiontask-1",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”).\nI went through and made a new data frame for me to preform some cleaning. I then went through and searched for all the blanks, 1500+ minutes, and values that had -999 in them. then used the “.replace” to get them out and replace them with “NaN”.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n::: {#cell-Q1-Clean Data .cell execution_count=4}\n\nCleaned data\n# Include and execute your code here\ndf_clean = df\n\ndf_clean.replace([\"\", \"1500+\", -999], np.nan, inplace=True)\n\nexample = df_clean.iloc[2].to_json()\nexample\n\n\n'{\"airport_code\":\"IAD\",\"airport_name\":null,\"month\":\"January\",\"year\":2005.0,\"num_of_flights_total\":12381,\"num_of_delays_carrier\":\"414\",\"num_of_delays_late_aircraft\":1058.0,\"num_of_delays_nas\":895,\"num_of_delays_security\":4,\"num_of_delays_weather\":61,\"num_of_delays_total\":2430,\"minutes_delayed_carrier\":null,\"minutes_delayed_late_aircraft\":70919,\"minutes_delayed_nas\":35660.0,\"minutes_delayed_security\":208,\"minutes_delayed_weather\":4497,\"minutes_delayed_total\":134881}'\nCleaned Data\n\n:::"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html#questiontask-2",
    "href": "Cleansing_Exploration/project2.html#questiontask-2",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays?\nI chose to look at the average time each flight is delayed determining the worst airport delays. In the table you will find the total delayed flights and total delayed minutes for each airport. The portion of delayed flights measure tells us what percentage of flights out of that airport are delayed. The last column displays the average hours each flight is delayed. This helped me determine that Chicago O’Hare International Airport (ORD) is the worst airport to fly out of.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nThe orst airport based on average delay hours\n# Include and execute your code here\n\ndf_clean_group = df_clean\n\ngrouped_data = df_clean_group.groupby('airport_code').agg(\n  total_flights=('num_of_flights_total', 'sum'),\n  total_delays=('num_of_delays_total', 'sum'),\n  total_delay_minutes=('minutes_delayed_total', 'sum')\n).reset_index()\n\ngrouped_data['proportion_delayed_flights'] = (grouped_data['total_delays'] / grouped_data['total_flights']) * 100\n\ngrouped_data['average_delay_hours'] = (grouped_data['total_delay_minutes'] / grouped_data['total_delays']) / 60\n\nworst_airport = grouped_data.loc[grouped_data['average_delay_hours'].idxmax()]\n\n\nworst_airport\n\n\nairport_code                        ORD\ntotal_flights                   3597588\ntotal_delays                     830825\ntotal_delay_minutes            56356129\nproportion_delayed_flights    23.093945\naverage_delay_hours            1.130525\nName: 3, dtype: object\nWorst Airport\n\n\n::: {#cell-Q2-Airport Tables .cell .tbl-cap-location-top tbl-cap=‘Airport Delays’ execution_count=7}\n\nAirport delays table\n# Include and execute your code here\ngrouped_data\n\n\n\n\n\n\n\n\n\nairport_code\ntotal_flights\ntotal_delays\ntotal_delay_minutes\nproportion_delayed_flights\naverage_delay_hours\n\n\n\n\n0\nATL\n4430047\n902443\n53983926\n20.370958\n0.996996\n\n\n1\nDEN\n2513974\n468519\n25173381\n18.636589\n0.895495\n\n\n2\nIAD\n851571\n168467\n10283478\n19.783083\n1.017358\n\n\n3\nORD\n3597588\n830825\n56356129\n23.093945\n1.130525\n\n\n4\nSAN\n917862\n175132\n8276248\n19.080428\n0.787620\n\n\n5\nSFO\n1630945\n425604\n26550493\n26.095546\n1.039718\n\n\n6\nSLC\n1403384\n205160\n10123371\n14.618950\n0.822396\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html#questiontask-3",
    "href": "Cleansing_Exploration/project2.html#questiontask-3",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length?\nI chose to look at the proportion of delayed flights to determine what month is the best month to fly. I chose this as my metric because I feel this paints an accurate picture about what the chances are of your flight being delayed dependent on month. From what I found, the best month to fly is September. 16.5% of all the flights that occur in September end up being delayed.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nProportion of delayed flights by month\n# Include and execute your code here\ndf_mo = df_clean\n\ndf_mo['month'] = df_mo['month'].replace('Febuary', 'February')\n\ndf_clean_month = df_mo[df['month'].notna()]\n\nmonth_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n\ndf_clean_month['month'] = pd.Categorical(df_clean_month['month'], categories=month_order, ordered=True)\n\nmonthly_delays = df_clean_month.groupby('month').agg(\n  total_flights=('num_of_flights_total', 'sum'),\n  total_delays=('num_of_delays_total', 'sum'),\n  total_delay_minutes=('minutes_delayed_total', 'sum')\n).reset_index()\n\nmonthly_delays['proportion_delayed_flights'] = (monthly_delays['total_delays'] / monthly_delays['total_flights']) * 100\n\n(\n  ggplot(monthly_delays, aes(x='month', y='proportion_delayed_flights')) +\n  geom_bar(stat='identity') +\n  ggtitle('Proportion of Delayed Flights by Month') +\n  xlab('Month') +\n  ylab('Proportion of Delayed Flights (%)') +\n  theme(axis_text_x=element_text(angle=45, hjust=1))\n)\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nProportion of Delayed Flights by Month"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html#questiontask-4",
    "href": "Cleansing_Exploration/project2.html#questiontask-4",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nYour job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild).\nI was able to create the new row and add it to the table. The column “weather_related_delays” will be used for the next question.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWeather delays\n# Include and execute your code here\n\ndf_clean_two = df_clean\n\ndf_clean_two['num_of_delays_late_aircraft'] = df_clean_two['num_of_delays_late_aircraft'].replace(-999, np.nan)\n\ndf_clean_two['num_of_delays_late_aircraft'].fillna(df['num_of_delays_late_aircraft'].mean(), inplace=True)\n\ndef weather_nas_delays(row):\n  if row['month'] in ['April', 'May', 'June', 'July', 'August']:\n    return 0.4 * round(row['num_of_delays_nas'], 2)\n  else:\n    return 0.65 * round(row['num_of_delays_nas'], 2)\n\ndf_clean_two['weather_related_delays'] = (\n  df_clean_two['num_of_delays_weather'] +\n  round(0.3 * df_clean['num_of_delays_late_aircraft'], 2) +\n  df_clean_two.apply(weather_nas_delays, axis=1)\n)\n\ndf_clean_two.head(5)\n\n\n\n\n\n\n\n\n\nairport_code\nairport_name\nmonth\nyear\nnum_of_flights_total\nnum_of_delays_carrier\nnum_of_delays_late_aircraft\nnum_of_delays_nas\nnum_of_delays_security\nnum_of_delays_weather\nnum_of_delays_total\nminutes_delayed_carrier\nminutes_delayed_late_aircraft\nminutes_delayed_nas\nminutes_delayed_security\nminutes_delayed_weather\nminutes_delayed_total\nweather_related_delays\n\n\n\n\n0\nATL\nAtlanta, GA: Hartsfield-Jackson Atlanta Intern...\nJanuary\n2005.0\n35048\nNaN\n1109.104072\n4598\n10\n448\n8355\n116423.0\n104415\n207467.0\n297\n36931\n465533\n3769.43\n\n\n1\nDEN\nDenver, CO: Denver International\nJanuary\n2005.0\n12687\n1041\n928.000000\n935\n11\n233\n3153\n53537.0\n70301\n36817.0\n363\n21779\n182797\n1119.15\n\n\n2\nIAD\nNaN\nJanuary\n2005.0\n12381\n414\n1058.000000\n895\n4\n61\n2430\nNaN\n70919\n35660.0\n208\n4497\n134881\n960.15\n\n\n3\nORD\nChicago, IL: Chicago O'Hare International\nJanuary\n2005.0\n28194\n1197\n2255.000000\n5415\n5\n306\n9178\n88691.0\n160811\n364382.0\n151\n24859\n638894\n4502.25\n\n\n4\nSAN\nSan Diego, CA: San Diego International\nJanuary\n2005.0\n7283\n572\n680.000000\n638\n7\n56\n1952\n27436.0\n38445\n21127.0\n218\n4326\n91552\n674.70\n\n\n\n\n\nWeather Delays"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html#questiontask-5",
    "href": "Cleansing_Exploration/project2.html#questiontask-5",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Describe what you learn from this graph.\nAccording the the data San Francisco International Airport (SFO) experiences the highest proportion of delays due to bad weather days. Approximately 9.81% of the flights from SFO are delayed due to weather. Upon further research I found that San Francisco experiences frequent oceanic fog that causes delays and groundings for outbound flights. The next airport to avoid flying out of is Chicago O’Hare International Airport (ORD) with 8.5% of their flights become delayed.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWeather delays by airport\n# Include and execute your code here\ndf_clean_three = df_clean_two\n\ndf_clean_three['proportion_weather_delays'] = (df_clean_three['weather_related_delays'] / df_clean_three['num_of_flights_total']) * 100\n\nairport_weather_delays = df_clean_three.groupby('airport_code').agg(proportion_weather_delays=('proportion_weather_delays', 'mean')\n).reset_index()\n\n(\n  ggplot(airport_weather_delays, aes(x='airport_code', y='proportion_weather_delays')) +\n  geom_bar(stat='identity') +\n  ggtitle('Proportion of Weather Delays by Airport') +\n  xlab('Airport Code') +\n  ylab('Proportion of Weather Delays (%)') +\n  theme(axis_text_x=element_text(angle=45, hjust=1))\n)\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nProportion of Weather Delays by Airport"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "Competition/project3.html#elevator-pitch",
    "href": "Competition/project3.html#elevator-pitch",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nIn analyzing all the baseball data, we were able to gain a few valuable key insights. First, we analyzed all the players that had BYU-Idaho listed as their college. We looked at what team they played for, what year they played for the team, and how much their salary was. Next, we identified and raked players batting averages using three different parameters. We found that as we include more data over years of players careers, we can get closer to the true batting average. Lastly, we compared two teams, the Los Angeles Dodgers and the New York Yankees, total wins from 1985 to the latest year of the data. We observed that the New York Yankees had a higher frequency of wins than the Los Angeles Dodgers.\n\n\nRead and format project data\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)",
    "crumbs": [
      "Competition",
      "Project 3"
    ]
  },
  {
    "objectID": "Competition/project3.html#questiontask-1",
    "href": "Competition/project3.html#questiontask-1",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nThe table displays the highest salaries reported by the players that came from BYU-Idaho. Matt Lindstrom (Lindsma01) had the most success out of all the baseball players that went to school in BYU-Idaho. Matt Lindstrom’s highest paid year on record was with the Chicago White Sox in 2014. Matt had a salary with the White Sox of $4 million.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\n\n\nBYU-Idaho Players Salaries\n# Include and execute your code here\nq_1 = \"\"\"\nselect distinct p.playerID, c.schoolID, printf('$%,.0f',s.salary) as 'Salary', s.yearID, s.teamID\nfrom collegeplaying c\njoin Salaries s on c.playerID = s.playerID\njoin People p on c.playerID = p.playerID\nwhere c.schoolID = 'idbyuid'\norder by s.salary desc\nlimit 10\n\"\"\"\nsalary_df = pd.read_sql_query(q_1,con)\ndisplay(salary_df)\n\n\n\n\n\n\n\n\n\nplayerID\nschoolID\nSalary\nyearID\nteamID\n\n\n\n\n0\nlindsma01\nidbyuid\n$4000000\n2014\nCHA\n\n\n1\nlindsma01\nidbyuid\n$3600000\n2012\nBAL\n\n\n2\nlindsma01\nidbyuid\n$2800000\n2011\nCOL\n\n\n3\nlindsma01\nidbyuid\n$2300000\n2013\nCHA\n\n\n4\nlindsma01\nidbyuid\n$1625000\n2010\nHOU\n\n\n5\nstephga01\nidbyuid\n$1025000\n2001\nSLN\n\n\n6\nstephga01\nidbyuid\n$900000\n2002\nSLN\n\n\n7\nstephga01\nidbyuid\n$800000\n2003\nSLN\n\n\n8\nstephga01\nidbyuid\n$550000\n2000\nSLN\n\n\n9\nlindsma01\nidbyuid\n$410000\n2009\nFLO\n\n\n\n\n\nBYU-Idaho Players Salaries",
    "crumbs": [
      "Competition",
      "Project 3"
    ]
  },
  {
    "objectID": "Competition/project3.html#questiontask-2",
    "href": "Competition/project3.html#questiontask-2",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\nPart 1: Write an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\nFrom part one we can see that when we include all those players who had at least at bat for the year their batting average for the year was perfect. All the top 5 players from part one had a batting average of 100%.\n::: {#cell-Q2 Part 1 .cell execution_count=6}\n\nBatting average\n# Include and execute your code here\nq2_1 = \"\"\"\nselect playerID, yearID, (h*1.0)/ab as 'BA'\nfrom batting\nwhere ab &gt;= 1\norder by (h*1.0)/ab desc, playerID asc\nlimit 5\n\"\"\"\nba_part_1 = pd.read_sql_query(q2_1,con)\ndisplay(ba_part_1)\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nBA\n\n\n\n\n0\naberal01\n1957\n1.0\n\n\n1\nabernte02\n1960\n1.0\n\n\n2\nabramge01\n1923\n1.0\n\n\n3\nacklefr01\n1964\n1.0\n\n\n4\nalanirj01\n2019\n1.0\n\n\n\n\n\nTop 5 Batting Averages\n\n:::\nPart 2: Use the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\nFor part two we start to whittle down the results, instead of players with at least one at bat we increase the threshold to at least 10 at bats. Now, our best batting average was only 64% instead of players have a 100% batting average.\n::: {#cell-Q2 Part 2 .cell execution_count=7}\n\nBatting average\n# Include and execute your code here\nq2_2 = \"\"\"\nselect playerID, yearID, (h*1.0)/ab as 'BA'\nfrom batting\nwhere ab &gt;= 10\norder by (h*1.0)/ab desc, playerID\nlimit 5\n\"\"\"\nba_part_2 = pd.read_sql_query(q2_2,con)\ndisplay(ba_part_2)\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nBA\n\n\n\n\n0\nnymanny01\n1974\n0.642857\n\n\n1\ncarsoma01\n2013\n0.636364\n\n\n2\naltizda01\n1910\n0.600000\n\n\n3\njohnsde01\n1975\n0.600000\n\n\n4\nsilvech01\n1948\n0.571429\n\n\n\n\n\nTop 5 Batting Averages\n\n:::\nPart 3: Now calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\nFor part three we whittled down even further by obtaining players lifetime batting average across all the years they played. The highest player had a batting average of only 35.8%. Following the progression of the parameters. When we widen the number of at bats and hits, it becomes clear that the best players do not always have a 100% batting average like we saw in part one\n::: {#cell-Q2 Part 3 .cell execution_count=8}\n\nBatting average\n# Include and execute your code here\nq2_3 = \"\"\"\nselect playerID, sum(h*1.0)/sum(ab) as 'BA'\nfrom batting\nwhere ab &gt;= 100\ngroup by playerID\norder by (h*1.0)/ab desc, playerID\nlimit 5\n\"\"\"\nba_part_3 = pd.read_sql_query(q2_3,con)\ndisplay(ba_part_3)\n\n\n\n\n\n\n\n\n\nplayerID\nBA\n\n\n\n\n0\nmeyerle01\n0.357542\n\n\n1\nmcveyca01\n0.345802\n\n\n2\njacksjo01\n0.357009\n\n\n3\nhazlebo01\n0.402985\n\n\n4\nbarnero01\n0.363201\n\n\n\n\n\nTop 5 Batting Averages\n\n:::",
    "crumbs": [
      "Competition",
      "Project 3"
    ]
  },
  {
    "objectID": "Competition/project3.html#questiontask-3",
    "href": "Competition/project3.html#questiontask-3",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Lets-Plot to visualize the comparison. What do you learn?\nI chose to use total wins for each year to compare the Los Angeles Dodgers (LAN) to the total wins for the New York Yankees (NYA). I chose this measure to compare the teams because I feel as though looking at the wins of the two teams gives a great overall comparison of the success of the team. In examining the data and graph I concluded that the New York Yankees had successful year more frequently than the Los Angeles Dodgers. Both teams had their good years and bad years. Overall, the New York Yankees won more frequently than the Los Angeles Dodgers.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\nfrom lets_plot import *\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nTable of team wins\n# Include and execute your code here\n\nq_3 = \"\"\"\nselect teamID, yearID, sum(w) as 'Total_Wins'\nfrom Teams\nwhere teamID in ('NYA', 'LAN') and yearID &gt;= 1985\ngroup by teamID, yearID\norder by yearID asc\n\"\"\"\n\nwins_df = pd.read_sql_query(q_3,con)\nwins_df.tail(14)\n\n\n\n\n\n\n\n\n\nteamID\nyearID\nTotal_Wins\n\n\n\n\n56\nLAN\n2013\n92\n\n\n57\nNYA\n2013\n85\n\n\n58\nLAN\n2014\n94\n\n\n59\nNYA\n2014\n84\n\n\n60\nLAN\n2015\n92\n\n\n61\nNYA\n2015\n87\n\n\n62\nLAN\n2016\n91\n\n\n63\nNYA\n2016\n84\n\n\n64\nLAN\n2017\n104\n\n\n65\nNYA\n2017\n91\n\n\n66\nLAN\n2018\n92\n\n\n67\nNYA\n2018\n100\n\n\n68\nLAN\n2019\n106\n\n\n69\nNYA\n2019\n103\n\n\n\n\n\nWins by Team\n\n\n\n\nGraph of wins\n# Include and execute your code here\n\n(ggplot(wins_df, aes(x='yearID', y='Total_Wins', color='teamID')) + geom_line() + ggtitle('Total Wins by Team') + xlab('Year') + ylab('Total Wins') + scale_x_continuous(format='d') + theme(plot_title=element_text(hjust=0.5))\n)",
    "crumbs": [
      "Competition",
      "Project 3"
    ]
  },
  {
    "objectID": "Competition/project3.html#questiontaskstretch-4",
    "href": "Competition/project3.html#questiontaskstretch-4",
    "title": "Client Report - Finding Relationships in Baseball",
    "section": "QUESTION|TASK(STRETCH) 4",
    "text": "QUESTION|TASK(STRETCH) 4\nAdvanced Salary Distribution by Position\nFrom this we can see that the on average the highest paid position is “Designated Hitter”. While this position has the highest average salary, when we look at the higest (max) salary for each position we find that “Third Baseman” and “Pitcher” are tied for the higest salary at $33 million. All positions are categorized as “High Salary”.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\n\n\nAverage salary by position\n# Include and execute your code here\n\nq_4 = \"\"\"\nselect \n  p.position, printf('$%,.0f',avg(s.salary)) as 'Average_Salary', count(distinct s.playerID) as 'Total_Players',\n  coalesce(printf('$%,.0f',max(s.salary)), 'N/A') as 'Highest_Salary',\n  case \n    when avg(s.salary) &gt; 1000000 then 'High Salary'\n    when avg(s.salary) between 5000000 and 1000000 then 'Medium salary'\n    else 'Low Salary'\n  end as salary_category\nfrom Salaries s\njoin appearances a on s.playerID = a.playerID\njoin(\n  select playerID, max(position) as position\n  from (\n    select playerID,\n      case\n        when G_p &gt; 0 then 'Pitcher'\n        when G_c &gt; 0 then 'Catcher'\n        when G_1b &gt; 0 then 'First Baseman'\n        when G_2b &gt; 0 then 'Second Baseman'\n        when G_3b &gt; 0 then 'Third Baseman'\n        when G_ss &gt; 0 then 'Shortstop'\n        when G_lf &gt; 0 then 'Left Fielder'\n        when G_cf &gt; 0 then 'Center Fielder'\n        when G_rf &gt; 0 then 'Right Fielder'\n        when G_of &gt; 0 then 'Outfielder'\n        when G_dh &gt; 0 then 'Designated Hitter'\n        when G_ph &gt; 0 then 'Pinch Hitter'\n        when G_pr &gt; 0 then 'Pinch Runner'\n        \n      end as position\n    from appearances\n  )\n  group by playerID\n) p on s.playerID = p.playerID\ngroup by p.position\norder by avg(s.salary) desc\nlimit 10\n\"\"\"\n\nsalary_df = pd.read_sql_query(q_4,con)\ndisplay(salary_df)\n\n\n\n\n\n\n\n\n\nposition\nAverage_Salary\nTotal_Players\nHighest_Salary\nsalary_category\n\n\n\n\n0\nDesignated Hitter\n$6625195\n7\n$16071429\nHigh Salary\n\n\n1\nFirst Baseman\n$4086740\n248\n$25000000\nHigh Salary\n\n\n2\nRight Fielder\n$3713566\n233\n$23854494\nHigh Salary\n\n\n3\nThird Baseman\n$2687984\n487\n$33000000\nHigh Salary\n\n\n4\nLeft Fielder\n$2640393\n521\n$27328046\nHigh Salary\n\n\n5\nShortstop\n$2500582\n169\n$22600000\nHigh Salary\n\n\n6\nPitcher\n$2364202\n2745\n$33000000\nHigh Salary\n\n\n7\nSecond Baseman\n$2020024\n370\n$24000000\nHigh Salary\n\n\n8\nCatcher\n$1617095\n298\n$20777778\nHigh Salary\n\n\n9\nCenter Fielder\n$1556184\n15\n$3900000\nHigh Salary\n\n\n\n\n\nAverage Salary by Position",
    "crumbs": [
      "Competition",
      "Project 3"
    ]
  },
  {
    "objectID": "Competition/project2.html",
    "href": "Competition/project2.html",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Using the data collected from the Bureau of Transportation Statistics I have determined what airports are the best to fly out of and when to fly in general to have the best chance of avoiding delays for several reasons altogether. Chicago O’Hare International Airport (ORD) was determined to be the worst airport based on the average length of delay (1.13 Hours). ORD also had the second highest proportion of delays due to weather out of all the airports I looked at. The highest proportion of delays due to weather was San Francisco International Airport (SFO). I also determined that September has the lowest proportion of total flights delayed. The holiday months (January, July, November, and December) have the highest likelihood of experiencing delays\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\"\ndf = pd.read_json(url)\n\nAirport_nan = df['airport_name'].isna().sum()\nyear_nan = df['year'].isna().sum()\nmin_delay_nan = df['minutes_delayed_carrier'].isna().sum()\nmin_delay_nas_nan = df['minutes_delayed_nas'].isna().sum()\n\nyear_nan\n\n\nnp.int64(23)\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "Competition/project2.html#elevator-pitch",
    "href": "Competition/project2.html#elevator-pitch",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Using the data collected from the Bureau of Transportation Statistics I have determined what airports are the best to fly out of and when to fly in general to have the best chance of avoiding delays for several reasons altogether. Chicago O’Hare International Airport (ORD) was determined to be the worst airport based on the average length of delay (1.13 Hours). ORD also had the second highest proportion of delays due to weather out of all the airports I looked at. The highest proportion of delays due to weather was San Francisco International Airport (SFO). I also determined that September has the lowest proportion of total flights delayed. The holiday months (January, July, November, and December) have the highest likelihood of experiencing delays\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\"\ndf = pd.read_json(url)\n\nAirport_nan = df['airport_name'].isna().sum()\nyear_nan = df['year'].isna().sum()\nmin_delay_nan = df['minutes_delayed_carrier'].isna().sum()\nmin_delay_nas_nan = df['minutes_delayed_nas'].isna().sum()\n\nyear_nan\n\n\nnp.int64(23)\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "Competition/project2.html#questiontask-1",
    "href": "Competition/project2.html#questiontask-1",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”).\nI went through and made a new data frame for me to preform some cleaning. I then went through and searched for all the blanks, 1500+ minutes, and values that had -999 in them. then used the “.replace” to get them out and replace them with “NaN”.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n::: {#cell-Q1-Clean Data .cell execution_count=4}\n\nCleaned data\n# Include and execute your code here\ndf_clean = df\n\ndf_clean.replace([\"\", \"1500+\", -999], np.nan, inplace=True)\n\nexample = df_clean.iloc[2].to_json()\nexample\n\n\n'{\"airport_code\":\"IAD\",\"airport_name\":null,\"month\":\"January\",\"year\":2005.0,\"num_of_flights_total\":12381,\"num_of_delays_carrier\":\"414\",\"num_of_delays_late_aircraft\":1058.0,\"num_of_delays_nas\":895,\"num_of_delays_security\":4,\"num_of_delays_weather\":61,\"num_of_delays_total\":2430,\"minutes_delayed_carrier\":null,\"minutes_delayed_late_aircraft\":70919,\"minutes_delayed_nas\":35660.0,\"minutes_delayed_security\":208,\"minutes_delayed_weather\":4497,\"minutes_delayed_total\":134881}'\nCleaned Data\n\n:::",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "Competition/project2.html#questiontask-2",
    "href": "Competition/project2.html#questiontask-2",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays?\nI chose to look at the average time each flight is delayed determining the worst airport delays. In the table you will find the total delayed flights and total delayed minutes for each airport. The portion of delayed flights measure tells us what percentage of flights out of that airport are delayed. The last column displays the average hours each flight is delayed. This helped me determine that Chicago O’Hare International Airport (ORD) is the worst airport to fly out of.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nThe orst airport based on average delay hours\n# Include and execute your code here\n\ndf_clean_group = df_clean\n\ngrouped_data = df_clean_group.groupby('airport_code').agg(\n  total_flights=('num_of_flights_total', 'sum'),\n  total_delays=('num_of_delays_total', 'sum'),\n  total_delay_minutes=('minutes_delayed_total', 'sum')\n).reset_index()\n\ngrouped_data['proportion_delayed_flights'] = (grouped_data['total_delays'] / grouped_data['total_flights']) * 100\n\ngrouped_data['average_delay_hours'] = (grouped_data['total_delay_minutes'] / grouped_data['total_delays']) / 60\n\nworst_airport = grouped_data.loc[grouped_data['average_delay_hours'].idxmax()]\n\n\nworst_airport\n\n\nairport_code                        ORD\ntotal_flights                   3597588\ntotal_delays                     830825\ntotal_delay_minutes            56356129\nproportion_delayed_flights    23.093945\naverage_delay_hours            1.130525\nName: 3, dtype: object\nWorst Airport\n\n\n::: {#cell-Q2-Airport Tables .cell .tbl-cap-location-top tbl-cap=‘Airport Delays’ execution_count=7}\n\nAirport delays table\n# Include and execute your code here\ngrouped_data\n\n\n\n\n\n\n\n\n\nairport_code\ntotal_flights\ntotal_delays\ntotal_delay_minutes\nproportion_delayed_flights\naverage_delay_hours\n\n\n\n\n0\nATL\n4430047\n902443\n53983926\n20.370958\n0.996996\n\n\n1\nDEN\n2513974\n468519\n25173381\n18.636589\n0.895495\n\n\n2\nIAD\n851571\n168467\n10283478\n19.783083\n1.017358\n\n\n3\nORD\n3597588\n830825\n56356129\n23.093945\n1.130525\n\n\n4\nSAN\n917862\n175132\n8276248\n19.080428\n0.787620\n\n\n5\nSFO\n1630945\n425604\n26550493\n26.095546\n1.039718\n\n\n6\nSLC\n1403384\n205160\n10123371\n14.618950\n0.822396\n\n\n\n\n\n\n:::",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "Competition/project2.html#questiontask-3",
    "href": "Competition/project2.html#questiontask-3",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length?\nI chose to look at the proportion of delayed flights to determine what month is the best month to fly. I chose this as my metric because I feel this paints an accurate picture about what the chances are of your flight being delayed dependent on month. From what I found, the best month to fly is September. 16.5% of all the flights that occur in September end up being delayed.\n\n\nRead and format data\n# Include and execute your code here\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nProportion of delayed flights by month\n# Include and execute your code here\ndf_mo = df_clean\n\ndf_mo['month'] = df_mo['month'].replace('Febuary', 'February')\n\ndf_clean_month = df_mo[df['month'].notna()]\n\nmonth_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n\ndf_clean_month['month'] = pd.Categorical(df_clean_month['month'], categories=month_order, ordered=True)\n\nmonthly_delays = df_clean_month.groupby('month').agg(\n  total_flights=('num_of_flights_total', 'sum'),\n  total_delays=('num_of_delays_total', 'sum'),\n  total_delay_minutes=('minutes_delayed_total', 'sum')\n).reset_index()\n\nmonthly_delays['proportion_delayed_flights'] = (monthly_delays['total_delays'] / monthly_delays['total_flights']) * 100\n\n(\n  ggplot(monthly_delays, aes(x='month', y='proportion_delayed_flights')) +\n  geom_bar(stat='identity') +\n  ggtitle('Proportion of Delayed Flights by Month') +\n  xlab('Month') +\n  ylab('Proportion of Delayed Flights (%)') +\n  theme(axis_text_x=element_text(angle=45, hjust=1))\n)\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nProportion of Delayed Flights by Month",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "Competition/project2.html#questiontask-4",
    "href": "Competition/project2.html#questiontask-4",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nYour job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild).\nI was able to create the new row and add it to the table. The column “weather_related_delays” will be used for the next question.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWeather delays\n# Include and execute your code here\n\ndf_clean_two = df_clean\n\ndf_clean_two['num_of_delays_late_aircraft'] = df_clean_two['num_of_delays_late_aircraft'].replace(-999, np.nan)\n\ndf_clean_two['num_of_delays_late_aircraft'].fillna(df['num_of_delays_late_aircraft'].mean(), inplace=True)\n\ndef weather_nas_delays(row):\n  if row['month'] in ['April', 'May', 'June', 'July', 'August']:\n    return 0.4 * round(row['num_of_delays_nas'], 2)\n  else:\n    return 0.65 * round(row['num_of_delays_nas'], 2)\n\ndf_clean_two['weather_related_delays'] = (\n  df_clean_two['num_of_delays_weather'] +\n  round(0.3 * df_clean['num_of_delays_late_aircraft'], 2) +\n  df_clean_two.apply(weather_nas_delays, axis=1)\n)\n\ndf_clean_two.head(5)\n\n\n\n\n\n\n\n\n\nairport_code\nairport_name\nmonth\nyear\nnum_of_flights_total\nnum_of_delays_carrier\nnum_of_delays_late_aircraft\nnum_of_delays_nas\nnum_of_delays_security\nnum_of_delays_weather\nnum_of_delays_total\nminutes_delayed_carrier\nminutes_delayed_late_aircraft\nminutes_delayed_nas\nminutes_delayed_security\nminutes_delayed_weather\nminutes_delayed_total\nweather_related_delays\n\n\n\n\n0\nATL\nAtlanta, GA: Hartsfield-Jackson Atlanta Intern...\nJanuary\n2005.0\n35048\nNaN\n1109.104072\n4598\n10\n448\n8355\n116423.0\n104415\n207467.0\n297\n36931\n465533\n3769.43\n\n\n1\nDEN\nDenver, CO: Denver International\nJanuary\n2005.0\n12687\n1041\n928.000000\n935\n11\n233\n3153\n53537.0\n70301\n36817.0\n363\n21779\n182797\n1119.15\n\n\n2\nIAD\nNaN\nJanuary\n2005.0\n12381\n414\n1058.000000\n895\n4\n61\n2430\nNaN\n70919\n35660.0\n208\n4497\n134881\n960.15\n\n\n3\nORD\nChicago, IL: Chicago O'Hare International\nJanuary\n2005.0\n28194\n1197\n2255.000000\n5415\n5\n306\n9178\n88691.0\n160811\n364382.0\n151\n24859\n638894\n4502.25\n\n\n4\nSAN\nSan Diego, CA: San Diego International\nJanuary\n2005.0\n7283\n572\n680.000000\n638\n7\n56\n1952\n27436.0\n38445\n21127.0\n218\n4326\n91552\n674.70\n\n\n\n\n\nWeather Delays",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "Competition/project2.html#questiontask-5",
    "href": "Competition/project2.html#questiontask-5",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Describe what you learn from this graph.\nAccording the the data San Francisco International Airport (SFO) experiences the highest proportion of delays due to bad weather days. Approximately 9.81% of the flights from SFO are delayed due to weather. Upon further research I found that San Francisco experiences frequent oceanic fog that causes delays and groundings for outbound flights. The next airport to avoid flying out of is Chicago O’Hare International Airport (ORD) with 8.5% of their flights become delayed.\n\n\nRead and format data\n# Include and execute your code here\n\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\n\n\n\nWeather delays by airport\n# Include and execute your code here\ndf_clean_three = df_clean_two\n\ndf_clean_three['proportion_weather_delays'] = (df_clean_three['weather_related_delays'] / df_clean_three['num_of_flights_total']) * 100\n\nairport_weather_delays = df_clean_three.groupby('airport_code').agg(proportion_weather_delays=('proportion_weather_delays', 'mean')\n).reset_index()\n\n(\n  ggplot(airport_weather_delays, aes(x='airport_code', y='proportion_weather_delays')) +\n  geom_bar(stat='identity') +\n  ggtitle('Proportion of Weather Delays by Airport') +\n  xlab('Airport Code') +\n  ylab('Proportion of Weather Delays (%)') +\n  theme(axis_text_x=element_text(angle=45, hjust=1))\n)\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nProportion of Weather Delays by Airport",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "About Me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "cleansing.html#my-data-science-portfolio",
    "href": "cleansing.html#my-data-science-portfolio",
    "title": "About Me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  }
]